<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>云服务器安装使用amysql</title>
    <url>/489243170.html</url>
    <content><![CDATA[<h3 id="记录一次腾讯云服务器安装Mysql遇到的坑"><a href="#记录一次腾讯云服务器安装Mysql遇到的坑" class="headerlink" title="记录一次腾讯云服务器安装Mysql遇到的坑"></a>记录一次腾讯云服务器安装Mysql遇到的坑</h3><h4 id="安装mysql"><a href="#安装mysql" class="headerlink" title="安装mysql"></a>安装mysql</h4><p>检测一下系统中是否已安装mysql的相关服务,无输出则表明未安装。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rpm -qa | grep mysql</span><br></pre></td></tr></table></figure>
<p>安装从网上下载文件的wget命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum -y install wget</span><br></pre></td></tr></table></figure>
<p>下载mysql的repo源</p>
<a id="more"></a>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm</span><br></pre></td></tr></table></figure>
<p>安装mysql-community-release-el7-5.noarch.rpm包</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rpm -ivh mysql-community-release-el7-5.noarch.rpm</span><br></pre></td></tr></table></figure>
<p>安装mysql</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install mysql-server</span><br></pre></td></tr></table></figure>
<p>查看mysql状态</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">service mysqld status 或者 # service mysql status</span><br></pre></td></tr></table></figure>
<p>查看mysql安装目录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ps -ef|grep mysql</span><br></pre></td></tr></table></figure>

<h4 id="数据库设置"><a href="#数据库设置" class="headerlink" title="数据库设置"></a>数据库设置</h4><p>启动数据库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">service mysqld start</span><br></pre></td></tr></table></figure>
<p>连接数据库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql -u root -p , 回车后输入密码</span><br></pre></td></tr></table></figure>
<p>或者使用绕过密码进行强制登陆</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysqld_safe--user=mysql --skip-grant-tables--skip-networking &amp;</span><br></pre></td></tr></table></figure>
<p>首选选择数据库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt;use mysql;</span><br></pre></td></tr></table></figure>
<p>修改密码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; update user set password=password(&quot;新密码&quot;) where user=&apos;root&apos;;</span><br></pre></td></tr></table></figure>
<p>查看用户</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select host,user from user;</span><br><span class="line">MySQL建用户的时候会指定一个host，默认是127.0.0.1/localhost只能本机访问；</span><br></pre></td></tr></table></figure>
<p>设置host为%</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">update user set host=&apos;%&apos; where user = &apos;root&apos;;</span><br><span class="line">%为任意用户都有权限连接，localhost为只能本机连接，配置完host为%这一步就已经能外网访问了</span><br></pre></td></tr></table></figure>
<p>授权用户具体权限</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">grant all privileges on *.* to &apos;root&apos;@&apos;%&apos; identified by &apos;123456&apos; with grant option;</span><br><span class="line"></span><br><span class="line">不推荐授权所有IP， GRANT ALL PRIVILEGES ON *.* TO ‘root’@’xxx.xxx.xxx.xxx’ IDENTIFIED BY ‘123456’ WITH GRANT OPTION;这种操作最好不要做。这相当于是给IP-xxx.xxx.xxx.xxx赋予了所有的权限，当然也包括远程访问权限。</span><br></pre></td></tr></table></figure>
<p>刷新权限</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">flush privileges;</span><br></pre></td></tr></table></figure>
<p>查看3306端口状态</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">netstat -an | grep 3306</span><br></pre></td></tr></table></figure>
<p>设置mysql的配置文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">/etc/mysql/my.cnf 找到 bind-address  =127.0.0.1  将其注释掉；</span><br><span class="line"></span><br><span class="line">也可以改成bind-address  =0.0.0.0</span><br></pre></td></tr></table></figure>
<p>重启mysql</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">/etc/init.d/mysql restart;</span><br></pre></td></tr></table></figure>
<p><strong>这里可能有个免密登陆的坑</strong><br>登陆mysql，输入sql</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SHOW VARIABLES LIKE ‘skip_networking’;</span><br></pre></td></tr></table></figure>
<p>如果skip-networking是on，需要关闭掉。<br>然后使用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysqld_safe --user=mysql &amp;</span><br></pre></td></tr></table></figure>
<p>重启服务</p>
<h4 id="防火墙"><a href="#防火墙" class="headerlink" title="防火墙"></a>防火墙</h4><p>关闭firewall：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl stop firewalld.service</span><br><span class="line">systemctl disable firewalld.service</span><br><span class="line">systemctl mask firewalld.service</span><br></pre></td></tr></table></figure>
<p>安装iptables防火墙</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install iptables-services -y</span><br></pre></td></tr></table></figure>
<p>设置开机启动防火墙</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl enable iptables</span><br><span class="line">systemctl start iptables</span><br></pre></td></tr></table></figure>
<p>查看防火墙状态</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl status iptables</span><br></pre></td></tr></table></figure>
<p>编辑防火墙，增加端口</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi /etc/sysconfig/iptables #编辑防火墙配置文件 将3306端口开放</span><br><span class="line">-A INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT</span><br><span class="line">-A INPUT -m state --state NEW -m tcp -p tcp --dport 80 -j ACCEPT</span><br><span class="line">-A INPUT -m state --state NEW -m tcp -p tcp --dport 3306 -j ACCEPT</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">:wq! #保存退出</span><br></pre></td></tr></table></figure>
<p> 重启配置，重启系统<br> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl restart iptables.service #重启防火墙使配置生效</span><br><span class="line">systemctl enable iptables.service #设置防火墙开机启动</span><br></pre></td></tr></table></figure><br>按照上述操作后，防火墙配置并没有生效，这里也是花时间最久的地方。<br>直到按照下面的方式处理：</p>
<p>CentOS7默认的防火墙不是iptables,而是firewalle.</p>
<p>安装iptable iptable-service</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#先检查是否安装了iptables</span><br><span class="line">service iptables status</span><br><span class="line">#安装iptables</span><br><span class="line">yum install -y iptables</span><br><span class="line">#升级iptables</span><br><span class="line">yum update iptables</span><br><span class="line">#安装iptables-services</span><br><span class="line">yum install iptables-services</span><br></pre></td></tr></table></figure>
<p>禁用/停止自带的firewalld服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#停止firewalld服务</span><br><span class="line">systemctl stop firewalld</span><br><span class="line">#禁用firewalld服务</span><br><span class="line">systemctl mask firewalld</span><br></pre></td></tr></table></figure>
<p>设置现有规则</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">#查看iptables现有规则</span><br><span class="line">iptables -L -n</span><br><span class="line">#先允许所有,不然有可能会杯具</span><br><span class="line">iptables -P INPUT ACCEPT</span><br><span class="line">#清空所有默认规则</span><br><span class="line">iptables -F</span><br><span class="line">#清空所有自定义规则</span><br><span class="line">iptables -X</span><br><span class="line">#所有计数器归0</span><br><span class="line">iptables -Z</span><br><span class="line">#允许来自于lo接口的数据包(本地访问)</span><br><span class="line">iptables -A INPUT -i lo -j ACCEPT</span><br><span class="line">#开放22端口</span><br><span class="line">iptables -A INPUT -p tcp --dport 22 -j ACCEPT</span><br><span class="line">#开放21端口(FTP)</span><br><span class="line">iptables -A INPUT -p tcp --dport 21 -j ACCEPT</span><br><span class="line">#开放80端口(HTTP)</span><br><span class="line">iptables -A INPUT -p tcp --dport 80 -j ACCEPT</span><br><span class="line">#开放443端口(HTTPS)</span><br><span class="line">iptables -A INPUT -p tcp --dport 443 -j ACCEPT</span><br><span class="line">#允许ping</span><br><span class="line">iptables -A INPUT -p icmp --icmp-type 8 -j ACCEPT</span><br><span class="line">#允许接受本机请求之后的返回数据 RELATED,是为FTP设置的</span><br><span class="line">iptables -A INPUT -m state --state  RELATED,ESTABLISHED -j ACCEPT</span><br><span class="line">#其他入站一律丢弃</span><br><span class="line">iptables -P INPUT DROP</span><br><span class="line">#所有出站一律绿灯</span><br><span class="line">iptables -P OUTPUT ACCEPT</span><br><span class="line">#所有转发一律丢弃</span><br><span class="line">iptables -P FORWARD DROP</span><br></pre></td></tr></table></figure>
<p>其他规则设定</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#如果要添加内网ip信任（接受其所有TCP请求）</span><br><span class="line">iptables -A INPUT -p tcp -s 45.96.174.68 -j ACCEPT</span><br><span class="line">#过滤所有非以上规则的请求</span><br><span class="line">iptables -P INPUT DROP</span><br><span class="line">#要封停一个IP，使用下面这条命令：</span><br><span class="line">iptables -I INPUT -s ***.***.***.*** -j DROP</span><br><span class="line">#要解封一个IP，使用下面这条命令:</span><br><span class="line">iptables -D INPUT -s ***.***.***.*** -j DROP</span><br></pre></td></tr></table></figure>
<p>解决vsftpd在iptables开启后,无法使用被动模式的问题</p>
<p>1.首先在/etc/sysconfig/iptables-config中修改或者添加以下内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#添加以下内容,注意顺序不能调换</span><br><span class="line">IPTABLES_MODULES=&quot;ip_conntrack_ftp&quot;</span><br><span class="line">IPTABLES_MODULES=&quot;ip_nat_ftp&quot;</span><br></pre></td></tr></table></figure>
<p>2.重新设置iptables设置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">iptables -A INPUT -m state --state  RELATED,ESTABLISHED -j ACCEPT</span><br></pre></td></tr></table></figure>
<p>以下为完整设置脚本</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!/bin/sh</span><br><span class="line">iptables -P INPUT ACCEPT</span><br><span class="line">iptables -F</span><br><span class="line">iptables -X</span><br><span class="line">iptables -Z</span><br><span class="line">iptables -A INPUT -i lo -j ACCEPT</span><br><span class="line">iptables -A INPUT -p tcp --dport 22 -j ACCEPT</span><br><span class="line">iptables -A INPUT -p tcp --dport 21 -j ACCEPT</span><br><span class="line">iptables -A INPUT -p tcp --dport 80 -j ACCEPT</span><br><span class="line">iptables -A INPUT -p tcp --dport 443 -j ACCEPT</span><br><span class="line">iptables -A INPUT -p icmp --icmp-type 8 -j ACCEPT</span><br><span class="line">iptables -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT</span><br><span class="line">iptables -P INPUT DROP</span><br><span class="line">iptables -P OUTPUT ACCEPT</span><br><span class="line">iptables -P FORWARD DROP</span><br><span class="line">service iptables save</span><br><span class="line">systemctl restart iptables.service</span><br></pre></td></tr></table></figure>
<h4 id="安全组"><a href="#安全组" class="headerlink" title="安全组"></a>安全组</h4><p>按照步骤配置即可<br><img src="/images/100037.bmp" alt="avatar"></p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>Redis的Key设计原则</title>
    <url>/2868983245.html</url>
    <content><![CDATA[<h3 id="Redis-的key-设计原则"><a href="#Redis-的key-设计原则" class="headerlink" title="Redis 的key 设计原则"></a>Redis 的key 设计原则</h3><p>Redis是一款基于内存式的key-value的NO-SQL数据库。可以作为数据库、缓存服务或消息服务使等。支持丰富的数据类型。比如: 字符串、哈希表、链表、集合、有序集合、位图、Hyperloglogs等</p>
<p> Redis具备LRU淘汰、事务实现、以及不同级别的硬盘持久化等能力，并且支持副本集和通过Redis Sentinel实现的高可用方案，同时还支持通过Redis Cluster实现的数据自动分片能力。</p>
<p> Redis的主要功能都基于单线程模型实现，也就是说Redis使用一个线程来服务所有的客户端请求，<br>同时Redis采用了非阻塞式IO，并精细地优化各种命令的算法时间复杂度，这些信息意味着：<br>Redis是线程安全的（因为只有一个线程），其所有操作都是原子的，不会因并发产生数据异<br>常</p>
<a id="more"></a>

<p>Redis的速度非常快（因为使用非阻塞式IO，且大部分命令的算法时间复杂度都是O(1))<br>使用高耗时的Redis命令是很危险的，会占用唯一的一个线程的大量处理时间，导致所有的请<br>求都被拖慢。（例如时间复杂度为O(N)的KEYS命令，严格禁止在生产环境中使用）</p>
<h5 id="关于Key的一些注意事项："><a href="#关于Key的一些注意事项：" class="headerlink" title="关于Key的一些注意事项："></a>关于Key的一些注意事项：</h5><p>不要使用过长的Key。例如使用一个1024字节的key就不是一个好主意，不仅会消耗更多的内存，还会导致查找的效率降低</p>
<p>Key短到缺失了可读性也是不好的，例如”u1000flw”比起”user:1000:followers”来说，节省了寥寥的存储空间，却引发了可读性和可维护性上的麻烦</p>
<p>最好使用统一的规范来设计Key，比如”object-type:id:attr”，以这一规范设计出的Key可能<br>是”user:1000”或”comment:1234:reply-to”</p>
<p>Redis允许的最大Key长度是512MB（对Value的长度限制也是512MB）</p>
<p>Redis没有Int、Float、Boolean等数据类型的概念，所有的基本类型在Redis中都以String体现。</p>
<h5 id="与String相关的常用命令："><a href="#与String相关的常用命令：" class="headerlink" title="与String相关的常用命令："></a>与String相关的常用命令：</h5><p>SET：为一个key设置value，可以配合EX/PX参数指定key的有效期，通过NX/XX参数针对<br>key是否存在的情况进行区别操作，时间复杂度O(1)</p>
<p>GET：获取某个key对应的value，时间复杂度O(1)</p>
<p>GETSET：为一个key设置value，并返回该key的原value，时间复杂度O(1)</p>
<p>MSET：为多个key设置value，时间复杂度O(N)</p>
<p>MSETNX：同MSET，如果指定的key中有任意一个已存在，则不进行任何操作，时间复杂度<br>O(N)</p>
<p>MGET：获取多个key对应的value，时间复杂度O(N)</p>
<p>Redis的基本数据类型只有String，但Redis可以把String作为整型或浮点型数字来使<br>用，主要体现在INCR、DECR类的命令上：</p>
<p>INCR：将key对应的value值自增1，并返回自增后的值。只对可以转换为整型的String数据起<br>作用。时间复杂度O(1)</p>
<p>INCRBY：将key对应的value值自增指定的整型数值，并返回自增后的值。只对可以转换为整<br>型的String数据起作用。时间复杂度O(1)</p>
<p>DECR/DECRBY：同INCR/INCRBY，自增改为自减。</p>
<p>INCR/DECR系列命令要求操作的value类型为String，并可以转换为64位带符号的整型数字，否则<br>会返回错误。<br>也就是说，进行INCR/DECR系列命令的value，必须在[-2^63 ~ 2^63 - 1]范围内。</p>
<p>前文提到过，Redis采用单线程模型，天然是线程安全的，这使得INCR/DECR命令可以非常便利的<br>实现高并发场景下的精确控制。</p>
<p><em>转自：<a href="https://blog.csdn.net/xingyue0422/article/details/88837790" target="_blank" rel="noopener">https://blog.csdn.net/xingyue0422/article/details/88837790</a></em></p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
  </entry>
  <entry>
    <title>Redis中设置了过期时间的Key，还需要知道什么</title>
    <url>/3685033284.html</url>
    <content><![CDATA[<h3 id="Redis中设置了过期时间的Key，那么你还要知道些什么？"><a href="#Redis中设置了过期时间的Key，那么你还要知道些什么？" class="headerlink" title="Redis中设置了过期时间的Key，那么你还要知道些什么？"></a>Redis中设置了过期时间的Key，那么你还要知道些什么？</h3><p><em>转自：<a href="https://cloud.tencent.com/developer/article/1522891" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1522891</a></em></p>
<p>熟悉Redis的同学应该知道，Redis的每个Key都可以设置一个过期时间，当达到过期时间的时候，这个key就会被自动删除。这就是Redis的过期策略。</p>
<h4 id="在为key设置过期时间需要注意的事项"><a href="#在为key设置过期时间需要注意的事项" class="headerlink" title="在为key设置过期时间需要注意的事项"></a>在为key设置过期时间需要注意的事项</h4><a id="more"></a>
<h5 id="1、-DEL-SET-GETSET等命令会清除过期时间"><a href="#1、-DEL-SET-GETSET等命令会清除过期时间" class="headerlink" title="1、 DEL/SET/GETSET等命令会清除过期时间"></a>1、 DEL/SET/GETSET等命令会清除过期时间</h5><p>在使用DEL、SET、GETSET等会覆盖key对应value的命令操作一个设置了过期时间的key的时候，会导致对应的key的过期时间被清除。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">//设置mykey的过期时间为300s127.0.0.1:6379&gt; set mykey hello ex 300OK//查看过期时间127.0.0.1:6379&gt; ttl mykey(integer) 294//使用set命令覆盖mykey的内容127.0.0.1:6379&gt; set mykey ollehOK//过期时间被清除127.0.0.1:6379&gt; ttl mykey(integer) -1</span><br></pre></td></tr></table></figure>
<h5 id="2、INCR-LPUSH-HSET等命令则不会清除过期时间"><a href="#2、INCR-LPUSH-HSET等命令则不会清除过期时间" class="headerlink" title="2、INCR/LPUSH/HSET等命令则不会清除过期时间"></a>2、INCR/LPUSH/HSET等命令则不会清除过期时间</h5><p>而在使用INCR/LPUSH/HSET这种只是修改一个key的value，而不是覆盖整个value的命令，则不会清除key的过期时间。INCR：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">//设置incr_key的过期时间为300s127.0.0.1:6379&gt; set incr_key 1 ex 300OK127.0.0.1:6379&gt; ttl incr_key(integer) 291//进行自增操作127.0.0.1:6379&gt; incr incr_key(integer) 2127.0.0.1:6379&gt; get incr_key&quot;2&quot;//查询过期时间，发现过期时间没有被清除127.0.0.1:6379&gt; ttl incr_key(integer) 277</span><br></pre></td></tr></table></figure>
<p>LPUSH：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">//新增一个list类型的key，并添加一个为1的值127.0.0.1:6379&gt; LPUSH list 1(integer) 1//为list设置300s的过期时间127.0.0.1:6379&gt; expire list 300(integer) 1//查看过期时间127.0.0.1:6379&gt; ttl list(integer) 292//往list里面添加值2127.0.0.1:6379&gt; lpush list 2(integer) 2//查看list的所有值127.0.0.1:6379&gt; lrange list 0 11) &quot;2&quot;2) &quot;1&quot;//能看到往list里面添加值并没有使过期时间清除127.0.0.1:6379&gt; ttl list(integer) 252</span><br></pre></td></tr></table></figure>
<h5 id="3、PERSIST命令会清除过期时间"><a href="#3、PERSIST命令会清除过期时间" class="headerlink" title="3、PERSIST命令会清除过期时间"></a>3、PERSIST命令会清除过期时间</h5><p>当使用PERSIST命令将一个设置了过期时间的key转变成一个持久化的key的时候，也会清除过期时间。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; set persist_key haha ex 300OK127.0.0.1:6379&gt; ttl persist_key(integer) 296//将key变为持久化的127.0.0.1:6379&gt; persist persist_key(integer) 1//过期时间被清除127.0.0.1:6379&gt; ttl persist_key(integer) -1</span><br></pre></td></tr></table></figure>
<h5 id="4、使用RENAME命令，老key的过期时间将会转到新key上"><a href="#4、使用RENAME命令，老key的过期时间将会转到新key上" class="headerlink" title="4、使用RENAME命令，老key的过期时间将会转到新key上"></a>4、使用RENAME命令，老key的过期时间将会转到新key上</h5><p>在使用例如：RENAME KEYA KEYB命令将KEYA重命名为KEYB，不管KEYB有没有设置过期时间，新的key KEYB将会继承KEY_A的所有特性。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">//设置key_a的过期时间为300s127.0.0.1:6379&gt; set key_a value_a ex 300OK//设置key_b的过期时间为600s127.0.0.1:6379&gt; set key_b value_b ex 600OK127.0.0.1:6379&gt; ttl key_a(integer) 279127.0.0.1:6379&gt; ttl key_b(integer) 591//将key_a重命名为key_b127.0.0.1:6379&gt; rename key_a key_bOK//新的key_b继承了key_a的过期时间127.0.0.1:6379&gt; ttl key_b(integer) 248</span><br></pre></td></tr></table></figure>
<p>这里篇幅有限，我就不一一将keya重命名到keyb的各个情况列出来，大家可以在自己电脑上试一下keya设置了过期时间，keyb没设置过期时间这种情况。</p>
<h5 id="5、使用EXPIRE-PEXPIRE设置的过期时间为负数或者使用EXPIREAT-PEXPIREAT设置过期时间戳为过去的时间会导致key被删除"><a href="#5、使用EXPIRE-PEXPIRE设置的过期时间为负数或者使用EXPIREAT-PEXPIREAT设置过期时间戳为过去的时间会导致key被删除" class="headerlink" title="5、使用EXPIRE/PEXPIRE设置的过期时间为负数或者使用EXPIREAT/PEXPIREAT设置过期时间戳为过去的时间会导致key被删除"></a>5、使用EXPIRE/PEXPIRE设置的过期时间为负数或者使用EXPIREAT/PEXPIREAT设置过期时间戳为过去的时间会导致key被删除</h5><p>EXPIRE：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; set key_1 value_1OK127.0.0.1:6379&gt; get key_1&quot;value_1&quot;//设置过期时间为-1127.0.0.1:6379&gt; expire key_1 -1(integer) 1//发现key被删除127.0.0.1:6379&gt; get key_1(nil)</span><br></pre></td></tr></table></figure>
<p>EXPIREAT：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; set key_2 value_2OK127.0.0.1:6379&gt; get key_2&quot;value_2&quot;//设置的时间戳为过去的时间127.0.0.1:6379&gt; expireat key_2 10000(integer) 1//key被删除127.0.0.1:6379&gt; get key_2(nil)</span><br></pre></td></tr></table></figure>
<h5 id="6、EXPIRE命令可以更新过期时间"><a href="#6、EXPIRE命令可以更新过期时间" class="headerlink" title="6、EXPIRE命令可以更新过期时间"></a>6、EXPIRE命令可以更新过期时间</h5><p>对一个已经设置了过期时间的key使用expire命令，可以更新其过期时间。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">//设置key_1的过期时间为100s127.0.0.1:6379&gt; set key_1 value_1 ex 100OK127.0.0.1:6379&gt; ttl key_1(integer) 95//更新key_1的过期时间为300s127.0.0.1:6379&gt; expire key_1 300(integer) 1127.0.0.1:6379&gt; ttl key_1(integer) 295</span><br></pre></td></tr></table></figure>
<p>在Redis2.1.3以下的版本中，使用expire命令更新一个已经设置了过期时间的key的过期时间会失败。并且对一个设置了过期时间的key使用LPUSH/HSET等命令修改其value的时候，会导致Redis删除该key。</p>
<h4 id="Redis的过期策略"><a href="#Redis的过期策略" class="headerlink" title="Redis的过期策略"></a>Redis的过期策略</h4><p>那你有没有想过一个问题，Redis里面如果有大量的key，怎样才能高效的找出过期的key并将其删除呢，难道是遍历每一个key吗？假如同一时期过期的key非常多，Redis会不会因为一直处理过期事件，而导致读写指令的卡顿。</p>
<p>这里说明一下，Redis是单线程的，所以一些耗时的操作会导致Redis卡顿，比如当Redis数据量特别大的时候，使用keys * 命令列出所有的key。</p>
<p>实际上Redis使用懒惰删除+定期删除相结合的方式处理过期的key。</p>
<h5 id="懒惰删除"><a href="#懒惰删除" class="headerlink" title="懒惰删除"></a>懒惰删除</h5><p>所谓懒惰删除就是在客户端访问该key的时候，redis会对key的过期时间进行检查，如果过期了就立即删除。</p>
<p>这种方式看似很完美，在访问的时候检查key的过期时间，不会占用太多的额外CPU资源。但是如果一个key已经过期了，如果长时间没有被访问，那么这个key就会一直存留在内存之中，严重消耗了内存资源。</p>
<h5 id="定期删除"><a href="#定期删除" class="headerlink" title="定期删除"></a>定期删除</h5><p>定期删除的原理是，Redis会将所有设置了过期时间的key放入一个字典中，然后每隔一段时间从字典中随机一些key检查过期时间并删除已过期的key。</p>
<p>Redis默认每秒进行10次过期扫描：</p>
<p>1、从过期字典中随机20个key</p>
<p>2、删除这20个key中已过期的</p>
<p>3、如果超过25%的key过期，则重复第一步</p>
<p>同时，为了保证不出现循环过度的情况，Redis还设置了扫描的时间上限，默认不会超过25ms。</p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
  </entry>
  <entry>
    <title>Redis缓存设计及常见问题</title>
    <url>/3694813783.html</url>
    <content><![CDATA[<h3 id="Redis缓存设计及常见问题"><a href="#Redis缓存设计及常见问题" class="headerlink" title="Redis缓存设计及常见问题"></a>Redis缓存设计及常见问题</h3><p><em>转自：<a href="https://www.bbsmax.com/A/MyJx9MjA5n/" target="_blank" rel="noopener">https://www.bbsmax.com/A/MyJx9MjA5n/</a></em></p>
<p>缓存能够有效地加速应用的读写速度，同时也可以降低后端负载，对日常应用的开发至关重要。下面会介绍缓存使<br>用技巧和设计方案，包含如下内容：缓存的收益和成本分析、缓存更新策略的选择和使用场景、缓存粒度控制法、穿透问题优化、无底洞问题优化、雪崩问题优化、热点key重建优化。</p>
<a id="more"></a>

<h4 id="缓存的收益和成本分析"><a href="#缓存的收益和成本分析" class="headerlink" title="缓存的收益和成本分析"></a>缓存的收益和成本分析</h4><p>下图左侧为客户端直接调用存储层的架构，右侧为比较典型的缓存层+存储层架构。</p>
<p><img src="/images/100025.jpg" alt="avatar"></p>
<h5 id="缓存加入后带来的收益和成本。"><a href="#缓存加入后带来的收益和成本。" class="headerlink" title="缓存加入后带来的收益和成本。"></a>缓存加入后带来的收益和成本。</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">收益：</span><br><span class="line">①加速读写：因为缓存通常都是全内存的，而存储层通常读写性能不够强悍（例如MySQL），通过缓存的使用可以</span><br><span class="line">有效地加速读写，优化用户体验。</span><br><span class="line">②降低后端负载：帮助后端减少访问量和复杂计算（例如很复杂的SQL语句），在很大程度降低了后端的负载。</span><br><span class="line">成本：</span><br><span class="line">①数据不一致性：缓存层和存储层的数据存在着一定时间窗口的不一致性，时间窗口跟更新策略有关。</span><br><span class="line">②代码维护成本：加入缓存后，需要同时处理缓存层和存储层的逻辑，增大了开发者维护代码的成本。</span><br><span class="line">③运维成本：以Redis Cluster为例，加入后无形中增加了运维成本。</span><br><span class="line">缓存的使用场景基本包含如下两种：</span><br><span class="line">①开销大的复杂计算：以MySQL为例子，一些复杂的操作或者计算（例如大量联表操作、一些分组计算），如果不</span><br><span class="line">加缓存，不但无法满足高并发量，同时也会给MySQL带来巨大的负担。</span><br><span class="line">②加速请求响应：即使查询单条后端数据足够快（例如select*from tablewhere id=），那么依然可以使用缓</span><br><span class="line">存，以Redis为例子，每秒可以完成数万次读写，并且提供的批量操作可以优化整个IO链的响应时间。</span><br></pre></td></tr></table></figure>
<h5 id="缓存更新策略"><a href="#缓存更新策略" class="headerlink" title="缓存更新策略"></a>缓存更新策略</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">缓存中的数据会和数据源中的真实数据有一段时间窗口的不一致，需要利用某些策略进行更新，下面会介绍几种主要</span><br><span class="line">的缓存更新策略。</span><br><span class="line">①LRU/LFU/FIFO算法剔除：剔除算法通常用于缓存使用量超过了预设的最大值时候，如何对现有的数据进行剔</span><br><span class="line">除。例如Redis使用maxmemory-policy这个配置作为内存最大值后对于数据的剔除策略。</span><br><span class="line">②超时剔除：通过给缓存数据设置过期时间，让其在过期时间后自动删除，例如Redis提供的expire命令。如果业</span><br><span class="line">务可以容忍一段时间内，缓存层数据和存储层数据不一致，那么可以为其设置过期时间。在数据过期后，再从真实数据</span><br><span class="line">源获取数据，重新放到缓存并设置过期时间。例如一个视频的描述信息，可以容忍几分钟内数据不一致，但是涉及交易</span><br><span class="line">方面的业务，后果可想而知。</span><br><span class="line">③主动更新：应用方对于数据的一致性要求高，需要在真实数据更新后，立即更新缓存数据。例如可以利用消息系</span><br><span class="line">统或者其他方式通知缓存更新</span><br></pre></td></tr></table></figure>
<h5 id="三种常见更新策略的对比："><a href="#三种常见更新策略的对比：" class="headerlink" title="三种常见更新策略的对比："></a>三种常见更新策略的对比：</h5><p><img src="/images/100026.jpg" alt="avatar"></p>
<p>有两个建议：<br>①低一致性业务建议配置最大内存和淘汰策略的方式使用。</p>
<p>②高一致性业务可以结合使用超时剔除和主动更新，<br>这样即使主动更新出了问题，也能保证数据过期时间后删除脏数据。</p>
<h4 id="缓存粒度控制"><a href="#缓存粒度控制" class="headerlink" title="缓存粒度控制"></a>缓存粒度控制</h4><p>缓存粒度问题是一个容易被忽视的问题，如果使用不当，可能会造成很多无用空间的浪费，网络带宽的浪费，代码</p>
<p>通用性较差等情况，需要综合数据通用性、空间占用比、代码维护性三点进行取舍。</p>
<p>缓存比较常用的选型，缓存层选用Redis，存储层选用MySQL。</p>
<p><img src="/images/100027.jpg" alt="avatar"></p>
<p>假如我现在需要对视频的信息做一个缓存，也就是需要对select * from video where id=?的每个id在redis里做一份缓存，这样cache层就可以帮助我抗住很多的访问量（注：这里不讨论一致性和架构等等问题，只讨论缓存的粒度问题）。<br>    我们假设视频表有100个属性（这个真有，有些人可能难以想象），那么问题来了，需要缓存什么维度呢，也<br>就是有两种选择吧：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">catch(id)=select * from video where id=#id</span><br><span class="line">catch(id)=select importantColumn1, importantColumn2 .. importantColumnN from video</span><br><span class="line">where id=#id 12</span><br></pre></td></tr></table></figure>
<p>其实这个问题就是缓存粒度问题，我们在缓存设计应该佮预估和考虑呢？下面我们将从通用性、空间、代码维<br>护三个角度进行说明。</p>
<h5 id="全部数据和部分数据比较"><a href="#全部数据和部分数据比较" class="headerlink" title="全部数据和部分数据比较"></a>全部数据和部分数据比较</h5><p>两者的特点是显而易见的：</p>
<table>
<thead>
<tr>
<th>数据类型</th>
<th>通用性</th>
<th>空间占用 （内存空间 + 网络码率）</th>
<th>代码维护</th>
</tr>
</thead>
<tbody><tr>
<td>全部数据</td>
<td>高</td>
<td>大</td>
<td>简单</td>
</tr>
<tr>
<td>部分数据</td>
<td>低</td>
<td>小</td>
<td>较为复杂</td>
</tr>
</tbody></table>
<h5 id="通用性"><a href="#通用性" class="headerlink" title="通用性"></a>通用性</h5><p>如果单从通用性上看、全部数据是最优秀的，但是有个问题就是是否有必要缓存全部数据，任务以后是否会有这样<br>的需求。但是从经验上看除了非常重要的信息，那些不重要的字段基本不会在缓存里出现，也就是说着中通用性，<br>通常都是想象出来的。太多人觉得通用性是最重要的。vid拿一些基本信息，回想专辑明星，于是加了全局的，通<br>用性很重要，但是要想清楚。</p>
<h5 id="空间占用："><a href="#空间占用：" class="headerlink" title="空间占用："></a>空间占用：</h5><p>很显然，缓存全部数据，会占用大量的内存，有人会说，不就费一点内存吗，能有多少钱？而且已经有人习惯<br>了把缓存当做下水道来使用，什么都框框的往里面放，但是我这里要说内存并不是免费的，可以说是很珍贵的资<br>源。instagram21-&gt;4G的例子就说明了这个道理，好的程序员可以帮助公司节约大量的资源。</p>
<h5 id="代码维护："><a href="#代码维护：" class="headerlink" title="代码维护："></a>代码维护：</h5><p>代码维护性，全部数据的优势更加明显，而部分数据一旦要加新字段就会修改代码，而且还需要对原来的数据<br>进行刷新。</p>
<h5 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h5><p>缓存粒度问题是一个容易被忽视的问题，如果使用不当，可能会造成很多无用空间的浪费，可能会造成网络带<br>宽的浪费，可能会造成代码通用性较差等情况，必须学会综合数据通用性、空间占用比、代码维护性 三点评估取舍因素权衡使用。</p>
<h4 id="缓存穿透"><a href="#缓存穿透" class="headerlink" title="缓存穿透"></a>缓存穿透</h4><p>缓存穿透是指查询一个根本不存在的数据，缓存层和存储层都不会命中，通常出于容错的考虑，如果从存储层查不<br>到数据则不写入缓存层。</p>
<p>通常可以在程序中分别统计总调用数、缓存层命中数、存储层命中数，如果发现大量存储层空命中，可能就是出现<br>了缓存穿透问题。造成缓存穿透的基本原因有两个。第一，自身业务代码或者数据出现问题，第二，一些恶意攻<br>击、爬虫等造成大量空命中。下面我们来看一下如何解决缓存穿透问题。</p>
<h5 id="1-缓存空对象："><a href="#1-缓存空对象：" class="headerlink" title="1.缓存空对象："></a>1.缓存空对象：</h5><p>如图下所示，当第2步存储层不命中后，仍然将空对象保留到缓存层中，之后再访问这个数据将会<br>从缓存中获取，这样就保护了后端数据源。</p>
<p><img src="/images/100028.jpg" alt="avatar"></p>
<p>缓存空对象会有两个问题：第一，空值做了缓存，意味着缓存层中存了更多的键，需要更多的内存空间（如果是攻<br>击，问题更严重），比较有效的方法是针对这类数据设置一个较短的过期时间，让其自动剔除。第二，缓存层和存<br>储层的数据会有一段时间窗口的不一致，可能会对业务有一定影响。例如过期时间设置为5分钟，如果此时存储层<br>添加了这个数据，那此段时间就会出现缓存层和存储层数据的不一致，此时可以利用消息系统或者其他方式清除掉<br>缓存层中的空对象。</p>
<h5 id="2-布隆过滤器拦截"><a href="#2-布隆过滤器拦截" class="headerlink" title="2.布隆过滤器拦截"></a>2.布隆过滤器拦截</h5><p>如下图所示，在访问缓存层和存储层之前，将存在的key用布隆过滤器提前保存起来，做第一层拦截。例如：一个<br>推荐系统有4亿个用户id，每个小时算法工程师会根据每个用户之前历史行为计算出推荐数据放到存储层中，但是<br>最新的用户由于没有历史行为，就会发生缓存穿透的行为，为此可以将所有推荐数据的用户做成布隆过滤器。如果<br>布隆过滤器认为该用户id不存在，那么就不会访问存储层，在一定程度保护了存储层。</p>
<p><img src="/images/100029.jpg" alt="avatar"></p>
<h6 id="缓存空对象和布隆过滤器方案对比"><a href="#缓存空对象和布隆过滤器方案对比" class="headerlink" title="缓存空对象和布隆过滤器方案对比"></a>缓存空对象和布隆过滤器方案对比</h6><p><img src="/images/100030.jpg" alt="avatar"></p>
<h4 id="无底洞优化"><a href="#无底洞优化" class="headerlink" title="无底洞优化"></a>无底洞优化</h4><p>为了满足业务需要可能会添加大量新的缓存节点，但是发现性能不但没有好转反而下降了。 用一句通俗的话解释就<br>是，更多的节点不代表更高的性能，所谓“无底洞”就是说投入越多不一定产出越多。但是分布式又是不可以避免<br>的，因为访问量和数据量越来越大，一个节点根本抗不住，所以如何高效地在分布式缓存中批量操作是一个难点。</p>
<p>无底洞问题分析：<br>①客户端一次批量操作会涉及多次网络操作，也就意味着批量操作会随着节点的增多，耗时会不断增大。<br>②网络连接数变多，对节点的性能也有一定影响。</p>
<p>如何在分布式条件下优化批量操作？我们来看一下常见的IO优化思路：</p>
<p>命令本身的优化，例如优化SQL语句等。<br>减少网络通信次数。<br>降低接入成本，例如客户端使用长连/连接池、NIO等。</p>
<p>这里我们假设命令、客户端连接已经为最优，重点讨论减少网络操作次数。下面我们将结合Redis Cluster的一<br>些特性对四种分布式的批量操作方式进行说明。</p>
<h5 id="①串行命令："><a href="#①串行命令：" class="headerlink" title="①串行命令："></a>①串行命令：</h5><p>由于n个key是比较均匀地分布在Redis Cluster的各个节点上，因此无法使用mget命令一次性获<br>取，所以通常来讲要获取n个key的值，最简单的方法就是逐次执行n个get命令，这种操作时间复杂度较高，<br>它的操作时间=n次网络时间+n次命令时间，网络次数是n。很显然这种方案不是最优的，但是实现起来比较<br>简单。</p>
<h5 id="②串行IO："><a href="#②串行IO：" class="headerlink" title="②串行IO："></a>②串行IO：</h5><p>Redis Cluster使用CRC16算法计算出散列值，再取对16383的余数就可以算出slot值，同时Smart客户<br>端会保存slot和节点的对应关系，有了这两个数据就可以将属于同一个节点的key进行归档，得到每个节点的key子<br>列表，之后对每个节点执行mget或者Pipeline操作，它的操作时间=node次网络时间+n次命令时间，网络次数是<br>node的个数，整个过程如下图所示，很明显这种方案比第一种要好很多，但是如果节点数太多，还是有一定的性<br>能问题。</p>
<p><img src="/images/100031.jpg" alt="avatar"></p>
<h5 id="③并行IO："><a href="#③并行IO：" class="headerlink" title="③并行IO："></a>③并行IO：</h5><p>此方案是将方案2中的最后一步改为多线程执行，网络次数虽然还是节点个数，但由于使用多线程网络<br>时间变为O（1），这种方案会增加编程的复杂度。</p>
<p><img src="/images/100032.jpg" alt="avatar"></p>
<h5 id="④hash-tag实现："><a href="#④hash-tag实现：" class="headerlink" title="④hash_tag实现："></a>④hash_tag实现：</h5><p>Redis Cluster的hash_tag功能，它可以将多个key强制分配到一个节点上，它的操作时间=1次网<br>络时间+n次命令时间。</p>
<h5 id="四种批量操作解决方案对比："><a href="#四种批量操作解决方案对比：" class="headerlink" title="四种批量操作解决方案对比："></a>四种批量操作解决方案对比：</h5><p><img src="/images/100033.jpg" alt="avatar"></p>
<h4 id="雪崩优化"><a href="#雪崩优化" class="headerlink" title="雪崩优化"></a>雪崩优化</h4><p>由于缓存层承载着大量请求，有效地保护了存储层，但是如果缓存层由于某些原因不能提供服务，于是所有的请求<br>都会达到存储层，存储层的调用量会暴增，造成存储层也会级联宕机的情况。</p>
<p>预防和解决缓存雪崩问题，可以从以下三个方面进行着手：</p>
<ol>
<li><p>保证缓存层服务高可用性。如果缓存层设计成高可用的，即使个别节点、个别机器、甚至是机房宕掉，依然可以提供服务，例如前面介绍过的Redis Sentinel和Redis Cluster都实现了高可用。</p>
</li>
<li><p>依赖隔离组件为后端限流并降级。在实际项目中，我们需要对重要的资源（例如Redis、MySQL、HBase、外部接口）都进行隔离，让每种资源都单独运行在自己的线程池中，即使个别资源出现了问题，对其他服务没有影响。但是线程池如何管理，比如如何关闭资源池、开启资源池、资源池阀值管理，这些做起来还是相当复杂的。</p>
</li>
<li><p>提前演练。在项目上线前，演练缓存层宕掉后，应用以及后端的负载情况以及可能出现的问题，在此基础上做一些预案设</p>
</li>
</ol>
<h4 id="热点key重建优化"><a href="#热点key重建优化" class="headerlink" title="热点key重建优化"></a>热点key重建优化</h4><p>开发人员使用“缓存+过期时间”的策略既可以加速数据读写，又保证数据的定期更新，这种模式基本能够满足绝大<br>部分需求。但是有两个问题如果同时出现，可能就会对应用造成致命的危害：</p>
<p>当前key是一个热点key（例如一个热门的娱乐新闻），并发量非常大。</p>
<p>重建缓存不能在短时间完成，可能是一个复杂计算，例如复杂的SQL、多次IO、多个依赖等。在缓存失效的瞬<br>间，有大量线程来重建缓存，造成后端负载加大，甚至可能会让应用崩溃。</p>
<p>要解决这个问题也不是很复杂，但是不能为了解决这个问题给系统带来更多的麻烦，所以需要制定如下目<br>标：</p>
<p>减少重建缓存的次数<br>数据尽可能一致<br>较少的潜在危险</p>
<h5 id="①互斥锁："><a href="#①互斥锁：" class="headerlink" title="①互斥锁："></a>①互斥锁：</h5><p>此方法只允许一个线程重建缓存，其他线程等待重建缓存的线程执行完，重新从缓存获取数据即<br>可，整个过程如图所示。</p>
<p><img src="/images/100034.jpg" alt="avatar"></p>
<h5 id="②永远不过期"><a href="#②永远不过期" class="headerlink" title="②永远不过期"></a>②永远不过期</h5><p>永远不过期”包含两层意思： 从缓存层面来看，确实没有设置过期时间，所以不会出现热点key过期后产生的问<br>题，也就是“物理”不过期。 从功能层面来看，为每个value设置一个逻辑过期时间，当发现超过逻辑过期时间后，<br>会使用单独的线程去构建缓存。</p>
<p>从实战看，此方法有效杜绝了热点key产生的问题，但唯一不足的就是重构缓存期间，会出现数据不一致的情况，<br>这取决于应用方是否容忍这种不一致。</p>
<p><img src="/images/100035.jpg" alt="avatar"></p>
<h5 id="两种热点key的解决方法"><a href="#两种热点key的解决方法" class="headerlink" title="两种热点key的解决方法"></a>两种热点key的解决方法</h5><p><img src="/images/100036.jpg" alt="avatar"></p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
  </entry>
  <entry>
    <title>数据分析之PV、UV、IP、TPS、QPS等概念的含义</title>
    <url>/3638810588.html</url>
    <content><![CDATA[<h3 id="关于PV、UV、IP、TPS、QPS、RPS、RT几个概念"><a href="#关于PV、UV、IP、TPS、QPS、RPS、RT几个概念" class="headerlink" title="关于PV、UV、IP、TPS、QPS、RPS、RT几个概念"></a>关于PV、UV、IP、TPS、QPS、RPS、RT几个概念</h3><h4 id="PV-page-view"><a href="#PV-page-view" class="headerlink" title="PV = page view"></a>PV = page view</h4><p>页面浏览量         用户每一次对网站中的每个页面访问均被记录1次。用户对同一页面的多次刷新，访问量累计。</p>
<h4 id="UV-Unique-visitor"><a href="#UV-Unique-visitor" class="headerlink" title="UV = Unique visitor"></a>UV = Unique visitor</h4><p>独立访客    通过客户端的cookies实现。即同一页面，客户端多次点击只计算一次，访问量不累计。</p>
<a id="more"></a>
<h4 id="IP-Internet-Protocol"><a href="#IP-Internet-Protocol" class="headerlink" title="IP =  Internet Protocol"></a>IP =  Internet Protocol</h4><p>本意本是指网络协议，在数据统计这块指通过ip的访问量。    即同一页面，客户端使用同一个IP访问多次只计算一次，访问量不累计。</p>
<p>UV、IP的区别</p>
<pre><code>1. 比如你是ADSL拨号上网，拨一次号自动分配一个IP，进入了网站，就算一个IP；断线了而没清理Cookies，又拨号一次自动分配一个IP，又进入了同一个网站，又统计到一个IP，这时统计数据里IP就显示统计了2次。UV没有变，是1次。

2. 同一个局域网内2个人，在2台电脑上访问同一个网站，他们的公网IP是相同的。IP就是1，但UV是2。</code></pre><h4 id="TPS-Transactions-Per-Second"><a href="#TPS-Transactions-Per-Second" class="headerlink" title="TPS = Transactions Per Second"></a>TPS = Transactions Per Second</h4><p>每秒处理的事务数目。一个事务是指一个客户机向服务器发送请求然后服务器做出反应的过程。客户机在发送请求时开始计时，收到服务器响应后结束计时，以此来计算使用的时间和完成的事务个数，最终利用这些信息作出的评估分。</p>
<h4 id="QPS-Queries-Per-Second"><a href="#QPS-Queries-Per-Second" class="headerlink" title="QPS = Queries Per Second"></a>QPS = Queries Per Second</h4><p>每秒能处理查询数目。是一台服务器每秒能够相应的查询次数，是对一个特定的查询服务器在规定时间内所处理流量多少的衡量标准。</p>
<p>每秒查询率QPS是对一个特定的查询服务器在规定时间内所处理流量多少的衡量标准，在因特网上，作为域名系统服务器的机器的性能经常用每秒查询率来衡量。对应fetches/sec，即每秒的响应请求数，也即是最大吞吐能力。 （看来是类似于TPS，只是应用于特定场景的吞吐量）</p>
<h4 id="RPS-Requests-Per-Second"><a href="#RPS-Requests-Per-Second" class="headerlink" title="RPS = Requests Per Second"></a>RPS = Requests Per Second</h4><p>每秒能处理的请求数目。等效于QPS</p>
<h4 id="RT-Response-time"><a href="#RT-Response-time" class="headerlink" title="RT = Response time"></a>RT = Response time</h4><p>响应时间是指系统对请求作出响应的时间。直观上看，这个指标与人对软件性能的主观感受是非常一致的，因为它完整地记录了整个计算机系统处理请求的时间。由于一个系统通常会提供许多功能，而不同功能的处理逻辑也千差万别，因而不同功能的响应时间也不尽相同，甚至同一功能在不同输入数据的情况下响应时间也不相同。所以，在讨论一个系统的响应时间时，人们通常是指该系统所有功能的平均时间或者所有功能的最大响应时间。当然，往往也需要对每个或每组功能讨论其平均响应时间和最大响应时间。</p>
<p>对于单机的没有并发操作的应用系统而言，人们普遍认为响应时间是一个合理且准确的性能指标。需要指出的是，响应时间的绝对值并不能直接反映软件的性能的高低，软件性能的高低实际上取决于用户对该响应时间的接受程度。对于一个游戏软件来说，响应时间小于100毫秒应该是不错的，响应时间在1秒左右可能属于勉强可以接受，如果响应时间达到3秒就完全难以接受了。而对于编译系统来说，完整编译一个较大规模软件的源代码可能需要几十分钟甚至更长时间，但这些响应时间对于用户来说都是可以接受的。</p>
<h4 id="TP-Throughput"><a href="#TP-Throughput" class="headerlink" title="TP = Throughput"></a>TP = Throughput</h4><p>吞吐量是指系统在单位时间内处理请求的数量。对于无并发的应用系统而言，吞吐量与响应时间成严格的反比关系，实际上此时吞吐量就是响应时间的倒数。前面已经说过，对于单用户的系统，响应时间（或者系统响应时间和应用延迟时间）可以很好地度量系统的性能，但对于并发系统，通常需要用吞吐量作为性能指标。</p>
<p>对于一个多用户的系统，如果只有一个用户使用时系统的平均响应时间是t，当有你n个用户使用时，每个用户看到的响应时间通常并不是n×t，而往往比n×t小很多（当然，在某些特殊情况下也可能比n×t大，甚至大很多）。这是因为处理每个请求需要用到很多资源，由于每个请求的处理过程中有许多不走难以并发执行，这导致在具体的一个时间点，所占资源往往并不多。也就是说在处理单个请求时，在每个时间点都可能有许多资源被闲置，当处理多个请求时，如果资源配置合理，每个用户看到的平均响应时间并不随用户数的增加而线性增加。实际上，不同系统的平均响应时间随用户数增加而增长的速度也不大相同，<br>这也是采用吞吐量来度量并发系统的性能的主要原因。一般而言，吞吐量是一个比较通用的指标，两个具有不同用户数和用户使用模式的系统，如果其最大吞吐量基本一致，则可以判断两个系统的处理能力基本一致。</p>
<h4 id="并发用户数"><a href="#并发用户数" class="headerlink" title="并发用户数"></a>并发用户数</h4><p>并发用户数是指系统可以同时承载的正常使用系统功能的用户的数量。与吞吐量相比，并发用户数是一个更直观但也更笼统的性能指标。实际上，并发用户数是一个非常不准确的指标，因为用户不同的使用模式会导致不同用户在单位时间发出不同数量的请求。一网站系统为例，假设用户只有注册后才能使用，但注册用户并不是每时每刻都在使用该网站，因此具体一个时刻只有部分注册用户同时在线，在线用户就在浏览网站时会花很多时间阅读网站上的信息，因而具体一个时刻只有部分在线用户同时向系统发出请求。这样，对于网站系统我们会有三个关于用户数的统计数字：注册用户数、在线用户数和同时发请求用户数。由于注册用户可能长时间不登陆网站，使用注册用户数作为性能指标会造成很大的误差。而在线用户数和同事发请求用户数都可以作为性能指标。相比而言，以在线用户作为性能指标更直观些，而以同时发请求用户数作为性能指标更准确些。</p>
]]></content>
      <categories>
        <category>产品经理</category>
      </categories>
  </entry>
  <entry>
    <title>领域驱动设计-软件模型要素</title>
    <url>/3670445945.html</url>
    <content><![CDATA[<h3 id="软件模型要素"><a href="#软件模型要素" class="headerlink" title="软件模型要素"></a>软件模型要素</h3><p><em>摘自 Eric Evans 《领域驱动设计：软件核心复杂性应对之道》</em></p>
<h3 id="ENTITY-实体"><a href="#ENTITY-实体" class="headerlink" title="ENTITY(实体)"></a>ENTITY(实体)</h3><h4 id="ENTITY建模"><a href="#ENTITY建模" class="headerlink" title="ENTITY建模"></a>ENTITY建模</h4><p>最基本的职责是确保连续性，使其行为清晰且可预测。</p>
<p>保持实体简练是实现上述职责的关键。</p>
<a id="more"></a>
<p>抓住定义实体最核心的特征，尤其是用于识别，查找，或匹配对象的特征。</p>
<p>实体除了标识问题，往往通过协调其关联的对象的操作来完成自己的职责。</p>
<p>示例：<br><img src="/images/100023.jpg" alt="avatar"></p>
<h4 id="唯一标识"><a href="#唯一标识" class="headerlink" title="唯一标识"></a>唯一标识</h4><p>保持全局唯一</p>
<h3 id="VALUE-OBJECT-值对象"><a href="#VALUE-OBJECT-值对象" class="headerlink" title="VALUE OBJECT(值对象)"></a>VALUE OBJECT(值对象)</h3><p>用于描述领域某个方面而本身没有概念标识的对象。</p>
<p>实例化后用来表示一些设计元素，只关心它们是什么，而不用关心它们是谁。</p>
<p>示例：在电力运营公司的软件中，一个地址对应于公司线路和服务的一个目的地。如果几个室友各自打电话申请电力服务，公司需要知道他们其实是住在同一个地方。在这种情况下，地址是一个ENTITY。换种方式，模型可以将电力服务与住处关联起来，住处就是一个带有地址属性的ENTITY了，这里的地址就是一个VALUE OBJECT。</p>
<p>VALUE OBJECT 经常作为参数在对象之间传递消息。</p>
<p>当我们只关心一个模型元素的属性时，应该把它归类为VALUE OBJECT。我们应该使这个模型元素能够表示出其属性的意义，并为它提供相关功能。VALUE OBJECT 是不可变的。不要为它分配任何标识。也不要设计成ENTITY那么复杂。</p>
<p><img src="/images/100024.jpg" alt="avatar"></p>
<h4 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h4><p>复制：可能导致系统被大量的对象阻塞。挡在两个机器之间传递一个副本时，只需发送一条消息，副本到达接收端是独立存在的。</p>
<p>共享：可能会减慢分布式系统的速度。传递的是一个引用，这要求每次交互都要向发送方返回一条消息。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1. 节省数据库空间或减少对象数量是一个关键要求时。</span><br><span class="line">2. 通信开销很低时（如中央服务器中）。</span><br><span class="line">3. 共享的对象被严格限定为不可变时。</span><br><span class="line">  3.1 保持VALUE OBJECT 不可变极大简化了实现，并确保共享和引用传递的安全性。</span><br><span class="line">  3.2 特殊情况</span><br><span class="line">    3.2.1 如果VALUE频繁改变。</span><br><span class="line">    3.2.2 如果创建或删除对象的开销很大。</span><br><span class="line">    3.2.3 如果替换（而不是修改）将打乱集群。</span><br><span class="line">    3.2.4 如果VALUE的共享不多，或者共享不会提高集群性能，或其他某种技术原因。</span><br></pre></td></tr></table></figure>
<h3 id="SERVICE-服务"><a href="#SERVICE-服务" class="headerlink" title="SERVICE(服务)"></a>SERVICE(服务)</h3><p>SERVICE是作为接口提供的一种操作，他在模型中是独立的，它不像ENTITY和VALUE OBJECT 那样具有封装的状态。</p>
<p>它强调与其他对象的关系。只是定义了能够给客户做什么。是一个动词，以一个活动来命名。</p>
<p>SERVICE有定义的职责，这种职责以及履行它的接口应该是作为领域模型的一部分来加以定义。</p>
<h4 id="特征"><a href="#特征" class="headerlink" title="特征"></a>特征</h4><p>与领域概念相关的操作不是ENTITY或VALUE OBJECT 的一个自然组成部分。</p>
<p>接口是根据领域模型的其他元素定义的。</p>
<p>操作是无状态的。<br>无状态指任何客户都可以使用某个SERVICE的任何实例，不必关心该实例的历史状态。</p>
<p>当领域中的某个重要的过程或转换操作不是ENTITY或VALUE OBJECT 的自然职责时，应该在模型中添加一个作为独立接口的操作，并将其声明为SERICE。定义接口时要使用模型语言，并确保操作名称是UBIQUITOUS LANGUAGE中的术语。此外，应该使SERVICE成为无状态的。</p>
<h4 id="粒度"><a href="#粒度" class="headerlink" title="粒度"></a>粒度</h4><p>控制领域层中接口的粒度，并且避免客户端与ENTITY和VALUE OBJECT耦合。</p>
<p>在大型系统中，中等粒度的、无状态的SERVICE更容易被复用，因为它们在简单的接口背后封装了重要的功能。此外，细粒度的对象可能导致分布式系统的消息传递效率低下。</p>
<p>引入领域层服务有助于应用层和领域层之间保持一条明确的界限。</p>
<h3 id="MODULE-模块，也称PACKAGE"><a href="#MODULE-模块，也称PACKAGE" class="headerlink" title="MODULE(模块，也称PACKAGE)"></a>MODULE(模块，也称PACKAGE)</h3><p>MODULE也是一种表达机制。</p>
<p>它的选择应该取决于被划分到模块中的对象的意义。</p>
<p>如果模型讲述了一个故事，MODULE就是这个故事的各个章节。</p>
<p>一个内聚的概念集合。MODULE之间是低耦合的。</p>
<p>在MODULE选择的早期，有些错误不可避免，这些错误导致了高耦合，从而使MODULE很难进行重构。而缺乏重构又会导致问题变得更加严重。克服这一问题的唯一方法就是接收挑战，仔细地分析问题的要害所在，并据此重新组织MODULE。</p>
]]></content>
      <categories>
        <category>领域驱动设计</category>
      </categories>
  </entry>
  <entry>
    <title>如何控制多线程执行顺序</title>
    <url>/2743582814.html</url>
    <content><![CDATA[<h3 id="如何控制多线程执行顺序"><a href="#如何控制多线程执行顺序" class="headerlink" title="如何控制多线程执行顺序"></a>如何控制多线程执行顺序</h3><h4 id="方法一：Join（）使用"><a href="#方法一：Join（）使用" class="headerlink" title="方法一：Join（）使用"></a>方法一：Join（）使用</h4><p>先看一段代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">package main.java;</span><br><span class="line"></span><br><span class="line">public class App &#123;</span><br><span class="line"></span><br><span class="line">  static Thread thread1 = new Thread(new Runnable()&#123;</span><br><span class="line"></span><br><span class="line">      @Override</span><br><span class="line">      public void run() &#123;</span><br><span class="line">      System.out.println(&quot;thread1&quot;);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      &#125;);</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  static Thread thread2 = new Thread(new Runnable()&#123;</span><br><span class="line"></span><br><span class="line">      @Override</span><br><span class="line">      public void run() &#123;</span><br><span class="line"></span><br><span class="line">      System.out.println(&quot;thread2&quot;);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      &#125;);</span><br><span class="line"></span><br><span class="line">  static Thread thread3 = new Thread(new Runnable()&#123;</span><br><span class="line"></span><br><span class="line">      @Override</span><br><span class="line">      public void run() &#123;</span><br><span class="line">      System.out.println(&quot;thread3&quot;);</span><br><span class="line"></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      &#125;);</span><br><span class="line"></span><br><span class="line">  public static void main(String[] args) throws InterruptedException&#123;</span><br><span class="line"></span><br><span class="line">      thread1.start();</span><br><span class="line">      thread1.join();</span><br><span class="line"></span><br><span class="line">      thread2.start();</span><br><span class="line">      thread2.join();</span><br><span class="line"></span><br><span class="line">      thread3.start();</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">thread1</span><br><span class="line">thread2</span><br><span class="line">thread3</span><br></pre></td></tr></table></figure>
<p>我们不管执行多少次都是按顺序执行的。</p>
<p>原理分析：<br>Join（）作用：让主线程等待子线程运行结束后才能继续运行。<br>这段代码里面的意思是这样的：</p>
<p>程序在main线程中调用thread1线程的join方法，则main线程放弃cpu控制权，并返回thread1线程继续执行直到线程thread1执行完毕<br>所以结果是thread1线程执行完后，才到主线程执行，相当于在main线程中同步thread1线程，thread1执行完了，main线程才有执行的机会.</p>
<p>我们来看看join（）的源码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">/**</span><br><span class="line">* Waits at most &lt;code&gt;millis&lt;/code&gt; milliseconds for this thread to</span><br><span class="line">* die. A timeout of &lt;code&gt;0&lt;/code&gt; means to wait forever.</span><br><span class="line">*/</span><br><span class="line"></span><br><span class="line">public final synchronized void join(long millis)</span><br><span class="line">  throws InterruptedException &#123;</span><br><span class="line">  long base = System.currentTimeMillis();</span><br><span class="line">  long now = 0;</span><br><span class="line"></span><br><span class="line">  if (millis &lt; 0) &#123;</span><br><span class="line">    throw new IllegalArgumentException(&quot;timeout value is negative&quot;);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  if (millis == 0) &#123;</span><br><span class="line">    while (isAlive()) &#123;</span><br><span class="line">      wait(0);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    while (isAlive()) &#123;</span><br><span class="line">      long delay = millis - now;</span><br><span class="line">      if (delay &lt;= 0) &#123;</span><br><span class="line">        break;</span><br><span class="line">      &#125;</span><br><span class="line">      wait(delay);</span><br><span class="line">      now = System.currentTimeMillis() - base;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>源码解读：<br>这里有一个isAlive()方法很重要。什么意思呢？<br>判断当前线程是否处于活动状态。活动状态就是线程启动且尚未终止，比如正在运行或准备开始运行。</p>
<p>所以从代码上看，如果线程被生成了，但还未被起动，调用它的 join() 方法是没有作用的，将直接继续向下执行。</p>
<p>wait()方法，什么意思呢？<br>在Object.java中，wait()的作用是让当前线程进入等待状态，同时，wait()也会让当前线程释放它所持有的锁。</p>
<p>所以Join()主要就是通过wait()方法来实现这个目的的。</p>
<p>最后来个代码步骤解读吧：<br>1： 主线程运行；<br>2：创建thread1线程 （创建后的thread1线程状态为新建状态）；<br>3：主线程调用thread1.start()方法 （thread1线程状态变为就绪状态，等待cpu的一个资源调度，有了资源后thread1状态为运行状态）；<br>4：主线程调用thread1.join() 方法 （主线程会休眠，等待子线程thread1运行结束后才会继续运行）。</p>
<h4 id="方法二：ExecutorService-的使用"><a href="#方法二：ExecutorService-的使用" class="headerlink" title="方法二：ExecutorService ()的使用"></a>方法二：ExecutorService ()的使用</h4><p>依旧先看代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">package main.java;</span><br><span class="line"></span><br><span class="line">import java.util.concurrent.ExecutorService;</span><br><span class="line">import java.util.concurrent.Executors;</span><br><span class="line"></span><br><span class="line">public class App &#123;</span><br><span class="line"></span><br><span class="line">static Thread thread1 = new Thread(new Runnable()&#123;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void run() &#123;</span><br><span class="line">    System.out.println(&quot;thread1&quot;);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">static Thread thread2 = new Thread(new Runnable()&#123;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void run() &#123;</span><br><span class="line"></span><br><span class="line">    System.out.println(&quot;thread2&quot;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">static Thread thread3 = new Thread(new Runnable()&#123;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void run() &#123;</span><br><span class="line">    System.out.println(&quot;thread3&quot;);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">static ExecutorService executorService = Executors.newSingleThreadExecutor();</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) throws InterruptedException&#123;</span><br><span class="line"></span><br><span class="line">    executorService.submit(thread1);</span><br><span class="line">    executorService.submit(thread2);</span><br><span class="line">    executorService.submit(thread3);</span><br><span class="line"></span><br><span class="line">    executorService.shutdown();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">thread1</span><br><span class="line">thread2</span><br><span class="line">thread3</span><br></pre></td></tr></table></figure>
<p>结果：无论运行多少次，结果都是按照我们的顺序执行的。</p>
<p>原理：利用并发包里的Excutors的newSingleThreadExecutor产生一个单线程的线程池，而这个线程池的底层原理就是一个先进先出（FIFO）的队列。代码中executor.submit依次添加了123线程，按照FIFO的特性，执行顺序也就是123的执行结果，从而保证了执行顺序。</p>
]]></content>
      <categories>
        <category>多线程</category>
      </categories>
  </entry>
  <entry>
    <title>explain执行计划详解</title>
    <url>/4132421017.html</url>
    <content><![CDATA[<h3 id="explain执行计划详解"><a href="#explain执行计划详解" class="headerlink" title="explain执行计划详解"></a>explain执行计划详解</h3><p>MySQL 优化sql  explain执行计划详解</p>
<p><img src="/images/100022.png" alt="avatar"></p>
<p>1）、id列数字越大越先执行，如果说数字一样大，那么就从上往下依次执行，id列为null的就表是这是一个结果集，不需要使用它来进行查询。</p>
<p>2）、select_type列常见的有：</p>
<p>A：simple：表示不需要union操作或者不包含子查询的简单select查询。有连接查询时，外层的查询为simple，且只有一个</p>
<a id="more"></a>
<p>B：primary：一个需要union操作或者含有子查询的select，位于最外层的单位查询的select_type即为primary。且只有一个</p>
<p>C：union：union连接的两个select查询，第一个查询是dervied派生表，除了第一个表外，第二个以后的表select_type都是union</p>
<p>D：dependent union：与union一样，出现在union 或union all语句中，但是这个查询要受到外部查询的影响</p>
<p>E：union result：包含union的结果集，在union和union all语句中,因为它不需要参与查询，所以id字段为null</p>
<p>F：subquery：除了from字句中包含的子查询外，其他地方出现的子查询都可能是subquery</p>
<p>G：dependent subquery：与dependent union类似，表示这个subquery的查询要受到外部表查询的影响</p>
<p>H：derived：from字句中出现的子查询，也叫做派生表，其他数据库中可能叫做内联视图或嵌套select</p>
<p>3）、table<br>显示的查询表名，如果查询使用了别名，那么这里显示的是别名，如果不涉及对数据表的操作，那么这显示为null，如果显示为尖括号括起来的<derived N>就表示这个是临时表，后边的N就是执行计划中的id，表示结果来自于这个查询产生。如果是尖括号括起来的&lt;union M,N&gt;，与<derived N>类似，也是一个临时表，表示这个结果来自于union查询的id为M,N的结果集。</p>
<p>4）、type<br>依次从好到差：system，const，eq_ref，ref，fulltext，ref_or_null，unique_subquery，index_subquery，range，index_merge，index，ALL，除了all之外，其他的type都可以使用到索引，除了index_merge之外，其他的type只可以用到一个索引</p>
<p>A：system：表中只有一行数据或者是空表，且只能用于myisam和memory表。如果是Innodb引擎表，type列在这个情况通常都是all或者index</p>
<p>B：const：使用唯一索引或者主键，返回记录一定是1行记录的等值where条件时，通常type是const。其他数据库也叫做唯一索引扫描</p>
<p>C：eq_ref：出现在要连接过个表的查询计划中，驱动表只返回一行数据，且这行数据是第二个表的主键或者唯一索引，且必须为not null，唯一索引和主键是多列时，只有所有的列都用作比较时才会出现eq_ref</p>
<p>D：ref：不像eq_ref那样要求连接顺序，也没有主键和唯一索引的要求，只要使用相等条件检索时就可能出现，常见与辅助索引的等值查找。或者多列主键、唯一索引中，使用第一个列之外的列作为等值查找也会出现，总之，返回数据不唯一的等值查找就可能出现。</p>
<p>E：fulltext：全文索引检索，要注意，全文索引的优先级很高，若全文索引和普通索引同时存在时，mysql不管代价，优先选择使用全文索引</p>
<p>F：ref_or_null：与ref方法类似，只是增加了null值的比较。实际用的不多。</p>
<p>G：unique_subquery：用于where中的in形式子查询，子查询返回不重复值唯一值</p>
<p>H：index_subquery：用于in形式子查询使用到了辅助索引或者in常数列表，子查询可能返回重复值，可以使用索引将子查询去重。</p>
<p>I：range：索引范围扫描，常见于使用&gt;,&lt;,is null,between ,in ,like等运算符的查询中。</p>
<p>J：index_merge：表示查询使用了两个以上的索引，最后取交集或者并集，常见and ，or的条件使用了不同的索引，官方排序这个在ref_or_null之后，但是实际上由于要读取所个索引，性能可能大部分时间都不如range</p>
<p>K：index：索引全表扫描，把索引从头到尾扫一遍，常见于使用索引列就可以处理不需要读取数据文件的查询、可以使用索引排序或者分组的查询。</p>
<p>L：all：这个就是全表扫描数据文件，然后再在server层进行过滤返回符合要求的记录。</p>
<p>5）、possible_keys<br>查询可能使用到的索引都会在这里列出来</p>
<p>6）、key<br>查询真正使用到的索引，select_type为index_merge时，这里可能出现两个以上的索引，其他的select_type这里只会出现一个。</p>
<p>7）、key_len<br>用于处理查询的索引长度，如果是单列索引，那就整个索引长度算进去，如果是多列索引，那么查询不一定都能使用到所有的列，具体使用到了多少个列的索引，这里就会计算进去，没有使用到的列，这里不会计算进去。留意下这个列的值，算一下你的多列索引总长度就知道有没有使用到所有的列了。要注意，mysql的ICP特性使用到的索引不会计入其中。另外，key_len只计算where条件用到的索引长度，而排序和分组就算用到了索引，也不会计算到key_len中。</p>
<p>8）、ref<br>如果是使用的常数等值查询，这里会显示const，如果是连接查询，被驱动表的执行计划这里会显示驱动表的关联字段，如果是条件使用了表达式或者函数，或者条件列发生了内部隐式转换，这里可能显示为func</p>
<p>9）、rows<br>这里是执行计划中估算的扫描行数，不是精确值</p>
<p>10）、extra</p>
<p>这个列可以显示的信息非常多，有几十种，常用的有</p>
<p>A：distinct：在select部分使用了distinc关键字</p>
<p>B：no tables used：不带from字句的查询或者From dual查询</p>
<p>C：使用not in()形式子查询或not exists运算符的连接查询，这种叫做反连接。即，一般连接查询是先查询内表，再查询外表，反连接就是先查询外表，再查询内表。</p>
<p>D：using filesort：排序时无法使用到索引时，就会出现这个。常见于order by和group by语句中</p>
<p>E：using index：查询时不需要回表查询，直接通过索引就可以获取查询的数据。</p>
<p>F：using join buffer（block nested loop），using join buffer（batched key accss）：5.6.x之后的版本优化关联查询的BNL，BKA特性。主要是减少内表的循环数量以及比较顺序地扫描查询。</p>
<p>G：using sort_union，using_union，using intersect，using sort_intersection：</p>
<p>   using intersect：表示使用and的各个索引的条件时，该信息表示是从处理结果获取交集</p>
<p>   using union：表示使用or连接各个使用索引的条件时，该信息表示从处理结果获取并集</p>
<p>   using sort_union和using sort_intersection：与前面两个对应的类似，只是他们是出现在用and和or查询信息量大时，先查询主键，然后进行排序合并后，才能读取记录并返回。</p>
<p>H：using temporary：表示使用了临时表存储中间结果。临时表可以是内存临时表和磁盘临时表，执行计划中看不出来，需要查看status变量，used_tmp_table，used_tmp_disk_table才能看出来。</p>
<p>I：using where：表示存储引擎返回的记录并不是所有的都满足查询条件，需要在server层进行过滤。查询条件中分为限制条件和检查条件，5.6之前，存储引擎只能根据限制条件扫描数据并返回，然后server层根据检查条件进行过滤再返回真正符合查询的数据。5.6.x之后支持ICP特性，可以把检查条件也下推到存储引擎层，不符合检查条件和限制条件的数据，直接不读取，这样就大大减少了存储引擎扫描的记录数量。extra列显示using index condition</p>
<p>J：firstmatch(tb_name)：5.6.x开始引入的优化子查询的新特性之一，常见于where字句含有in()类型的子查询。如果内表的数据量比较大，就可能出现这个</p>
<p>K：loosescan(m..n)：5.6.x之后引入的优化子查询的新特性之一，在in()类型的子查询中，子查询返回的可能有重复记录时，就可能出现这个</p>
<p>除了这些之外，还有很多查询数据字典库，执行计划过程中就发现不可能存在结果的一些提示信息</p>
<p>11）、filtered<br>使用explain extended时会出现这个列，5.7之后的版本默认就有这个字段，不需要使用explain extended了。这个字段表示存储引擎返回的数据在server层过滤后，剩下多少满足查询的记录数量的比例，注意是百分比，不是具体记录数。</p>
]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
  </entry>
  <entry>
    <title>数据库查询优化30条注意事项</title>
    <url>/2161281511.html</url>
    <content><![CDATA[<h3 id="数据库查询优化30点注意事项"><a href="#数据库查询优化30点注意事项" class="headerlink" title="数据库查询优化30点注意事项"></a>数据库查询优化30点注意事项</h3><p>1.对查询进行优化，要尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。</p>
<p>2.应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select id from t where num is null</span><br></pre></td></tr></table></figure>
<p>最好不要给数据库留NULL，尽可能的使用 NOT NULL填充数据库.</p>
<a id="more"></a>
<p>备注、描述、评论之类的可以设置为 NULL，其他的，最好不要使用NULL。</p>
<p>不要以为 NULL 不需要空间，比如：char(100) 型，在字段建立时，空间就固定了， 不管是否插入值（NULL也包含在内），都是占用 100个字符的空间的，如果是varchar这样的变长字段， null 不占用空间。</p>
<p>可以在num上设置默认值0，确保表中num列没有null值，然后这样查询：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select id from t where num = 0</span><br></pre></td></tr></table></figure>

<p>3.应尽量避免在 where 子句中使用 != 或 &lt;&gt; 操作符，否则将引擎放弃使用索引而进行全表扫描。</p>
<p>4.应尽量避免在 where 子句中使用 or 来连接条件，如果一个字段有索引，一个字段没有索引，将导致引擎放弃使用索引而进行全表扫描，如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select id from t where num=10 or Name = &apos;admin&apos;</span><br></pre></td></tr></table></figure>
<p>可以这样查询：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select id from t where num = 10</span><br><span class="line">union all</span><br><span class="line">select id from t where Name = &apos;admin&apos;</span><br></pre></td></tr></table></figure>
<p>5.in 和 not in 也要慎用，否则会导致全表扫描，如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select id from t where num in(1,2,3)</span><br></pre></td></tr></table></figure>
<p>对于连续的数值，能用 between 就不要用 in 了：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select id from t where num between 1 and 3</span><br></pre></td></tr></table></figure>
<p>很多时候用 exists 代替 in 是一个好的选择：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select num from a where num in(select num from b)</span><br></pre></td></tr></table></figure>
<p>用下面的语句替换：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select num from a where exists(select 1 from b where num=a.num)</span><br></pre></td></tr></table></figure>
<p>6.下面的查询也将导致全表扫描：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select id from t where name like ‘%abc%’</span><br></pre></td></tr></table></figure>
<p>若要提高效率，可以考虑全文检索。</p>
<p>7.如果在 where 子句中使用参数，也会导致全表扫描。因为SQL只有在运行时才会解析局部变量，但优化程序不能将访问计划的选择推迟到运行时；它必须在编译时进行选择。然 而，如果在编译时建立访问计划，变量的值还是未知的，因而无法作为索引选择的输入项。如下面语句将进行全表扫描：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select id from t where num = @num</span><br></pre></td></tr></table></figure>
<p>可以改为强制查询使用索引：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select id from t with(index(索引名)) where num = @num</span><br></pre></td></tr></table></figure>
<p>.应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select id from t where num/2 = 100</span><br></pre></td></tr></table></figure>
<p>应改为:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select id from t where num = 100*2</span><br></pre></td></tr></table></figure>

<p>9.应尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select id from t where substring(name,1,3) = ’abc’       -–name以abc开头的id</span><br><span class="line">select id from t where datediff(day,createdate,’2005-11-30′) = 0    -–‘2005-11-30’    --生成的id</span><br></pre></td></tr></table></figure>
<p>应改为:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select id from t where name like &apos;abc%&apos;</span><br><span class="line">select id from t where createdate &gt;= &apos;2005-11-30&apos; and createdate &lt; &apos;2005-12-1&apos;</span><br></pre></td></tr></table></figure>
<p>10.不要在 where 子句中的“=”左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引。</p>
<p>11.在使用索引字段作为条件时，如果该索引是复合索引，那么必须使用到该索引中的第一个字段作为条件时才能保证系统使用该索引，否则该索引将不会被使用，并且应尽可能的让字段顺序与索引顺序相一致。</p>
<p>12.不要写一些没有意义的查询，如需要生成一个空表结构：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select col1,col2 into #t from t where 1=0</span><br></pre></td></tr></table></figure>
<p>这类代码不会返回任何结果集，但是会消耗系统资源的，应改成这样：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create table #t(…)</span><br></pre></td></tr></table></figure>
<p>13.Update 语句，如果只更改1、2个字段，不要Update全部字段，否则频繁调用会引起明显的性能消耗，同时带来大量日志。</p>
<p>14.对于多张大数据量（这里几百条就算大了）的表JOIN，要先分页再JOIN，否则逻辑读会很高，性能很差。</p>
<p>15.select count(*) from table；这样不带任何条件的count会引起全表扫描，并且没有任何业务意义，是一定要杜绝的。</p>
<p>16.索引并不是越多越好，索引固然可以提高相应的 select 的效率，但同时也降低了 insert 及 update 的效率，因为 insert 或 update 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。一个表的索引数最好不要超过6个，若太多则应考虑一些不常使用到的列上建的索引是否有 必要。</p>
<p>17.应尽可能的避免更新 clustered 索引数据列，因为 clustered 索引数据列的顺序就是表记录的物理存储顺序，一旦该列值改变将导致整个表记录的顺序的调整，会耗费相当大的资源。若应用系统需要频繁更新 clustered 索引数据列，那么需要考虑是否应将该索引建为 clustered 索引。</p>
<p>18.尽量使用数字型字段，若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。这是因为引擎在处理查询和连 接时会逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。</p>
<p>19.尽可能的使用 varchar/nvarchar 代替 char/nchar ，因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。</p>
<p>20.任何地方都不要使用 select * from t ，用具体的字段列表代替“*”，不要返回用不到的任何字段。</p>
<p>21.尽量使用表变量来代替临时表。如果表变量包含大量数据，请注意索引非常有限（只有主键索引）。</p>
<p>22.避免频繁创建和删除临时表，以减少系统表资源的消耗。临时表并不是不可使用，适当地使用它们可以使某些例程更有效，例如，当需要重复引用大型表或常用表中的某个数据集时。但是，对于一次性事件， 最好使用导出表。</p>
<p>23.在新建临时表时，如果一次性插入数据量很大，那么可以使用 select into 代替 create table，避免造成大量 log ，以提高速度；如果数据量不大，为了缓和系统表的资源，应先create table，然后insert。</p>
<p>24.如果使用到了临时表，在存储过程的最后务必将所有的临时表显式删除，先 truncate table ，然后 drop table ，这样可以避免系统表的较长时间锁定。</p>
<p>25.尽量避免使用游标，因为游标的效率较差，如果游标操作的数据超过1万行，那么就应该考虑改写。</p>
<p>26.使用基于游标的方法或临时表方法之前，应先寻找基于集的解决方案来解决问题，基于集的方法通常更有效。</p>
<p>27.与临时表一样，游标并不是不可使用。对小型数据集使用 FAST_FORWARD 游标通常要优于其他逐行处理方法，尤其是在必须引用几个表才能获得所需的数据时。在结果集中包括“合计”的例程通常要比使用游标执行的速度快。如果开发时 间允许，基于游标的方法和基于集的方法都可以尝试一下，看哪一种方法的效果更好。</p>
<p>28.在所有的存储过程和触发器的开始处设置 SET NOCOUNT ON ，在结束时设置 SET NOCOUNT OFF 。无需在执行存储过程和触发器的每个语句后向客户端发送 DONE_IN_PROC 消息。</p>
<p>29.尽量避免大事务操作，提高系统并发能力。</p>
<p>30.尽量避免向客户端返回大数据量，若数据量过大，应该考虑相应需求是否合理。</p>
]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
  </entry>
  <entry>
    <title>如何优化数据库(概述)</title>
    <url>/3806922126.html</url>
    <content><![CDATA[<h3 id="面试被问到如何优化数据库"><a href="#面试被问到如何优化数据库" class="headerlink" title="面试被问到如何优化数据库"></a>面试被问到如何优化数据库</h3><p>可以从不同层面进行回答，例如：</p>
<p>（1）、根据服务层面：配置mysql性能优化参数；</p>
<p>（2）、从系统层面增强mysql的性能：优化数据表结构、字段类型、字段索引、分表，分库、读写分离等等。</p>
<a id="more"></a>
<p>（3）、从数据库层面增强性能：优化SQL语句，合理使用字段索引。</p>
<p>（4）、从代码层面增强性能：使用缓存和NoSQL数据库方式存储，如MongoDB/Memcached/Redis来缓解高并发下数据库查询的压力。</p>
<p>（5）、减少数据库操作次数，尽量使用数据库访问驱动的批处理方法。</p>
<p>（6）、不常使用的数据迁移备份，避免每次都在海量数据中去检索。</p>
<p>（7）、提升数据库服务器硬件配置，或者搭建数据库集群。</p>
<p>（8）、编程手段防止SQL注入：使用JDBC PreparedStatement按位插入或查询；正则表达式过滤（非法字符串过滤）</p>
]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
  </entry>
  <entry>
    <title>PO BO VO DTO POJO DAO DO在java中的概念及应用</title>
    <url>/4237283706.html</url>
    <content><![CDATA[<h3 id="PO-BO-VO-DTO-POJO-DAO-DO这些Java中的概念分别指一些什么？"><a href="#PO-BO-VO-DTO-POJO-DAO-DO这些Java中的概念分别指一些什么？" class="headerlink" title="PO BO VO DTO POJO DAO DO这些Java中的概念分别指一些什么？"></a>PO BO VO DTO POJO DAO DO这些Java中的概念分别指一些什么？</h3><p>有面试官问道该问题，知道部分概念，现进行全面的总结。</p>
<h4 id="缩写的含义"><a href="#缩写的含义" class="headerlink" title="缩写的含义"></a>缩写的含义</h4><h5 id="PO"><a href="#PO" class="headerlink" title="PO"></a>PO</h5><p>是 Persistant Object 的缩写，用于表示数据库中的一条记录映射成的 java 对象。PO 仅仅用于表示数据，没有任何数据操作。通常遵守 Java Bean 的规范，拥有 getter/setter 方法。</p>
<a id="more"></a>
<h5 id="DAO"><a href="#DAO" class="headerlink" title="DAO"></a>DAO</h5><p>是 Data Access Object 的缩写，用于表示一个数据访问对象。使用 DAO 访问数据库，包括插入、更新、删除、查询等操作，与 PO 一起使用。DAO 一般在持久层，完全封装数据库操作，对外暴露的方法使得上层应用不需要关注数据库相关的任何信息。</p>
<h5 id="VO"><a href="#VO" class="headerlink" title="VO"></a>VO</h5><p>是 Value Object 的缩写，用于表示一个与前端进行交互的 java 对象。有的朋友也许有疑问，这里可不可以使用 PO 传递数据？实际上，这里的 VO 只包含前端需要展示的数据即可，对于前端不需要的数据，比如数据创建和修改的时间等字段，出于减少传输数据量大小和保护数据库结构不外泄的目的，不应该在 VO 中体现出来。通常遵守 Java Bean 的规范，拥有 getter/setter 方法。</p>
<h5 id="DTO"><a href="#DTO" class="headerlink" title="DTO"></a>DTO</h5><p>是 Data Transfer Object 的缩写，用于表示一个数据传输对象。DTO 通常用于不同服务或服务不同分层之间的数据传输。DTO 与 VO 概念相似，并且通常情况下字段也基本一致。但 DTO 与 VO 又有一些不同，这个不同主要是设计理念上的，比如 API 服务需要使用的 DTO 就可能与 VO 存在差异。通常遵守 Java Bean 的规范，拥有 getter/setter 方法。</p>
<h5 id="BO"><a href="#BO" class="headerlink" title="BO"></a>BO</h5><p>是 Business Object 的缩写，用于表示一个业务对象。BO 包括了业务逻辑，常常封装了对 DAO、RPC 等的调用，可以进行 PO 与 VO/DTO 之间的转换。BO 通常位于业务层，要区别于直接对外提供服务的服务层：BO 提供了基本业务单元的基本业务操作，在设计上属于被服务层业务流程调用的对象，一个业务流程可能需要调用多个 BO 来完成。</p>
<h5 id="POJO"><a href="#POJO" class="headerlink" title="POJO"></a>POJO</h5><p>是 Plain Ordinary Java Object 的缩写，表示一个简单 java 对象。上面说的 PO、VO、DTO 都是典型的 POJO。而 DAO、BO 一般都不是 POJO，只提供一些调用方法。</p>
<h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><p>如图表示了各个Object在分层架构中承担的职责，它有效地控制了信息地传播。</p>
<p><img src="/images/100017.jpg" alt="avatar"></p>
]]></content>
      <categories>
        <category>基础</category>
      </categories>
  </entry>
  <entry>
    <title>Java实体类序列化id的作用</title>
    <url>/664034925.html</url>
    <content><![CDATA[<h3 id="Java序列化时候序列ID作用"><a href="#Java序列化时候序列ID作用" class="headerlink" title="Java序列化时候序列ID作用"></a>Java序列化时候序列ID作用</h3><p>简单来说，Java的序列化机制是通过在运行时判断类的serialVersionUID来验证版本一致性的。在进行反序列化时，JVM会把传来的字节流中的serialVersionUID与本地相应实体（类）的serialVersionUID进行比较，如果相同就认为是一致的，可以进行反序列化，否则就会出现序列化版本不一致的异常。</p>
<a id="more"></a>
<p>当实现java.io.Serializable接口的实体（类）没有显式地定义一个名为serialVersionUID，类型为long的变量时，Java序列化机制会根据编译的class自动生成一个serialVersionUID作序列化版本比较用，这种情况下，只有同一次编译生成的class才会生成相同的serialVersionUID 。</p>
<p>如果我们不希望通过编译来强制划分软件版本，即实现序列化接口的实体能够兼容先前版本，未作更改的类，就需要显式地定义一个名为serialVersionUID，类型为long的变量，不修改这个变量值的序列化实体都可以相互进行串行化和反串行化。</p>
<p>假设两年前我保存了某个类的一个对象，这两年来，我修改该类，删除了某个属性和增加了另外一个属性，两年后，我又去读取那个保存的对象，或有什么结果？未知！sun的jdk就会蒙了。为此，一个解决办法就是在类中增加版本后，每一次类的属性修改，都应该把版本号升级一下，这样，在读取时，比较存储对象时的版本号与当前类的版本号，如果不一致，则直接报版本号不同的错!</p>
<p>异常场景回忆如下：</p>
<pre><code>环境：

1）分布式即多台resin服务器；

2）序列化的java类没有显示定义serialVersionUID；

3）使用了redis作为缓存（序列化后的对象存入redis缓存中）；</code></pre><p>在使用过程，报序列化时候版本不兼容，即serialVersionUID不同异常。由于不显示定义serialVersionUID，在分布式环境，相同类的class在不同resin在的class的serialVersionUID不同，反序列化就容易报此异常。</p>
<p>所以，在使用序列化过程中，最好显示定义序列号。</p>
]]></content>
      <categories>
        <category>基础</category>
      </categories>
  </entry>
  <entry>
    <title>ArrayList与LinkedList的区别</title>
    <url>/724537085.html</url>
    <content><![CDATA[<h3 id="ArrayList-与-LinkedList-区别"><a href="#ArrayList-与-LinkedList-区别" class="headerlink" title="ArrayList 与 LinkedList 区别"></a>ArrayList 与 LinkedList 区别</h3><p>一，是否线程安全：都是不同步的，也就是不保证线程安全；</p>
<p>二，底层数据结构：ArrayList 底层使用的是 Object[] 数组；LinkedList 使用的是双向链表数据结构（JDK1.6前为双向循环链表，JDK1.7后取消了循环）；</p>
<a id="more"></a>
<p>三，插入删除影响：① ArrayList 采用数组存储，因此插入和删除元素的时间复杂度都受元素位置的影响。比如：执行 add(E e) 方法的时候， ArrayList 会默认将该元素追加到此列表的末尾，这种情况的时间复杂度就是 O(1)。但是如果要在指定位置 i 插入和删除元素的话（add(int index, E element)），那么时间复杂度就为 O(n-i)，因为在进行上述操作的时候，集合中第 i 和第 i 个元素之后的 (n-i) 个元素，都要执行向后/前位移一位的操作。 ② LinkedList 采用的是链表存储，所以插入、删除元素时间复杂度不受元素位置的影响，都是近似 O(1)，而数组为近似 O(n)；</p>
<p>四，快速随机访问： LinkedList 不支持高效的随机元素访问，而 ArrayList 支持，所谓快速随机访问，是通过元素的序号来定位元素对象 (对应get(int index)方法)；</p>
<p>五，内存空间占用： ArrayList 的空间浪费，主要体现在 List 列表的结尾，会预留一定的空间容量；而 LinkedList 的空间花费，则体现在它的每一个元素，都需要消耗比 ArrayList 更多的空间，这是由于直接后继和直接前驱的存在。</p>
<p>关于 RandomAccess 接口</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public interface RandomAccess &#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>查看源码可以看到， RandomAccess 接口中什么都没有定义。所以，这就是个标识接口，标识那些实现了这个接口的类，具有随机访问的功能。</p>
<p>在 binarySearch() 方法中，它要判断传入的 List 是否是 RamdomAccess 的实例。如果是，则调用 indexedBinarySearch() 方法；如果不是，那么就调用 iteratorBinarySearch() 方法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public static &lt;T&gt; int binarySearch(List&lt;? extends Comparable&lt;? super T&gt;&gt; list, T key) &#123;</span><br><span class="line">    if (list instanceof RandomAccess || list.size()&lt;BINARYSEARCH_THRESHOLD)</span><br><span class="line">        return Collections.indexedBinarySearch(list, key);</span><br><span class="line">    else</span><br><span class="line">        return Collections.iteratorBinarySearch(list, key);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>ArrayList 实现了 RandomAccess 接口， 而 LinkedList 没有实现。为什么呢？还是和底层数据结构有关。ArrayList 底层是数组，而 LinkedList 底层是链表。数组天然支持随机访问，时间复杂度为 O(1)，所以支持快速随机访问。而链表需要遍历到特定的位置，才能访问特定位置的元素，时间复杂度为 O(n)，所以不是快速高效的随机访问。</p>
<p>ArrayList 实现了 RandomAccess 接口，表明了具有快速随机访问的功能。不过 RandomAccess 接口只是一个标识，并不是说 ArrayList 实现 RandomAccess 接口，才具有快速随机访问的功能！</p>
<p>总结 List 的遍历方式选择：</p>
<p>实现了 RandomAccess 接口的 list，优先选择普通 for 循环 ，其次 foreach；<br>未实现 RandomAccess 接口的 ist， 优先选择 iterator 遍历（foreach 遍历底层也是通过 iterator 实现的），大 size 的数据，千万不要使用普通 for 循环。<br>关于双向链表和双向循环链表<br>双向链表</p>
<p>双向链表： 包含两个指针，一个 prev 指针指向前一个节点（第一个 prev 指向 nil），一个 next 指针指向后一个节点（最后一个 next 同样指向 nil）；</p>
<p>双向循环链表： 最后一个节点的 next 指针指向第一个节点的 head，而第一个节点的 head 的 prev 指针指向最后一个节点，所以是构成一个循环。</p>
]]></content>
      <categories>
        <category>List</category>
      </categories>
  </entry>
  <entry>
    <title>拦截器（Interceptor）与过滤器（Filter）的区别</title>
    <url>/1588495736.html</url>
    <content><![CDATA[<h3 id="拦截器（Interceptor）与过滤器（Filter）的区别"><a href="#拦截器（Interceptor）与过滤器（Filter）的区别" class="headerlink" title="拦截器（Interceptor）与过滤器（Filter）的区别"></a>拦截器（Interceptor）与过滤器（Filter）的区别</h3><h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><p>过滤器，是在java web中将你传入的request、response提前过滤掉一些信息，或者提前设置一些参数。然后再传入Servlet或Struts2的 action进行业务逻辑处理。比如过滤掉非法url（不是login.do的地址请求，如果用户没有登陆都过滤掉），或者在传入Servlet或Struts2的action前统一设置字符集，或者去除掉一些非法字符。</p>
<p>拦截器，是面向切面编程（AOP，Aspect Oriented Program）的。就是在你的Service或者一个方法前调用一个方法，或者在方法后调用一个方法。比如动态代理就是拦截器的简单实现，在你调用方法前打印出字符串（或者做其它业务逻辑的操作），也可以在你调用方法后打印出字符串，甚至在你抛出异常的时候做业务逻辑的操作。</p>
<a id="more"></a>
<p>通俗理解：</p>
<p>（1）过滤器（Filter）：当你有一堆东西的时候，你只希望选择符合你要求的某一些东西。定义这些要求的工具，就是过滤器。（理解：就是一堆字母中取一个B）</p>
<p>（2）拦截器（Interceptor）：在一个流程正在进行的时候，你希望干预它的进展，甚至终止它进行，这是拦截器做的事情。（理解：就是一堆字母中，干预它，通过验证的少点，顺便干点别的东西）</p>
<h4 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h4><p>主要区别：<br>拦截器是基于Java的反射机制的，而过滤器是基于函数回调。<br>拦截器不依赖于servlet容器，过滤器依赖于servlet容器。<br>拦截器只能对action请求起作用，而过滤器则可以对几乎所有的请求起作用。<br>拦截器可以访问action上下文、值栈里的对象，而过滤器不能访问。<br>在action的生命周期中，拦截器可以多次被调用，而过滤器只能在容器初始化时被调用一次<br>拦截器可以获取IOC容器中的各个bean，而过滤器就不行，这点很重要，在拦截器里注入一个service，可以调用业务逻辑。</p>
<p>本质区别：<br>从灵活性上说拦截器功能更强大些，Filter能做的事情它都能做，而且可以在请求前，请求后执行，比较灵活。Filter主要是针对URL地址做一个编码的事情、过滤掉没用的参数、安全校验（比较泛的，比如登录不登录之类），太细的活儿还是建议用interceptor。具体还得根据不同情况选择合适的。<br><img src="/images/100020.jpeg" alt="avatar"></p>
<h4 id="执行顺序-："><a href="#执行顺序-：" class="headerlink" title="执行顺序 ："></a>执行顺序 ：</h4><p>过滤前—–拦截前—–Action处理—–拦截后—–过滤后。</p>
<p>个人认为过滤是一个横向的过程，首先把客户端提交的内容进行过滤（例如未登录用户不能访问内部页面的处理）；过滤通过后，拦截器将进行用户提交数据的验证，做一些前期的数据处理；接着把处理后的数据发给对应的Action，Action处理完成返回后，拦截器还可以做些其他事情，再向上返回到过滤器的后续操作。<br><img src="/images/100021.jpeg" alt="avatar"></p>
<p>过滤器（Filter）是在请求进入容器后，但还未进入Servlet之前进行预处理的。请求结束返回也是，是在Servlet处理完后，返回给前端之前。所以过滤器（Filter）的doFilter(ServletRequest request, ServletResponse response, FilterChain chain<br>)的入参是ServletRequest ，而不是httpservletrequest。因为过滤器是在httpservlet之前。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">@Override</span><br><span class="line">public void init(FilterConfig arg0) throws ServletException &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"> @Override</span><br><span class="line">public void doFilter(ServletRequest req, ServletResponse res, FilterChain chain) throws IOException, ServletException &#123;</span><br><span class="line">        System.out.println(&quot;before...&quot;);</span><br><span class="line">        chain.doFilter(request, response);</span><br><span class="line">        System.out.println(&quot;after...&quot;);</span><br><span class="line"> &#125;</span><br><span class="line">@Override</span><br><span class="line">public void destroy() &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Filter跟Servlet一样都是由服务器负责创建和销毁的。</p>
<p>在web应用程序启动时，服务器会根据应用程序的web.xml文件中的配置信息调用public void init(FilterConfig filterConfig) throws ServletException方法来初始化Filter；在web应用程序被移除或者是服务器关闭时，会调用public void destroy()来销毁Filter。<br>在一个应用程序中一个Filter只会被创建和销毁一次。在初始化之后，Filter中声明了public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException方法，用来实现一些需要在拦截完成之后的业务逻辑。</p>
<p>注意到上面的doFilter()方法的参数中，有FilterChain chain这个参数，它是传递过来的拦截链对象，里面包含了用户定义的一系列的拦截器，这些拦截器根据其在web.xml中定义的顺序依次被执行。当用户的信息验证通过或者当前拦截器不起作用时，我们可以执行chain.doFilter(request, response); 方法来跳过当前拦截器来执行拦截器链中的下一个拦截器。chain.doFilter(request, response); 这个方法的调用作为分水岭。事实上调用Servlet的doService()方法是在chain.doFilter(request, response); 这个方法中进行的。</p>
<h4 id="拦截器与过滤器使用场景："><a href="#拦截器与过滤器使用场景：" class="headerlink" title="拦截器与过滤器使用场景："></a>拦截器与过滤器使用场景：</h4><p>SpringMVC的处理器拦截器类似于Servlet开发中的过滤器Filter，用于对处理器进行预处理和后处理。</p>
<p>1、日志记录：记录请求信息的日志，以便进行信息监控、信息统计、计算PV（Page View）等。<br>2、权限检查：如登录检测，进入处理器检测检测是否登录，如果没有直接返回到登录页面；<br>3、性能监控：有时候系统在某段时间莫名其妙的慢，可以通过拦截器在进入处理器之前记录开始时间，在处理完后记录结束时间，从而得到该请求的处理时间（如果有反向代理，如apache可以自动记录）；<br>4、通用行为：读取cookie得到用户信息并将用户对象放入请求，从而方便后续流程使用，还有如提取Locale、Theme信息等，只要是多个处理器都需要的即可使用拦截器实现。<br>5、OpenSessionInView：如hibernate，在进入处理器打开Session，在完成后关闭Session。</p>
<h4 id="补充说明："><a href="#补充说明：" class="headerlink" title="补充说明："></a>补充说明：</h4><p>Spring的拦截器与Servlet的Filter有相似之处，比如二者都是AOP编程思想的体现，都能实现权限检查、日志记录等。</p>
<p>不同的是：<br>使用范围不同：Filter是Servlet规范规定的，只能用于Web程序中。而拦截器既可以用于Web程序，也可以用于Application、Swing程序中。</p>
<p>规范不同：Filter是在Servlet规范中定义的，是Servlet容器支持的。而拦截器是在Spring容器内的，是Spring框架支持的。</p>
<p>使用的资源不同：同其他的代码块一样，拦截器也是一个Spring的组件，归Spring管理，配置在Spring文件中，因此能使用Spring里的任何资源、对象，例如Service对象、数据源、事务管理等，通过IOC注入到拦截器即可；而Filter则不能。</p>
<p>深度不同：Filter在只在Servlet前后起作用。而拦截器能够深入到方法前后、异常抛出前后等，因此拦截器的使用具有更大的弹性。所以在Spring构架的程序中，要优先使用拦截器。<br>实际上Filter和Servlet极其相似，区别只是Filter不能直接对用户生成响应r里。实际上FiltedoFilter()方法里的代码就是从多个Servlet的service()方法里抽取的通用代码，通过使用Filter可以实现更好的复用。</p>
<p>Filter是一个可以复用的代码片段，可以用来转换Http请求、响应和头信息。Filter不像Servlet，它不能产生一个请求或者响应，它只是修改对某一资源的请求，或者修改从某一资源的响应。</p>
<p>JSR中说明的是，按照多个匹配的Filter，是按照其在web.xml中配置的顺序来执行的。所以这也就是，把自己的Filter或者其他的Filter（比如UrlRewrite的Filter）放在Struts2的 DispatcherFilter的前面的原因。因为它们需要在请求被Struts2框架处理之前，做一些前置的工作。<br>当Filter被调用，并且进入了Struts2的DispatcherFilter中后，Struts2会按照在Action中配置的Interceptor Stack中的Interceptor的顺序，来调用Interceptor。</p>
]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title>线程的五大状态</title>
    <url>/1776674258.html</url>
    <content><![CDATA[<h4 id="线程的五大状态"><a href="#线程的五大状态" class="headerlink" title="线程的五大状态"></a>线程的五大状态</h4><p>线程从创建、运行到结束总是处于下面五个状态之一：新建状态、就绪状态、运行状态、阻塞状态及死亡状态。<br><img src="/images/100019.png" alt="avatar"></p>
<a id="more"></a>
<h5 id="1-新建状态"><a href="#1-新建状态" class="headerlink" title="1.新建状态"></a>1.新建状态</h5><p>当用new操作符创建一个线程时。此时程序还没有开始运行线程中的代码。</p>
<h5 id="2-就绪状态"><a href="#2-就绪状态" class="headerlink" title="2.就绪状态"></a>2.就绪状态</h5><p>一个新创建的线程并不自动开始运行，要执行线程，必须调用线程的start()方法。当线程对象调用start()方法即启动了线程，start()方法创建线程运行的系统资源，并等待调度线程调度。当start()方法返回后，线程就处于就绪状态。</p>
<p>处于就绪状态的线程并不一定立即运行run()方法，线程还必须同其他线程竞争CPU时间，只有获得CPU时间才可以运行线程。因为在单CPU的计算机系统中，不可能同时运行多个线程，一个时刻仅有一个线程处于运行状态。因此此时可能有多个线程处于就绪状态。对多个处于就绪状态的线程是由Java运行时系统的线程调度程序来调度的。</p>
<p>当线程对象调用start()方法之后，该线程就处于就绪状态，处于这个状态的线程并没有开始运行，只是表示该线程可以运行了。至于该线程何时开始运行，取决于JVM里线程调度器的调度。</p>
<h5 id="3-运行状态（running）"><a href="#3-运行状态（running）" class="headerlink" title="3.运行状态（running）"></a>3.运行状态（running）</h5><p>当线程获得CPU时间后，它才进入运行状态，真正开始执行run()方法。</p>
<h5 id="4-阻塞状态（blocked）"><a href="#4-阻塞状态（blocked）" class="headerlink" title="4.阻塞状态（blocked）"></a>4.阻塞状态（blocked）</h5><p>线程运行过程中，可能由于各种原因进入阻塞状态：<br>①线程通过调用sleep方法进入睡眠状态；<br>②线程调用一个在I/O上被阻塞的操作，即该操作在输入输出操作完成之前不会返回到它的调用者；<br>③线程试图得到一个锁，而该锁正被其他线程持有；<br>④线程在等待某个触发条件；<br>所谓阻塞状态是正在运行的线程没有运行结束，暂时让出CPU，这时其他处于就绪状态的线程就可以获得CPU时间，进入运行状态。</p>
<h5 id="5-死亡状态（dead）"><a href="#5-死亡状态（dead）" class="headerlink" title="5.死亡状态（dead）"></a>5.死亡状态（dead）</h5><p>有两个原因会导致线程死亡：<br>①run方法正常退出而自然死亡；<br>②一个未捕获的异常终止了run方法而使线程猝死；</p>
<p>为了确定线程在当前是否存活着（就是要么是可运行的，要么是被阻塞了），需要使用isAlive方法，如果是可运行或被阻塞，这个方法返回true；如果线程仍旧是new状态且不是可运行的，或者线程死亡了，则返回false。</p>
]]></content>
      <categories>
        <category>多线程</category>
      </categories>
  </entry>
  <entry>
    <title>SQL 优化总结</title>
    <url>/2939935991.html</url>
    <content><![CDATA[<h2 id="Sql-调优总结"><a href="#Sql-调优总结" class="headerlink" title="Sql 调优总结"></a>Sql 调优总结</h2><p><em>转自 ： <a href="https://www.cnblogs.com/swanyf/p/10946948.html" target="_blank" rel="noopener">https://www.cnblogs.com/swanyf/p/10946948.html</a></em></p>
<h3 id="sql语句优化"><a href="#sql语句优化" class="headerlink" title="sql语句优化"></a>sql语句优化</h3><p>性能不理想的系统中除了一部分是因为应用程序的负载确实超过了服务器的实际处理能力外,更多的是因为系统存在大量的SQL语句需要优化。<br>为了获得稳定的执行性能，SQL语句越简单越好。对复杂的SQL语句，要设法对之进行简化。</p>
<p>常见的简化规则如下：</p>
<ol>
<li>不要有超过5个以上的表连接（JOIN）</li>
<li>考虑使用临时表或表变量存放中间结果。</li>
<li>少用子查询</li>
<li>视图嵌套不要过深,一般视图嵌套不要超过2个为宜。</li>
</ol>
<a id="more"></a>
<p>连接的表越多，其编译的时间和连接的开销也越大，性能越不好控制。<br>最好是把连接拆开成较小的几个部分逐个顺序执行。<br>优先执行那些能够大量减少结果的连接。</p>
<p>拆分的好处不仅仅是减少SQL Server优化的时间，更使得SQL语句能够以你可以预测的方式和顺序执行。<br>如果一定需要连接很多表才能得到数据，那么很可能意味着设计上的缺陷。</p>
<p>外连接是outer join，非常不好。因为outer join意味着必须对左表或右表查询所有行。<br>如果表很大而没有相应的where语句，那么outer join很容易导致table scan或index scan。<br>要尽量使用inner join避免scan整个表。</p>
<h4 id="优化建议："><a href="#优化建议：" class="headerlink" title="优化建议："></a>优化建议：</h4><ol>
<li><p>限制结果集</p>
<p>要尽量减少返回的结果行，包括行数和字段列数。<br>返回的结果越大，意味着相应的SQL语句的logical reads 就越大，对服务器的性能影响就越甚。<br>一个很不好的设计就是返回表的所有数据：</p>
<p>Select * from tablename</p>
<p>即使表很小也会导致并发问题。更坏的情况是，如果表有上百万行的话，那后果将是灾难性的。<br>它不但可能带来极重的磁盘IO，更有可能把数据库缓冲区中的其他缓存数据挤出，使得这些数据下次必须再从磁盘读取。<br>必须设计良好的SQL语句，使得其有where语句或TOP语句来限制结果集大小。</p>
</li>
</ol>
<ol start="2">
<li><p>合理的表设计</p>
<p>SQL Server 2005将支持表分区技术。利用表分区技术可以实现数据表的流动窗口功能。<br>在流动窗口中可以轻易的把历史数据移出，把新的数据加入，从而使表的大小基本保持稳定。<br>另外，表的设计未必需要非常范式化。有一定的字段冗余可以增加SQL语句的效率，减少JOIN的数目，提高语句的执行速度。</p>
</li>
</ol>
<ol start="3">
<li><p>OLAP和OLTP模块要分开</p>
<p>OLAP和OLTP类型的语句是截然不同的。前者往往需要扫描整个表做统计分析，索引对这样的语句几乎没有多少用处。<br>索引只能够加快那些如sum，group by之类的聚合运算。因为这个原因，几乎很难对OLAP类型的SQL语句进行优化。<br>而OLTP语句则只需要访问表的很小一部分数据，而且这些数据往往可以从内存缓存中得到。<br>为了避免OLAP 和OLTP语句相互影响，这两类模块需要分开运行在不同服务器上。<br>因为OLAP语句几乎都是读取数据，没有更新和写入操作，所以一个好的经验是配置一台standby 服务器，然后OLAP只访问standby服务器。</p>
</li>
</ol>
<ol start="4">
<li><p>使用存储过程</p>
<p>可以考虑使用存储过程封装那些复杂的SQL语句或商业逻辑，这样做有几个好处。<br>一是存储过程的执行计划可以被缓存在内存中较长时间，减少了重新编译的时间。<br>二是存储过程减少了客户端和服务器的繁复交互。<br>三是如果程序发布后需要做某些改变你可以直接修改存储过程而不用修改程序，避免需要重新安装部署程序。</p>
</li>
</ol>
<h3 id="索引优化"><a href="#索引优化" class="headerlink" title="索引优化"></a>索引优化</h3><p>很多数据库系统性能不理想是因为系统没有经过整体优化，存在大量性能低下的SQL 语句。</p>
<p>这类SQL语句性能不好的首要原因是缺乏高效的索引。</p>
<p>没有索引除了导致语句本身运行速度慢外，更是导致大量的磁盘读写操作，使得整个系统性能都受之影响而变差。</p>
<p>解决这类系统的首要办法是优化这些没有索引或索引不够好的SQL语句。</p>
<h4 id="创建索引的关键"><a href="#创建索引的关键" class="headerlink" title="创建索引的关键"></a>创建索引的关键</h4><p>优化SQL语句的关键是尽可能减少语句的logical reads（逻辑读）。</p>
<p>这里说的logical reads是指语句执行时需要访问的单位为8K的数据页总数。</p>
<p>logical reads 越少，其需要的内存和CPU时间也就越少，语句执行速度就越快。</p>
<p>不言而喻，索引的最大好处是它可以极大减少SQL语句的logical reads数目，从而极大减少语句的执行时间。</p>
<p>创建索引的关键是索引要能够大大减少语句的logical reads。一个索引好不好，主要看它减少的logical reads多不多。</p>
<p>运行set statistics io命令可以得到SQL语句的logical reads信息。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set statistics io on</span><br><span class="line">select au_id,au_lname ,au_fname</span><br><span class="line">from pubs..authors where au_lname =&apos;Green&apos;</span><br><span class="line">set statistics io on</span><br></pre></td></tr></table></figure>
<p>如果Logical reads很大，而返回的行数很少，也即两者相差较大，那么往往意味者语句需要优化。</p>
<p>Logical reads中包含该语句从内存数据缓冲区中访问的页数和从物理磁盘读取的页数。</p>
<p>而physical reads(物理读)表示那些没有驻留在内存缓冲区中需要从磁盘读取的数据页。</p>
<p>Read-ahead reads是SQL Server为了提高性能而产生的预读。预读可能会多读取一些数据。</p>
<p>优化的时候我们主要关注Logical Reads就可以了。</p>
<p>注意如果physical Reads或Read-ahead reads很大，那么往往意味着语句的执行时间（duration）里面会有一部分耗费在等待物理磁盘IO上。</p>
<h4 id="单字段索引，组合索引和覆盖索引"><a href="#单字段索引，组合索引和覆盖索引" class="headerlink" title="单字段索引，组合索引和覆盖索引"></a>单字段索引，组合索引和覆盖索引</h4><p>单字段索引是指只有一个字段的索引，而组合索引指有多个字段构成的索引。</p>
<h5 id="对出现在where子句中的字段加索引"><a href="#对出现在where子句中的字段加索引" class="headerlink" title="对出现在where子句中的字段加索引"></a>对出现在where子句中的字段加索引</h5><p>set statistics profile命令将输出语句的执行计划。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set statistics profile on</span><br><span class="line">set statistics io on</span><br><span class="line">go</span><br><span class="line">select .... from tb where ...</span><br><span class="line">go</span><br><span class="line">set statistics profile off</span><br><span class="line">set statistics io off</span><br></pre></td></tr></table></figure>
<p>也许你会问，为什么不用SET SHOWPLAN_ALL呢？使用SET SHOWPLAN_ALL也是可以的。</p>
<p>不过set statistics profile输出的是SQL 语句的运行时候真正使用的执行计划，</p>
<p>而SET SHOWPLAN_ALL输出的是预计（Estimate）的执行计划。</p>
<p>使用SET SHOWPLAN_ALL是后面的语句并不会真正运行。</p>
<p>用了Table Scan，也就是对整个表进行了全表扫描。全表扫描的性能通常是很差的，要尽量避免。</p>
<p>如果上面的select语句是数据库系统经常运行的关键语句， 那么应该对它创建相应的索引。</p>
<p>创建索引的技巧之一是对经常出现在where条件中的字段创建索引</p>
<p>Table Scan也变成了Index Seek，性能极大提高</p>
<p>设法避免Table scan或Index scan是优化SQL 语句使用的常用技巧。通常Index Seek需要的logical reads比前两者要少得多。</p>
<h5 id="组合索引"><a href="#组合索引" class="headerlink" title="组合索引"></a>组合索引</h5><p>如果where语句中有多个字段，那么可以考虑创建组合索引。</p>
<p>组合索引中字段的顺序是非常重要的，越是唯一的字段越是要靠前。</p>
<p>另外，无论是组合索引还是单个列的索引，尽量不要选择那些唯一性很低的字段。</p>
<p>比如说，在只有两个值0和1的字段上建立索引没有多大意义。</p>
<p>所以如果对单字段进行索引，建议使用set statistics profile来验证索引确实被充分使用。logical reads越少的索引越好。</p>
<h5 id="覆盖索引"><a href="#覆盖索引" class="headerlink" title="覆盖索引"></a>覆盖索引</h5><p>覆盖索引能够使得语句不需要访问表仅仅访问索引就能够得到所有需要的数据。</p>
<p>因为聚集索引叶子节点就是数据所以无所谓覆盖与否，所以覆盖索引主要是针对非聚集索引而言。</p>
<p>执行计划中除了index seek外，还有一个Bookmark Lookup关键字。</p>
<p>Bookmark Lookup表示语句在访问索引后还需要对表进行额外的Bookmark Lookup操作才能得到数据。</p>
<p>也就是说为得到一行数据起码有两次IO，一次访问索引，一次访问基本表。</p>
<p>如果语句返回的行数很多，那么Bookmark Lookup操作的开销是很大的。</p>
<p>覆盖索引能够避免昂贵的Bookmark Lookup操作，减少IO的次数，提高语句的性能。</p>
<p>覆盖索引需要包含select子句和WHERE子句中出现的所有字段。Where语句中的字段在前面，select中的在后面。</p>
<p>logical reads，是大大减少了。Bookmark Lookup操作也消失了。所以创建覆盖索引是减少logical reads提升语句性能的非常有用的优化技巧。</p>
<h5 id="关于索引"><a href="#关于索引" class="headerlink" title="关于索引"></a>关于索引</h5><p>实际上索引的创建原则是比较复杂的。有时候你无法在索引中包含了Where子句中所有的字段。</p>
<p>在考虑索引是否应该包含一个字段时，应考虑该字段在语句中的作用。</p>
<p>比如说如果经常以某个字段作为where条件作精确匹配返回很少的行，那么就绝对值得为这个字段建立索引。</p>
<p>再比如说，对那些非常唯一的字段如主键和外键，经常出现在group by，order by中的字段等等都值得创建索引。</p>
<p>问题1，是否值得在identity字段上建立聚集索引。</p>
<p>答案取决于identity 字段如何在语句中使用。如果你经常根据该字段搜索返回很少的行，那么在其上建立索引是值得的。</p>
<p>反之如果identity字段根本很少在语句中使用，那么就不应该对其建立任何索引。</p>
<p>问题2，一个表应该建立多少索引合适。</p>
<p>如果表的80％以上的语句都是读操作，那么索引可以多些。但是不要太多。</p>
<p>特别是不要对那些更新频繁的表其建立很多的索引。很少表有超过5个以上的索引。</p>
<p>过多的索引不但增加其占用的磁盘空间，也增加了SQL Server 维护索引的开销。</p>
<p>问题4：为什么SQL Server 在执行计划中没有使用你认为应该使用的索引？原因是多样的。</p>
<p>一种原因是该语句返回的结果超过了表的20％数据，使得SQL Server 认为scan比seek更有效。</p>
<p>另一种原因可能是表字段的statistics过期了，不能准确反映数据的分布情况。</p>
<p>你可以使用命令UPDATE STATISTICS tablename with FULLSCAN来更新它。</p>
<p>只有同步的准确的statistics才能保证SQL Server 产生正确的执行计划。</p>
<p>过时的老的statistics常会导致SQL Server生成不够优化的甚至愚蠢的执行计划。</p>
<p>所以如果你的表频繁更新，而你又觉得和之相关的SQL语句运行缓慢，不妨试试UPDATE STATISTIC with FULLSCAN 语句。</p>
<p>问题5、什么使用聚集索引，什么时候使用非聚集索引</p>
<p>在SQL Server 中索引有聚集索引和非聚集索引两种。它们的主要差别是前者的索引叶子就是数据本身，而后者的叶子节点包含的是指向数据的书签（即数据行号或聚集索引的key）。</p>
<p>对一个表而言聚集索引只能有一个，而非聚集索引可以有多个。</p>
<p>只是聚集索引没有Bookmark Lookup操作。</p>
<p>什么时候应该使用聚集索引?  什么时候使用非聚集索引? 取决于应用程序的访问模式。</p>
<p>我的建议是在那些关键的字段上使用聚集索引。一个表一般都需要建立一个聚集索引。</p>
<p>对于什么时候使用聚集索引，SQL Server 2000联机手册中有如下描述：</p>
<p>在创建聚集索引之前，应先了解您的数据是如何被访问的。可考虑将聚集索引用于：</p>
<p>包含大量非重复值的列。</p>
<p>使用下列运算符返回一个范围值的查询：BETWEEN、&gt;、&gt;=、&lt; 和 &lt;=。</p>
<p>被连续访问的列。</p>
<p>返回大型结果集的查询。</p>
<p>经常被使用联接或 GROUP BY 子句的查询访问的列；一般来说，这些是外键列。</p>
<p>对 ORDER BY 或 GROUP BY 子句中指定的列进行索引，可以使 SQL Server 不必对数据进行排序，因为这些行已经排序。这样可以提高查询性能。</p>
<p>OLTP 类型的应用程序，这些程序要求进行非常快速的单行查找（一般通过主键）。应在主键上创建聚集索引。</p>
<p>聚集索引不适用于：</p>
<p>频繁更改的列</p>
<p>这将导致整行移动（因为 SQL Server 必须按物理顺序保留行中的数据值）。这一点要特别注意，因为在大数据量事务处理系统中数据是易失的。</p>
<p>宽键</p>
<p>来自聚集索引的键值由所有非聚集索引作为查找键使用，因此存储在每个非聚集索引的叶条目内。</p>
<p>总结：</p>
<p>如何使一个性能缓慢的系统运行更快更高效，不但需要整体分析数据库系统，找出系统的性能瓶颈，更需要优化数据库系统发出的SQL 语句。</p>
<p>一旦找出关键的SQL 语句并加与优化，性能问题就会迎刃而解。</p>
<p><img src="/images/100018.jpg" alt="avatar"></p>
<p>《 数据库技术内幕 》</p>
<h4 id="处理百万级以上的数据提高查询速度的方法"><a href="#处理百万级以上的数据提高查询速度的方法" class="headerlink" title="处理百万级以上的数据提高查询速度的方法"></a>处理百万级以上的数据提高查询速度的方法</h4><p>1.应尽量避免在 where 子句中使用!=或&lt;&gt;操作符，否则将引擎放弃使用索引而进行全表扫描。</p>
<p>2.对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。</p>
<p>3.应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如：<br>    select id from t where num is null<br>    可以在num上设置默认值0，确保表中num列没有null值，然后这样查询：<br>    select id from t where num=0</p>
<p>4.应尽量避免在 where 子句中使用 or 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描，如：<br>    select id from t where num=10 or num=20<br>    可以这样查询：<br>    select id from t where num=10<br>    union all<br>    select id from t where num=20</p>
<p>5.下面的查询也将导致全表扫描：(不能前置百分号)<br>    select id from t where name like ‘%abc%’<br>   若要提高效率，可以考虑全文检索。</p>
<p>6.in 和 not in 也要慎用，否则会导致全表扫描，如：<br>    select id from t where num in(1,2,3)<br>    对于连续的数值，能用 between 就不要用 in 了：<br>    select id from t where num between 1 and 3</p>
<p>select xx,phone FROM send  a JOIN (<br>select ‘13891030091’ phone  union select ‘13992085916’ …………  UNION  SELECT ‘13619100234’ ) b<br> on  a.Phone=b.phone<br>–替代下面  很多数据隔开的时候<br>in(‘13891030091’,’13992085916’,’13619100234’…………)</p>
<p>7.如果在 where 子句中使用参数，也会导致全表扫描。因为SQL只有在运行时才会解析局部变量，但优化程序不能将访问计划的选择推迟到运行时；它必须在编译时进行选择。然 而，如果在编译时建立访问计划，变量的值还是未知的，因而无法作为索引选择的输入项。如下面语句将进行全表扫描：<br>    select id from t where num=@num     可以改为强制查询使用索引：<br>    select id from t with(index(索引名)) where num=@num</p>
<p>8.应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。如：<br>    select id from t where num/2=100<br>    应改为:<br>    select id from t where num=100*2</p>
<p>9.应尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。如：<br>    select id from t where substring(name,1,3)=’abc’–name以abc开头的id<br>    select id from t where datediff(day,createdate,’2005-11-30′)=0–’2005-11-30′生成的id<br>    应改为:<br>    select id from t where name like ‘abc%’<br>    select id from t where createdate&gt;=’2005-11-30′ and createdate&lt;’2005-12-1′</p>
<p>10.不要在 where 子句中的“=”左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引。</p>
<p>11.在使用索引字段作为条件时，如果该索引是复合索引，那么必须使用到该索引中的第一个字段作为条件时才能保证系统使用该索引，否则该索引将不会被使 用，并且应尽可能的让字段顺序与索引顺序相一致。</p>
<p>12.不要写一些没有意义的查询，如需要生成一个空表结构：<br>    select col1,col2 into #t from t where 1=0<br>    这类代码不会返回任何结果集，但是会消耗系统资源的，应改成这样：<br>    create table #t(…)</p>
<p>13.很多时候用 exists 代替 in 是一个好的选择：<br>    select num from a where num in(select num from b)<br>    用下面的语句替换：<br>    select num from a where exists(select 1 from b where num=a.num)</p>
<p>14.并不是所有索引对查询都有效，SQL是根据表中数据来进行查询优化的，当索引列有大量数据重复时，SQL查询可能不会去利用索引，如一表中有字段 sex，male、female几乎各一半，那么即使在sex上建了索引也对查询效率起不了作用。</p>
<p>15.索引并不是越多越好，索引固然可以提高相应的 select 的效率，但同时也降低了 insert 及 update 的效率，因为 insert 或 update 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。一个表的索引数最好不要超过6个，若太多则应考虑一些不常使用到的列上建的索引是否有 必要。</p>
<p>16.应尽可能的避免更新 clustered 索引数据列，因为 clustered 索引数据列的顺序就是表记录的物理存储顺序，一旦该列值改变将导致整个表记录的顺序的调整，会耗费相当大的资源。若应用系统需要频繁更新 clustered 索引数据列，那么需要考虑是否应将该索引建为 clustered 索引。</p>
<p>17.尽量使用数字型字段，若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。这是因为引擎在处理查询和连接时会 逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。</p>
<p>18.尽可能的使用 varchar/nvarchar 代替 char/nchar ，因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。</p>
<p>19.任何地方都不要使用 select * from t ，用具体的字段列表代替“*”，不要返回用不到的任何字段。</p>
<p>20.尽量使用表变量来代替临时表。如果表变量包含大量数据，请注意索引非常有限（只有主键索引）。</p>
<p>21.避免频繁创建和删除临时表，以减少系统表资源的消耗。</p>
<p>22.临时表并不是不可使用，适当地使用它们可以使某些例程更有效，例如，当需要重复引用大型表或常用表中的某个数据集时。但是，对于一次性事件，最好使 用导出表。</p>
<p>23.在新建临时表时，如果一次性插入数据量很大，那么可以使用 select into 代替 create table，避免造成大量 log ，以提高速度；如果数据量不大，为了缓和系统表的资源，应先create table，然后insert。</p>
<p>24.如果使用到了临时表，在存储过程的最后务必将所有的临时表显式删除，先 truncate table ，然后 drop table ，这样可以避免系统表的较长时间锁定。</p>
<p>25.尽量避免使用游标，因为游标的效率较差，如果游标操作的数据超过1万行，那么就应该考虑改写。</p>
<p>26.使用基于游标的方法或临时表方法之前，应先寻找基于集的解决方案来解决问题，基于集的方法通常更有效。</p>
<p>27.与临时表一样，游标并不是不可使用。对小型数据集使用 FAST_FORWARD 游标通常要优于其他逐行处理方法，尤其是在必须引用几个表才能获得所需的数据时。在结果集中包括“合计”的例程通常要比使用游标执行的速度快。如果开发时 间允许，基于游标的方法和基于集的方法都可以尝试一下，看哪一种方法的效果更好。</p>
<p>28.在所有的存储过程和触发器的开始处设置 SET NOCOUNT ON ，在结束时设置 SET NOCOUNT OFF 。无需在执行存储过程和触发器的每个语句后向客户端发送 DONE_IN_PROC 消息。</p>
<p>29.尽量避免向客户端返回大数据量，若数据量过大，应该考虑相应需求是否合理。</p>
<p>30.尽量避免大事务操作，提高系统并发能力。</p>
<h4 id="查询速度慢的原因"><a href="#查询速度慢的原因" class="headerlink" title="查询速度慢的原因"></a>查询速度慢的原因</h4><p>1、没有索引或者没有用到索引(这是查询慢最常见的问题，是程序设计的缺陷)</p>
<p>2、I/O吞吐量小，形成了瓶颈效应。</p>
<p>3、没有创建计算列导致查询不优化。</p>
<p>4、内存不足</p>
<p>5、网络速度慢</p>
<p>6、查询出的数据量过大（可以采用多次查询，其他的方法降低数据量）</p>
<p>7、锁或者死锁(这也是查询慢最常见的问题，是程序设计的缺陷)</p>
<p>8、sp_lock,sp_who,活动的用户查看,原因是读写竞争资源。</p>
<p>9、返回了不必要的行和列</p>
<p>10、查询语句不好，没有优化</p>
<h4 id="可以通过如下方法来优化查询"><a href="#可以通过如下方法来优化查询" class="headerlink" title="可以通过如下方法来优化查询"></a>可以通过如下方法来优化查询</h4><p>1、把数据、日志、索引放到不同的I/O设备上，增加读取速度，以前可以将Tempdb应放在RAID0上，SQL2000不在支持。数据量（尺寸）越大，提高I/O越重要.</p>
<p>2、纵向、横向分割表，减少表的尺寸(sp_spaceuse)</p>
<p>3、升级硬件</p>
<p>4、根据查询条件,建立索引,优化索引、优化访问方式，限制结果集的数据量。注意填充因子要适当（最好是使用默认值0）。索引应该尽量小，使用字节数小的列建索引好（参照索引的创建）,不要对有限的几个值的字段建单一索引如性别字段</p>
<p>5、提高网速;</p>
<p>6、扩大服务器的内存,Windows   2000和SQL   server   2000能支持4-8G的内存。配置虚拟内存：虚拟内存大小应基于计算机上并发运行的服务进行配置。运行   Microsoft   SQL   Server?   2000   时，可考虑将虚拟内存大小设置为计算机中安装的物理内存的   1.5   倍。如果另外安装了全文检索功能，并打算运行   Microsoft   搜索服务以便执行全文索引和查询，可考虑：将虚拟内存大小配置为至少是计算机中安装的物理内存的   3   倍。将   SQL   Server   max   server   memory   服务器配置选项配置为物理内存的   1.5   倍（虚拟内存大小设置的一半）。</p>
<p>7、增加服务器CPU个数;但是必须明白并行处理串行处理更需要资源例如内存。使用并行还是串行程是MsSQL自动评估选择的。单个任务分解成多个任务，就可以在处理器上运行。例如耽搁查询的排序、连接、扫描和GROUP   BY字句同时执行，SQL   SERVER根据系统的负载情况决定最优的并行等级，复杂的需要消耗大量的CPU的查询最适合并行处理。但是更新操作UPDATE,INSERT， DELETE还不能并行处理。</p>
<p>8、如果是使用like进行查询的话，简单的使用index是不行的，但是全文索引，耗空间。   like   ‘a%’   使用索引   like   ‘%a’   不使用索引用   like   ‘%a%’   查询时，查询耗时和字段值总长度成正比,所以不能用CHAR类型，而是VARCHAR。对于字段的值很长的建全文索引。</p>
<p>9、DB   Server   和APPLication   Server   分离；OLTP和OLAP分离</p>
<p>10、分布式分区视图可用于实现数据库服务器联合体。联合体是一组分开管理的服务器，但它们相互协作分担系统的处理负荷。这种通过分区数据形成数据库服务器联合体的机制能够扩大一组服务器，以支持大型的多层   Web   站点的处理需要。有关更多信息，参见设计联合数据库服务器。（参照SQL帮助文件’分区视图’）<br>    a、在实现分区视图之前，必须先水平分区表<br>    b、在创建成员表后，在每个成员服务器上定义一个分布式分区视图，并且每个视图具有相同的名称。这样，引用分布式分区视图名的查询可以在任何一个成员服务器上运行。系统操作如同每个成员服务器上都有一个原始表的复本一样，但其实每个服务器上只有一个成员表和一个分布式分区视图。数据的位置对应用程序是透明的。</p>
<p>11、重建索引   DBCC   REINDEX   ,DBCC   INDEXDEFRAG,收缩数据和日志   DBCC   SHRINKDB,DBCC   SHRINKFILE.   设置自动收缩日志.对于大的数据库不要设置数据库自动增长，它会降低服务器的性能。   在T-sql的写法上有很大的讲究，下面列出常见的要点：首先，DBMS处理查询计划的过程是这样的：<br>    1、   查询语句的词法、语法检查<br>    2、   将语句提交给DBMS的查询优化器<br>    3、   优化器做代数优化和存取路径的优化<br>    4、   由预编译模块生成查询规划<br>    5、   然后在合适的时间提交给系统处理执行<br>    6、   最后将执行结果返回给用户其次，看一下SQL   SERVER的数据存放的结构：一个页面的大小为8K(8060)字节，8个页面为一个盘区，按照B树存放。</p>
<p>12、Commit和rollback的区别   Rollback:回滚所有的事物。   Commit:提交当前的事物.   没有必要在动态SQL里写事物，如果要写请写在外面如：   begin   tran   exec(@s)   commit   trans   或者将动态SQL   写成函数或者存储过程。</p>
<p>13、在查询Select语句中用Where字句限制返回的行数,避免表扫描,如果返回不必要的数据，浪费了服务器的I/O资源，加重了网络的负担降低性能。如果表很大，在表扫描的期间将表锁住，禁止其他的联接访问表,后果严重。</p>
<p>14、SQL的注释申明对执行没有任何影响</p>
<p>15、尽可能不使用游标，它占用大量的资源。如果需要row-by-row地执行，尽量采用非光标技术,如：在客户端循环，用临时表，Table变量，用子查询，用Case语句等等。游标可以按照它所支持的提取选项进行分类：   只进   必须按照从第一行到最后一行的顺序提取行。FETCH   NEXT   是唯一允许的提取操作,也是默认方式。可滚动性   可以在游标中任何地方随机提取任意行。游标的技术在SQL2000下变得功能很强大，他的目的是支持循环。<br>有四个并发选项</p>
<ol>
<li>READ_ONLY：不允许通过游标定位更新(Update)，且在组成结果集的行中没有锁。</li>
<li>OPTIMISTIC   WITH   valueS:乐观并发控制是事务控制理论的一个标准部分。乐观并发控制用于这样的情形，即在打开游标及更新行的间隔中，只有很小的机会让第二个用户更新某一行。当某个游标以此选项打开时，没有锁控制其中的行，这将有助于最大化其处理能力。如果用户试图修改某一行，则此行的当前值会与最后一次提取此行时获取的值进行比较。如果任何值发生改变，则服务器就会知道其他人已更新了此行，并会返回一个错误。如果值是一样的，服务器就执行修改。   选择这个并发选项</li>
<li>OPTIMISTIC   WITH   ROW   VERSIONING:此乐观并发控制选项基于行版本控制。使用行版本控制，其中的表必须具有某种版本标识符，服务器可用它来确定该行在读入游标后是否有所更改。<br>在   SQL   Server   中，这个性能由   timestamp   数据类型提供，它是一个二进制数字，表示数据库中更改的相对顺序。每个数据库都有一个全局当前时间戳值：@@DBTS。每次以任何方式更改带有   timestamp   列的行时，SQL   Server   先在时间戳列中存储当前的   @@DBTS   值，然后增加   @@DBTS   的值。如果某   个表具有   timestamp   列，则时间戳会被记到行级。服务器就可以比较某行的当前时间戳值和上次提取时所存储的时间戳值，从而确定该行是否已更新。服务器不必比较所有列的值，只需比较   timestamp   列即可。如果应用程序对没有   timestamp   列的表要求基于行版本控制的乐观并发，则游标默认为基于数值的乐观并发控制。</li>
<li>SCROLL   LOCKS   这个选项实现悲观并发控制。在悲观并发控制中，在把数据库的行读入游标结果集时，应用程序将试图锁定数据库行。在使用服务器游标时，将行读入游标时会在其上放置一个更新锁。如果在事务内打开游标，则该事务更新锁将一直保持到事务被提交或回滚；当提取下一行时，将除去游标锁。如果在事务外打开游标，则提取下一行时，锁就被丢弃。因此，每当用户需要完全的悲观并发控制时，游标都应在事务内打开。更新锁将阻止任何其它任务获取更新锁或排它锁，从而阻止其它任务更新该行。<br>然而，更新锁并不阻止共享锁，所以它不会阻止其它任务读取行，除非第二个任务也在要求带更新锁的读取。滚动锁根据在游标定义的   SELECT   语句中指定的锁提示，这些游标并发选项可以生成滚动锁。滚动锁在提取时在每行上获取，并保持到下次提取或者游标关闭，以先发生者为准。下次提取时，服务器为新提取中的行获取滚动锁，并释放上次提取中行的滚动锁。滚动锁独立于事务锁，并可以保持到一个提交或回滚操作之后。如果提交时关闭游标的选项为关，则   COMMIT   语句并不关闭任何打开的游标，而且滚动锁被保留到提交之后，以维护对所提取数据的隔离。所获取滚动锁的类型取决于游标并发选项和游标   SELECT   语句中的锁提示。<br>锁提示   只读   乐观数值   乐观行版本控制   锁定无提示   未锁定   未锁定   未锁定   更新   NOLOCK   未锁定   未锁定   未锁定   未锁定   HOLDLOCK   共享   共享   共享   更新   UPDLOCK   错误   更新   更新   更新   TABLOCKX   错误   未锁定   未锁定   更新其它   未锁定   未锁定   未锁定   更新   *指定   NOLOCK   提示将使指定了该提示的表在游标内是只读的。</li>
</ol>
<p>16、用Profiler来跟踪查询，得到查询所需的时间，找出SQL的问题所在;用索引优化器优化索引</p>
<p>17、注意UNion和UNion   all   的区别。UNION   all好</p>
<p>18、注意使用DISTINCT，在没有必要时不要用，它同UNION一样会使查询变慢。重复的记录在查询里是没有问题的</p>
<p>19、查询时不要返回不需要的行、列</p>
<p>20、用sp_configure   ‘query   governor   cost   limit’或者SET   QUERY_GOVERNOR_COST_LIMIT来限制查询消耗的资源。当评估查询消耗的资源超出限制时，服务器自动取消查询,在查询之前就扼杀掉。 SET   LOCKTIME设置锁的时间</p>
<p>21、用select   top   100   /   10   Percent   来限制用户返回的行数或者SET   ROWCOUNT来限制操作的行</p>
<p>22、在SQL2000以前，一般不要用如下的字句 “IS   NULL”,   “ &lt;&gt; “,   “!=”,   “!&gt; “,   “! &lt;”,   “NOT”,   “NOT   EXISTS”,   “NOT   IN”,   “NOT   LIKE”,   and   “LIKE   ‘%500’”，因为他们不走索引全是表扫描。<br>也不要在WHere字句中的列名加函数，如Convert，substring等,如果必须用函数的时候，创建计算列再创建索引来替代.还可以变通写法：WHERE   SUBSTRING(firstname,1,1)   =   ‘m’改为WHERE   firstname   like   ‘m%’（索引扫描），一定要将函数和列名分开。并且索引不能建得太多和太大。<br>NOT   IN会多次扫描表，使用EXISTS、NOT   EXISTS   ，IN   ,   LEFT   OUTER   JOIN   来替代，特别是左连接,而Exists比IN更快，最慢的是NOT操作.如果列的值含有空，以前它的索引不起作用，现在2000的优化器能够处理了。相同的是IS   NULL，“NOT”,   “NOT   EXISTS”,   “NOT   IN”能优化她，而” &lt;&gt; ”等还是不能优化，用不到索引。</p>
<p>23、使用Query   Analyzer，查看SQL语句的查询计划和评估分析是否是优化的SQL。一般的20%的代码占据了80%的资源，我们优化的重点是这些慢的地方。</p>
<p>24、如果使用了IN或者OR等时发现查询没有走索引，使用显示申明指定索引：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT   *   FROM   PersonMember</span><br><span class="line">(INDEX   =   IX_Title)   WHERE   processid   IN   (&apos;男&apos;，&apos;女&apos;)</span><br></pre></td></tr></table></figure>

<p>25、将需要查询的结果预先计算好放在表中，查询的时候再SELECT。这在SQL7.0以前是最重要的手段。例如医院的住院费计算。</p>
<p>26、MIN()   和   MAX()能使用到合适的索引</p>
<p>27、数据库有一个原则是代码离数据越近越好，所以优先选择Default,依次为Rules,Triggers,   Constraint（约束如外健主健CheckUNIQUE……,数据类型的最大长度等等都是约束）,Procedure.这样不仅维护工作小，编写程序质量高，并且执行的速度快。</p>
<p>28、如果要插入大的二进制值到Image列，使用存储过程，千万不要用内嵌INsert来插入(不知JAVA是否)。因为这样应用程序首先将二进制值转换成字符串（尺寸是它的两倍），服务器受到字符后又将他转换成二进制值.存储过程就没有这些动作:   方法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Create   procedure   p_insert   as   insert   into   table(Fimage)   values   (@image)</span><br></pre></td></tr></table></figure>
<p>在前台调用这个存储过程传入二进制参数，这样处理速度明显改善。</p>
<p>29、Between在某些时候比IN速度更快,Between能够更快地根据索引找到范围。用查询优化器可见到差别。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select * from chineseresume where title in (&apos;男&apos;,&apos;女&apos;)</span><br><span class="line">Select * from chineseresume where between &apos;男&apos; and &apos;女&apos;</span><br></pre></td></tr></table></figure>
<p>是一样的。由于in会在比较多次，所以有时会慢些。</p>
<p>30、在必要是对全局或者局部临时表创建索引，有时能够提高速度，但不是一定会这样，因为索引也耗费大量的资源。他的创建同是实际表一样。</p>
<p>31、不要建没有作用的事物例如产生报表时，浪费资源。只有在必要使用事物时使用它。</p>
<p>32、用OR的字句可以分解成多个查询，并且通过UNION   连接多个查询。他们的速度只同是否使用索引有关,如果查询需要用到联合索引，用UNION   all执行的效率更高.多个OR的字句没有用到索引，改写成UNION的形式再试图与索引匹配。一个关键的问题是否用到索引。</p>
<p>33、尽量少用视图，它的效率低。对视图操作比直接对表操作慢,可以用stored   procedure来代替她。特别的是不要用视图嵌套,嵌套视图增加了寻找原始资料的难度。我们看视图的本质：它是存放在服务器上的被优化好了的已经产生了查询规划的SQL。对单个表检索数据时，不要使用指向多个表的视图，直接从表检索或者仅仅包含这个表的视图上读，否则增加了不必要的开销,查询受到干扰.为了加快视图的查询，MsSQL增加了视图索引的功能。</p>
<p>34、没有必要时不要用DISTINCT和ORDER   BY，这些动作可以改在客户端执行。它们增加了额外的开销。这同UNION   和UNION   ALL一样的道理。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT top 20 ad.companyname,comid,position,ad.referenceid,worklocation,</span><br><span class="line">convert(varchar(10),ad.postDate,120) as ostDate1,workyear,degreedescription</span><br><span class="line">FROM jobcn_query.dbo.COMPANYAD_query ad</span><br><span class="line">where referenceID in(&apos;JCNAD00329667&apos;,&apos;JCNAD132168&apos;,&apos;JCNAD00337748&apos;,&apos;JCNAD00338345&apos;,&apos;JCNAD00333138&apos;,&apos;JCNAD00303570&apos;,</span><br><span class="line">&apos;JCNAD00303569&apos;,&apos;JCNAD00303568&apos;,&apos;JCNAD00306698&apos;,&apos;JCNAD00231935&apos;,&apos;JCNAD00231933&apos;,&apos;JCNAD00254567&apos;,</span><br><span class="line">&apos;JCNAD00254585&apos;,&apos;JCNAD00254608&apos;,&apos;JCNAD00254607&apos;,&apos;JCNAD00258524&apos;,&apos;JCNAD00332133&apos;,&apos;JCNAD00268618&apos;,&apos;JCNAD00279196&apos;,&apos;JCNAD00268613&apos;)</span><br><span class="line">order   by   postdate   desc</span><br></pre></td></tr></table></figure>
<p>35、在IN后面值的列表中，将出现最频繁的值放在最前面，出现得最少的放在最后面，减少判断的次数</p>
<p>36、当用SELECT   INTO时，它会锁住系统表(sysobjects，sysindexes等等)，阻塞其他的连接的存取。创建临时表时用显示申明语句，而不是 select   INTO.   drop   table   t_lxh   begin   tran   select   *   into   t_lxh   from   chineseresume   where   name   =   ‘XYZ’   –commit   在另一个连接中SELECT   *   from   sysobjects可以看到   SELECT   INTO   会锁住系统表，Create   table   也会锁系统表(不管是临时表还是系统表)。所以千万不要在事物内使用它！！！这样的话如果是经常要用的临时表请使用实表，或者临时表变量。</p>
<p>37、一般在GROUP   BY   个HAVING字句之前就能剔除多余的行，所以尽量不要用它们来做剔除行的工作。他们的执行顺序应该如下最优：select   的Where字句选择所有合适的行，Group   By用来分组个统计行，Having字句用来剔除多余的分组。这样Group   By   个Having的开销小，查询快.对于大的数据行进行分组和Having十分消耗资源。如果Group   BY的目的不包括计算，只是分组，那么用Distinct更快</p>
<p>38、一次更新多条记录比分多次更新每次一条快,就是说批处理好</p>
<p>39、少用临时表，尽量用结果集和Table类性的变量来代替它,Table   类型的变量比临时表好</p>
<p>40、在SQL2000下，计算字段是可以索引的，需要满足的条件如下：</p>
<ol>
<li>计算字段的表达是确定的</li>
<li>不能用在TEXT,Ntext，Image数据类型</li>
<li>必须配制如下选项   ANSI_NULLS   =   ON,   ANSI_PADDINGS   =   ON,   …….</li>
</ol>
<p>41、尽量将数据的处理工作放在服务器上，减少网络的开销，如使用存储过程。存储过程是编译好、优化过、并且被组织到一个执行规划里、且存储在数据库中的 SQL语句，是控制流语言的集合，速度当然快。反复执行的动态SQL,可以使用临时存储过程，该过程（临时表）被放在Tempdb中。以前由于SQL   SERVER对复杂的数学计算不支持，所以不得不将这个工作放在其他的层上而增加网络的开销。SQL2000支持UDFs,现在支持复杂的数学计算，函数的返回值不要太大，这样的开销很大。用户自定义函数象光标一样执行的消耗大量的资源，如果返回大的结果采用存储过程</p>
<p>42、不要在一句话里再三的使用相同的函数，浪费资源,将结果放在变量里再调用更快</p>
<p>43、SELECT   COUNT(*)的效率教低，尽量变通他的写法，而EXISTS快.同时请注意区别：   select   count(Field   of   null)   from   Table   和   select   count(Field   of   NOT   null)   from   Table   的返回值是不同的。</p>
<p>44、当服务器的内存够多时，配制线程数量   =   最大连接数+5，这样能发挥最大的效率；否则使用   配制线程数量 &lt;最大连接数启用SQL   SERVER的线程池来解决,如果还是数量   =   最大连接数+5，严重的损害服务器的性能。</p>
<p>45、按照一定的次序来访问你的表。如果你先锁住表A，再锁住表B，那么在所有的存储过程中都要按照这个顺序来锁定它们。如果你（不经意的）某个存储过程中先锁定表B，再锁定表A，这可能就会导致一个死锁。如果锁定顺序没有被预先详细的设计好，死锁很难被发现</p>
<p>46、通过SQL   Server   Performance   Monitor监视相应硬件的负载   Memory:   Page   Faults   /   sec计数器如果该值偶尔走高，表明当时有线程竞争内存。如果持续很高，则内存可能是瓶颈。   Process:</p>
<pre><code>1. %   DPC   Time   指在范例间隔期间处理器用在缓延程序调用(DPC)接收和提供服务的百分比。(DPC   正在运行的为比标准间隔优先权低的间隔)。   由于   DPC   是以特权模式执行的，DPC   时间的百分比为特权时间   百分比的一部分。这些时间单独计算并且不属于间隔计算总数的一部   分。这个总数显示了作为实例时间百分比的平均忙时。
2. %Processor   Time计数器　如果该参数值持续超过95%，表明瓶颈是CPU。可以考虑增加一个处理器或换一个更快的处理器。
3. %   Privileged   Time   指非闲置处理器时间用于特权模式的百分比。(特权模式是为操作系统组件和操纵硬件驱动程序而设计的一种处理模式。它允许直接访问硬件和所有内存。另一种模式为用户模式，它是一种为应用程序、环境分系统和整数分系统设计的一种有限处理模式。操作系统将应用程序线程转换成特权模式以访问操作系统服务)。   特权时间的   %   包括为间断和   DPC   提供服务的时间。特权时间比率高可能是由于失败设备产生的大数量的间隔而引起的。这个计数器将平均忙时作为样本时间的一部分显示。
4. %   User   Time表示耗费CPU的数据库操作，如排序，执行aggregate   functions等。如果该值很高，可考虑增加索引，尽量使用简单的表联接，水平分割大表格等方法来降低该值。   Physical   Disk:   Curretn   Disk   Queue   Length计数器该值应不超过磁盘数的1.5~2倍。要提高性能，可增加磁盘。   SQLServer:Cache   Hit   Ratio计数器该值越高越好。如果持续低于80%，应考虑增加内存。   注意该参数值是从SQL   Server启动后，就一直累加记数，所以运行经过一段时间后，该值将不能反映系统当前值。</code></pre><p>47、分析select   emp_name   form   employee   where   salary   &gt;   3000   在此语句中若salary是Float类型的，则优化器对其进行优化为Convert(float,3000)，因为3000是个整数，我们应在编程时使用3000.0而不要等运行时让DBMS进行转化。同样字符和整型数据的转换。</p>
<p>–查找所有索引</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT  &apos;dbcc showcontig (&apos; + CONVERT(VARCHAR(20), i.id) + &apos;,&apos; + -- table id</span><br><span class="line">CONVERT(VARCHAR(20), i.indid) + &apos;)--&apos; + --index id</span><br><span class="line">OBJECT_NAME(i.id) + &apos;.&apos; + -- table name</span><br><span class="line">i.name--index name</span><br><span class="line">FROM    sysobjects o</span><br><span class="line">        INNER JOIN sysindexes i ON ( o.id = i.id )</span><br><span class="line">WHERE   o.type = &apos;U&apos;</span><br><span class="line">        AND i.indid &lt; 2</span><br><span class="line">        AND i.id = OBJECT_ID(o.name)</span><br><span class="line">ORDER BY OBJECT_NAME(i.id), i.indid</span><br></pre></td></tr></table></figure>
<p>–结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dbcc showcontig (2052202361,1)–AddBusiness.PK_AddBusiness</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
  </entry>
  <entry>
    <title>kafka入门：简介、使用场景、设计原理、主要配置及集群搭建</title>
    <url>/1376638572.html</url>
    <content><![CDATA[<h3 id="Kafka简介使用场景设计原理"><a href="#Kafka简介使用场景设计原理" class="headerlink" title="Kafka简介使用场景设计原理"></a>Kafka简介使用场景设计原理</h3><p><em>关于kafka说明可以参考：<a href="http://kafka.apache.org/documentation.html" target="_blank" rel="noopener">http://kafka.apache.org/documentation.html</a></em></p>
<p><em>文章转自：<a href="http://www.aboutyun.com/thread-9341-1-1.html" target="_blank" rel="noopener">http://www.aboutyun.com/thread-9341-1-1.html</a></em></p>
<h4 id="问题导读："><a href="#问题导读：" class="headerlink" title="问题导读："></a>问题导读：</h4><ol>
<li>zookeeper在kafka的作用是什么？  </li>
<li>kafka中几乎不允许对消息进行“随机读写”的原因是什么？</li>
<li>kafka集群consumer和producer状态信息是如何保存的？</li>
<li>partitions设计的目的的根本原因是什么？</li>
</ol>
<a id="more"></a>
<h4 id="一、入门"><a href="#一、入门" class="headerlink" title="一、入门"></a>一、入门</h4><h5 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h5><p>Kafka is a distributed,partitioned,replicated commit logservice。它提供了类似于JMS的特性，但是在设计实现上完全不同，此外它并不是JMS规范的实现。kafka对消息保存时根据Topic进行归类，发送消息者成为Producer,消息接受者成为Consumer,此外kafka集群有多个kafka实例组成，每个实例(server)成为broker。无论是kafka集群，还是producer和consumer都依赖于zookeeper来保证系统可用性集群保存一些meta信息。</p>
<p><img src="/images/100009.png" alt="avatar"></p>
<h5 id="2、Topics-logs"><a href="#2、Topics-logs" class="headerlink" title="2、Topics/logs"></a>2、Topics/logs</h5><p>一个Topic可以认为是一类消息，每个topic将被分成多个partition(区),每个partition在存储层面是append log文件。任何发布到此partition的消息都会被直接追加到log文件的尾部，每条消息在文件中的位置称为offset（偏移量），offset为一个long型数字，它是唯一标记一条消息。它唯一的标记一条消息。kafka并没有提供其他额外的索引机制来存储offset，因为在kafka中几乎不允许对消息进行“随机读写”。</p>
<p><img src="/images/100010.png" alt="avatar"></p>
<p>kafka和JMS（Java Message Service）实现(activeMQ)不同的是:即使消息被消费,消息仍然不会被立即删除.日志文件将会根据broker中的配置要求,保留一定的时间之后删除;比如log文件保留2天,那么两天后,文件会被清除,无论其中的消息是否被消费.kafka通过这种简单的手段,来释放磁盘空间,以及减少消息消费之后对文件内容改动的磁盘IO开支.</p>
<p>对于consumer而言,它需要保存消费消息的offset,对于offset的保存和使用,有consumer来控制;当consumer正常消费消息时,offset将会”线性”的向前驱动,即消息将依次顺序被消费.事实上consumer可以使用任意顺序消费消息,它只需要将offset重置为任意值..(offset将会保存在zookeeper中,参见下文)</p>
<p>kafka集群几乎不需要维护任何consumer和producer状态信息,这些信息有zookeeper保存;因此producer和consumer的客户端实现非常轻量级,它们可以随意离开,而不会对集群造成额外的影响.</p>
<p>partitions的设计目的有多个.最根本原因是kafka基于文件存储.通过分区,可以将日志内容分散到多个server上,来避免文件尺寸达到单机磁盘的上限,每个partiton都会被当前server(kafka实例)保存;可以将一个topic切分多任意多个partitions,来消息保存/消费的效率.此外越多的partitions意味着可以容纳更多的consumer,有效提升并发消费的能力.(具体原理参见下文).</p>
<h5 id="3、Distribution"><a href="#3、Distribution" class="headerlink" title="3、Distribution"></a>3、Distribution</h5><p>一个Topic的多个partitions,被分布在kafka集群中的多个server上;每个server(kafka实例)负责partitions中消息的读写操作;此外kafka还可以配置partitions需要备份的个数(replicas),每个partition将会被备份到多台机器上,以提高可用性.</p>
<p>基于replicated方案,那么就意味着需要对多个备份进行调度;每个partition都有一个server为”leader”;leader负责所有的读写操作,如果leader失效,那么将会有其他follower来接管(成为新的leader);follower只是单调的和leader跟进,同步消息即可..由此可见作为leader的server承载了全部的请求压力,因此从集群的整体考虑,有多少个partitions就意味着有多少个”leader”,kafka会将”leader”均衡的分散在每个实例上,来确保整体的性能稳定.</p>
<p>Producers<br>Producer将消息发布到指定的Topic中,同时Producer也能决定将此消息归属于哪个partition;比如基于”round-robin”方式或者通过其他的一些算法等.</p>
<p>Consumers<br>本质上kafka只支持Topic.每个consumer属于一个consumer group;反过来说,每个group中可以有多个consumer.发送到Topic的消息,只会被订阅此Topic的每个group中的一个consumer消费.</p>
<p>如果所有的consumer都具有相同的group,这种情况和queue模式很像;消息将会在consumers之间负载均衡.<br>如果所有的consumer都具有不同的group,那这就是”发布-订阅”;消息将会广播给所有的消费者.<br>在kafka中,一个partition中的消息只会被group中的一个consumer消费;每个group中consumer消息消费互相独立;我们可以认为一个group是一个”订阅”者,一个Topic中的每个partions,只会被一个”订阅者”中的一个consumer消费,不过一个consumer可以消费多个partitions中的消息.kafka只能保证一个partition中的消息被某个consumer消费时,消息是顺序的.事实上,从Topic角度来说,消息仍不是有序的.</p>
<p>kafka的设计原理决定,对于一个topic,同一个group中不能有多于partitions个数的consumer同时消费,否则将意味着某些consumer将无法得到消息.</p>
<p>Guarantees</p>
<p>1) 发送到partitions中的消息将会按照它接收的顺序追加到日志中<br>2) 对于消费者而言,它们消费消息的顺序和日志中消息顺序一致.<br>3) 如果Topic的”replicationfactor”为N,那么允许N-1个kafka实例失效.</p>
<h4 id="二、使用场景"><a href="#二、使用场景" class="headerlink" title="二、使用场景"></a>二、使用场景</h4><h5 id="1、Messaging"><a href="#1、Messaging" class="headerlink" title="1、Messaging"></a>1、Messaging</h5><p>对于一些常规的消息系统,kafka是个不错的选择;partitons/replication和容错,可以使kafka具有良好的扩展性和性能优势.不过到目前为止,我们应该很清楚认识到,kafka并没有提供JMS中的”事务性””消息传输担保(消息确认机制)””消息分组”等企业级特性;kafka只能使用作为”常规”的消息系统,在一定程度上,尚未确保消息的发送与接收绝对可靠(比如,消息重发,消息发送丢失等)</p>
<h5 id="2、Websit-activity-tracking"><a href="#2、Websit-activity-tracking" class="headerlink" title="2、Websit activity tracking"></a>2、Websit activity tracking</h5><p>kafka可以作为”网站活性跟踪”的最佳工具;可以将网页/用户操作等信息发送到kafka中.并实时监控,或者离线统计分析等</p>
<h5 id="3、Log-Aggregation"><a href="#3、Log-Aggregation" class="headerlink" title="3、Log Aggregation"></a>3、Log Aggregation</h5><p>kafka的特性决定它非常适合作为”日志收集中心”;application可以将操作日志”批量””异步”的发送到kafka集群中,而不是保存在本地或者DB中;kafka可以批量提交消息/压缩消息等,这对producer端而言,几乎感觉不到性能的开支.此时consumer端可以使hadoop等其他系统化的存储和分析系统.</p>
<h4 id="三、设计原理"><a href="#三、设计原理" class="headerlink" title="三、设计原理"></a>三、设计原理</h4><p>kafka的设计初衷是希望作为一个统一的信息收集平台,能够实时的收集反馈信息,并需要能够支撑较大的数据量,且具备良好的容错能力.</p>
<h5 id="1、持久性"><a href="#1、持久性" class="headerlink" title="1、持久性"></a>1、持久性</h5><p>kafka使用文件存储消息,这就直接决定kafka在性能上严重依赖文件系统的本身特性.且无论任何OS下,对文件系统本身的优化几乎没有可能.文件缓存/直接内存映射等是常用的手段.因为kafka是对日志文件进行append操作,因此磁盘检索的开支是较小的;同时为了减少磁盘写入的次数,broker会将消息暂时buffer起来,当消息的个数(或尺寸)达到一定阀值时,再flush到磁盘,这样减少了磁盘IO调用的次数.</p>
<h5 id="2、性能"><a href="#2、性能" class="headerlink" title="2、性能"></a>2、性能</h5><p>需要考虑的影响性能点很多,除磁盘IO之外,我们还需要考虑网络IO,这直接关系到kafka的吞吐量问题.kafka并没有提供太多高超的技巧;对于producer端,可以将消息buffer起来,当消息的条数达到一定阀值时,批量发送给broker;对于consumer端也是一样,批量fetch多条消息.不过消息量的大小可以通过配置文件来指定.对于kafka broker端,似乎有个sendfile系统调用可以潜在的提升网络IO的性能:将文件的数据映射到系统内存中,socket直接读取相应的内存区域即可,而无需进程再次copy和交换. 其实对于producer/consumer/broker三者而言,CPU的开支应该都不大,因此启用消息压缩机制是一个良好的策略;压缩需要消耗少量的CPU资源,不过对于kafka而言,网络IO更应该需要考虑.可以将任何在网络上传输的消息都经过压缩.kafka支持gzip/snappy等多种压缩方式.</p>
<h5 id="3、生产者"><a href="#3、生产者" class="headerlink" title="3、生产者"></a>3、生产者</h5><p>负载均衡: producer将会和Topic下所有partition leader保持socket连接;消息由producer直接通过socket发送到broker,中间不会经过任何”路由层”.事实上,消息被路由到哪个partition上,有producer客户端决定.比如可以采用”random””key-hash””轮询”等,如果一个topic中有多个partitions,那么在producer端实现”消息均衡分发”是必要的.</p>
<p>其中partition leader的位置(host:port)注册在zookeeper中,producer作为zookeeper client,已经注册了watch用来监听partition leader的变更事件.<br>异步发送：将多条消息暂且在客户端buffer起来，并将他们批量的发送到broker，小数据IO太多，会拖慢整体的网络延迟，批量延迟发送事实上提升了网络效率。不过这也有一定的隐患，比如说当producer失效时，那些尚未发送的消息将会丢失。</p>
<h5 id="4、消费者"><a href="#4、消费者" class="headerlink" title="4、消费者"></a>4、消费者</h5><p>consumer端向broker发送”fetch”请求,并告知其获取消息的offset;此后consumer将会获得一定条数的消息;consumer端也可以重置offset来重新消费消息.</p>
<p>在JMS实现中,Topic模型基于push方式,即broker将消息推送给consumer端.不过在kafka中,采用了pull方式,即consumer在和broker建立连接之后,主动去pull(或者说fetch)消息;这中模式有些优点,首先consumer端可以根据自己的消费能力适时的去fetch消息并处理,且可以控制消息消费的进度(offset);此外,消费者可以良好的控制消息消费的数量,batch fetch.</p>
<p>其他JMS实现,消息消费的位置是有prodiver保留,以便避免重复发送消息或者将没有消费成功的消息重发等,同时还要控制消息的状态.这就要求JMS broker需要太多额外的工作.在kafka中,partition中的消息只有一个consumer在消费,且不存在消息状态的控制,也没有复杂的消息确认机制,可见kafka broker端是相当轻量级的.当消息被consumer接收之后,consumer可以在本地保存最后消息的offset,并间歇性的向zookeeper注册offset.由此可见,consumer客户端也很轻量级.</p>
<p><img src="/images/100011.png" alt="avatar"></p>
<h5 id="5、消息传送机制"><a href="#5、消息传送机制" class="headerlink" title="5、消息传送机制"></a>5、消息传送机制</h5><p>对于JMS实现,消息传输担保非常直接:有且只有一次(exactly once).在kafka中稍有不同:</p>
<ol>
<li>at most once: 最多一次,这个和JMS中”非持久化”消息类似.发送一次,无论成败,将不会重发.</li>
<li>at least once: 消息至少发送一次,如果消息未能接受成功,可能会重发,直到接收成功.</li>
<li>exactly once: 消息只会发送一次.</li>
<li>at most once: 消费者fetch消息,然后保存offset,然后处理消息;当client保存offset之后,但是在消息处理过程中出现了异常,导致部分消息未能继续处理.那么此后”未处理”的消息将不能被fetch到,这就是”at most once”.</li>
<li>at least once: 消费者fetch消息,然后处理消息,然后保存offset.如果消息处理成功之后,但是在保存offset阶段zookeeper异常导致保存操作未能执行成功,这就导致接下来再次fetch时可能获得上次已经处理过的消息,这就是”at least once”，原因offset没有及时的提交给zookeeper，zookeeper恢复正常还是之前offset状态.</li>
<li>exactly once: kafka中并没有严格的去实现(基于2阶段提交,事务),我们认为这种策略在kafka中是没有必要的.</li>
</ol>
<p>通常情况下”at-least-once”是我们首选.(相比at most once而言,重复接收数据总比丢失数据要好).</p>
<h5 id="6、复制备份"><a href="#6、复制备份" class="headerlink" title="6、复制备份"></a>6、复制备份</h5><p>kafka将每个partition数据复制到多个server上,任何一个partition有一个leader和多个follower(可以没有);备份的个数可以通过broker配置文件来设定.leader处理所有的read-write请求,follower需要和leader保持同步.Follower和consumer一样,消费消息并保存在本地日志中;leader负责跟踪所有的follower状态,如果follower”落后”太多或者失效,leader将会把它从replicas同步列表中删除.当所有的follower都将一条消息保存成功,此消息才被认为是”committed”,那么此时consumer才能消费它.即使只有一个replicas实例存活,仍然可以保证消息的正常发送和接收,只要zookeeper集群存活即可.(不同于其他分布式存储,比如hbase需要”多数派”存活才行)</p>
<p>当leader失效时,需在followers中选取出新的leader,可能此时follower落后于leader,因此需要选择一个”up-to-date”的follower.选择follower时需要兼顾一个问题,就是新leaderserver上所已经承载的partition leader的个数,如果一个server上有过多的partition leader,意味着此server将承受着更多的IO压力.在选举新leader,需要考虑到”负载均衡”.</p>
<h5 id="7、日志"><a href="#7、日志" class="headerlink" title="7、日志"></a>7、日志</h5><p>如果一个topic的名称为”my_topic”,它有2个partitions,那么日志将会保存在my_topic_0和my_topic_1两个目录中;日志文件中保存了一序列”log entries”(日志条目),每个log entry格式为”4个字节的数字N表示消息的长度” + “N个字节的消息内容”;每个日志都有一个offset来唯一的标记一条消息,offset的值为8个字节的数字,表示此消息在此partition中所处的起始位置..每个partition在物理存储层面,有多个log file组成(称为segment).segmentfile的命名为”最小offset”.kafka.例如”00000000000.kafka”;其中”最小offset”表示此segment中起始消息的offset.</p>
<p><img src="/images/100012.png" alt="avatar"></p>
<p>其中每个partiton中所持有的segments列表信息会存储在zookeeper中.</p>
<p>当segment文件尺寸达到一定阀值时(可以通过配置文件设定,默认1G),将会创建一个新的文件;当buffer中消息的条数达到阀值时将会触发日志信息flush到日志文件中,同时如果”距离最近一次flush的时间差”达到阀值时,也会触发flush到日志文件.如果broker失效,极有可能会丢失那些尚未flush到文件的消息.因为server意外实现,仍然会导致log文件格式的破坏(文件尾部),那么就要求当server启东是需要检测最后一个segment的文件结构是否合法并进行必要的修复.</p>
<p>获取消息时,需要指定offset和最大chunk尺寸,offset用来表示消息的起始位置,chunk size用来表示最大获取消息的总长度(间接的表示消息的条数).根据offset,可以找到此消息所在segment文件,然后根据segment的最小offset取差值,得到它在file中的相对位置,直接读取输出即可.</p>
<p>日志文件的删除策略非常简单:启动一个后台线程定期扫描log file列表,把保存时间超过阀值的文件直接删除(根据文件的创建时间).为了避免删除文件时仍然有read操作(consumer消费),采取copy-on-write方式.</p>
<h5 id="8、分配"><a href="#8、分配" class="headerlink" title="8、分配"></a>8、分配</h5><p>kafka使用zookeeper来存储一些meta信息,并使用了zookeeper watch机制来发现meta信息的变更并作出相应的动作(比如consumer失效,触发负载均衡等)</p>
<ol>
<li><p>Broker node registry: 当一个kafkabroker启动后,首先会向zookeeper注册自己的节点信息(临时znode),同时当broker和zookeeper断开连接时,此znode也会被删除.<br> 格式: /broker/ids/[0…N]   –&gt;host:port;其中[0..N]表示broker id,每个broker的配置文件中都需要指定一个数字类型的id(全局不可重复),znode的值为此broker的host:port信息.</p>
</li>
<li><p>Broker Topic Registry: 当一个broker启动时,会向zookeeper注册自己持有的topic和partitions信息,仍然是一个临时znode.<br> 格式: /broker/topics/[topic]/[0…N]  其中[0..N]表示partition索引号.</p>
</li>
<li><p>Consumer and Consumer group: 每个consumer客户端被创建时,会向zookeeper注册自己的信息;此作用主要是为了”负载均衡”.<br> 一个group中的多个consumer可以交错的消费一个topic的所有partitions;简而言之,保证此topic的所有partitions都能被此group所消费,且消费时为了性能考虑,让partition相对均衡的分散到每个consumer上.</p>
</li>
<li><p>Consumer id Registry: 每个consumer都有一个唯一的ID(host:uuid,可以通过配置文件指定,也可以由系统生成),此id用来标记消费者信息.<br> 格式:/consumers/[group_id]/ids/[consumer_id]<br> 仍然是一个临时的znode,此节点的值为{“topic_name”:#streams…},即表示此consumer目前所消费的topic + partitions列表.</p>
</li>
<li><p>Consumer offset Tracking: 用来跟踪每个consumer目前所消费的partition中最大的offset.<br> 格式:/consumers/[group_id]/offsets/[topic]/[broker_id-partition_id]–&gt;offset_value<br> 此znode为持久节点,可以看出offset跟group_id有关,以表明当group中一个消费者失效,其他consumer可以继续消费.</p>
</li>
<li><p>Partition Owner registry: 用来标记partition被哪个consumer消费.临时znode<br> 格式:/consumers/[group_id]/owners/[topic]/[broker_id-partition_id]–&gt;consumer_node_id当consumer启动时,所触发的操作:</p>
<p>  A) 首先进行”Consumer id Registry”;<br>  B) 然后在”Consumer id Registry”节点下注册一个watch用来监听当前group中其他consumer的”leave”和”join”;只要此znode path下节点列表变更,都会触发此group下consumer的负载均衡.(比如一个consumer失效,那么其他consumer接管partitions).<br>  C) 在”Broker id registry”节点下,注册一个watch用来监听broker的存活情况;如果broker列表变更,将会触发所有的groups下的consumer重新balance.</p>
</li>
</ol>
<p><img src="/images/100013.png" alt="avatar"></p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1) Producer端使用zookeeper用来&quot;发现&quot;broker列表,以及和Topic下每个partition leader建立socket连接并发送消息.</span><br><span class="line">2) Broker端使用zookeeper用来注册broker信息,已经监测partitionleader存活性.</span><br><span class="line">3) Consumer端使用zookeeper用来注册consumer信息,其中包括consumer消费的partition列表等,同时也用来发现broker列表,并和partition leader建立socket连接,并获取消息.</span><br></pre></td></tr></table></figure>
<h4 id="四、主要配置"><a href="#四、主要配置" class="headerlink" title="四、主要配置"></a>四、主要配置</h4><p>1、Broker配置<br><img src="/images/100014.png" alt="avatar"></p>
<p>2.Consumer主要配置<br><img src="/images/100015.png" alt="avatar"></p>
<p>3.Producer主要配置<br><img src="/images/100016.png" alt="avatar"></p>
<p>以上是关于kafka一些基础说明，在其中我们知道如果要kafka正常运行，必须配置zookeeper，否则无论是kafka集群还是客户端的生存者和消费者都无法正常的工作的，以下是对zookeeper进行一些简单的介绍：</p>
<h4 id="五、zookeeper集群"><a href="#五、zookeeper集群" class="headerlink" title="五、zookeeper集群"></a>五、zookeeper集群</h4><p>zookeeper是一个为分布式应用提供一致性服务的软件，它是开源的Hadoop项目的一个子项目，并根据google发表的一篇论文来实现的。zookeeper为分布式系统提供了高笑且易于使用的协同服务，它可以为分布式应用提供相当多的服务，诸如统一命名服务，配置管理，状态同步和组服务等。zookeeper接口简单，我们不必过多地纠结在分布式系统编程难于处理的同步和一致性问题上，你可以使用zookeeper提供的现成(off-the-shelf)服务来实现来实现分布式系统额配置管理，组管理，Leader选举等功能。</p>
<p>zookeeper集群的安装,准备三台服务器server1:192.168.0.1,server2:192.168.0.2,server3:192.168.0.3.</p>
<p>1)下载zookeeper<br>到<a href="http://zookeeper.apache.org/releases.html去下载最新版本Zookeeper-3.4.5的安装包zookeeper-3.4.5.tar.gz.将文件保存server1的~目录下" target="_blank" rel="noopener">http://zookeeper.apache.org/releases.html去下载最新版本Zookeeper-3.4.5的安装包zookeeper-3.4.5.tar.gz.将文件保存server1的~目录下</a></p>
<p>2)安装zookeeper<br>先在服务器server分别执行a-c步骤<br>a)解压<br>tar -zxvf zookeeper-3.4.5.tar.gz<br>解压完成后在目录~下会发现多出一个目录zookeeper-3.4.5,重新命令为zookeeper<br>b）配置<br>将conf/zoo_sample.cfg拷贝一份命名为zoo.cfg，也放在conf目录下。然后按照如下值修改其中的配置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># The number of milliseconds of each tick</span><br><span class="line">tickTime=2000</span><br><span class="line"># The number of ticks that the initial</span><br><span class="line"># synchronization phase can take</span><br><span class="line">initLimit=10</span><br><span class="line"># The number of ticks that can pass between</span><br><span class="line"># sending a request and getting an acknowledgement</span><br><span class="line">syncLimit=5</span><br><span class="line"># the directory where the snapshot is stored.</span><br><span class="line"># do not use /tmp for storage, /tmp here is just</span><br><span class="line"># example sakes.</span><br><span class="line">dataDir=/home/wwb/zookeeper /data</span><br><span class="line">dataLogDir=/home/wwb/zookeeper/logs</span><br><span class="line"># the port at which the clients will connect</span><br><span class="line">clientPort=2181</span><br><span class="line">#</span><br><span class="line"># Be sure to read the maintenance section of the</span><br><span class="line"># administrator guide before turning on autopurge.</span><br><span class="line">#http://zookeeper.apache.org/doc/ ... html#sc_maintenance</span><br><span class="line">#</span><br><span class="line"># The number of snapshots to retain in dataDir</span><br><span class="line">#autopurge.snapRetainCount=3</span><br><span class="line"># Purge task interval in hours</span><br><span class="line"># Set to &quot;0&quot; to disable auto purge feature</span><br><span class="line">#autopurge.purgeInterval=1</span><br><span class="line">server.1=192.168.0.1:3888:4888</span><br><span class="line">server.2=192.168.0.2:3888:4888</span><br><span class="line">server.3=192.168.0.3:3888:4888</span><br></pre></td></tr></table></figure>

<p>tickTime：这个时间是作为 Zookeeper 服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个 tickTime 时间就会发送一个心跳。</p>
<p>dataDir：顾名思义就是 Zookeeper 保存数据的目录，默认情况下，Zookeeper 将写数据的日志文件也保存在这个目录里。</p>
<p>clientPort：这个端口就是客户端连接 Zookeeper 服务器的端口，Zookeeper 会监听这个端口，接受客户端的访问请求。</p>
<p>initLimit：这个配置项是用来配置 Zookeeper 接受客户端（这里所说的客户端不是用户连接 Zookeeper 服务器的客户端，而是 Zookeeper 服务器集群中连接到 Leader 的 Follower 服务器）初始化连接时最长能忍受多少个心跳时间间隔数。当已经超过 5个心跳的时间（也就是 tickTime）长度后 Zookeeper 服务器还没有收到客户端的返回信息，那么表明这个客户端连接失败。总的时间长度就是 5*2000=10 秒</p>
<p>syncLimit：这个配置项标识 Leader 与Follower 之间发送消息，请求和应答时间长度，最长不能超过多少个 tickTime 的时间长度，总的时间长度就是2*2000=4 秒</p>
<p>server.A=B：C：D：其中 A 是一个数字，表示这个是第几号服务器；B 是这个服务器的 ip 地址；C 表示的是这个服务器与集群中的 Leader 服务器交换信息的端口；D 表示的是万一集群中的 Leader 服务器挂了，需要一个端口来重新进行选举，选出一个新的 Leader，而这个端口就是用来执行选举时服务器相互通信的端口。如果是伪集群的配置方式，由于 B 都是一样，所以不同的 Zookeeper 实例通信端口号不能一样，所以要给它们分配不同的端口号</p>
<p>注意:dataDir,dataLogDir中的wwb是当前登录用户名，data，logs目录开始是不存在，需要使用mkdir命令创建相应的目录。并且在该目录下创建文件myid,serve1,server2,server3该文件内容分别为1,2,3。</p>
<p>针对服务器server2,server3可以将server1复制到相应的目录，不过需要注意dataDir,dataLogDir目录,并且文件myid内容分别为2,3</p>
<p>3)依次启动server1，server2,server3的zookeeper.<br>/home/wwb/zookeeper/bin/zkServer.sh start,出现类似以下内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">JMX enabled by default</span><br><span class="line">Using config: /home/wwb/zookeeper/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br></pre></td></tr></table></figure>

<p>4) 测试zookeeper是否正常工作，在server1上执行以下命令<br>/home/wwb/zookeeper/bin/zkCli.sh -server192.168.0.2:2181,出现类似以下内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">JLine support is enabled</span><br><span class="line">2013-11-27 19:59:40,560 - INFO      [main-SendThread(localhost.localdomain:2181):ClientCnxn$SendThread@736]- Session   establishmentcomplete on server localhost.localdomain/127.0.0.1:2181, sessionid =    0x1429cdb49220000, negotiatedtimeout = 30000</span><br><span class="line"></span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected type:None path:null</span><br><span class="line">[zk: 127.0.0.1:2181(CONNECTED) 0] [root@localhostzookeeper2]#</span><br></pre></td></tr></table></figure>
<p>即代表集群构建成功了,如果出现错误那应该是第三部时没有启动好集群，<br>运行，先利用<br>ps aux | grep zookeeper查看是否有相应的进程的，没有话，说明集群启动出现问题，可以在每个服务器上使用<br>./home/wwb/zookeeper/bin/zkServer.sh stop。再依次使用./home/wwb/zookeeper/binzkServer.sh start，这时在执行4一般是没有问题，如果还是有问题，那么先stop再到bin的上级目录执行./bin/zkServer.shstart试试。</p>
<p>注意：zookeeper集群时，zookeeper要求半数以上的机器可用，zookeeper才能提供服务。</p>
<h4 id="六、kafka集群"><a href="#六、kafka集群" class="headerlink" title="六、kafka集群"></a>六、kafka集群</h4><p>(利用上面server1,server2,server3,下面以server1为实例)</p>
<p>1)下载kafka0.8(<a href="http://kafka.apache.org/downloads.html),保存到服务器/home/wwb目录下kafka-0.8.0-beta1-src.tgz(kafka_2.8.0-0.8.0-beta1.tgz)" target="_blank" rel="noopener">http://kafka.apache.org/downloads.html),保存到服务器/home/wwb目录下kafka-0.8.0-beta1-src.tgz(kafka_2.8.0-0.8.0-beta1.tgz)</a></p>
<p>2)解压 tar -zxvf kafka-0.8.0-beta1-src.tgz,产生文件夹kafka-0.8.0-beta1-src更改为kafka01</p>
<p>3)配置<br>修改kafka01/config/server.properties,其中broker.id,log.dirs,zookeeper.connect必须根据实际情况进行修改，其他项根据需要自行斟酌。大致如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">broker.id=1</span><br><span class="line">port=9091</span><br><span class="line">num.network.threads=2</span><br><span class="line">num.io.threads=2</span><br><span class="line">socket.send.buffer.bytes=1048576</span><br><span class="line">socket.receive.buffer.bytes=1048576</span><br><span class="line">socket.request.max.bytes=104857600</span><br><span class="line">log.dir=./logs</span><br><span class="line">num.partitions=2</span><br><span class="line">log.flush.interval.messages=10000</span><br><span class="line">log.flush.interval.ms=1000</span><br><span class="line">log.retention.hours=168</span><br><span class="line">#log.retention.bytes=1073741824</span><br><span class="line">log.segment.bytes=536870912</span><br><span class="line">num.replica.fetchers=2</span><br><span class="line">log.cleanup.interval.mins=10</span><br><span class="line">zookeeper.connect=192.168.0.1:2181,192.168.0.2:2182,192.168.0.3:2183</span><br><span class="line">zookeeper.connection.timeout.ms=1000000</span><br><span class="line">kafka.metrics.polling.interval.secs=5</span><br><span class="line">kafka.metrics.reporters=kafka.metrics.KafkaCSVMetricsReporter</span><br><span class="line">kafka.csv.metrics.dir=/tmp/kafka_metrics</span><br><span class="line">kafka.csv.metrics.reporter.enabled=false</span><br></pre></td></tr></table></figure>
<p>4）初始化因为kafka用scala语言编写，因此运行kafka需要首先准备scala相关环境。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; cd kafka01</span><br><span class="line">&gt; ./sbt update</span><br><span class="line">&gt; ./sbt package</span><br><span class="line">&gt; ./sbt assembly-package-dependency</span><br></pre></td></tr></table></figure>
<p>在第二个命令时可能需要一定时间，由于要下载更新一些依赖包。所以请大家 耐心点。</p>
<p>5) 启动kafka01<br> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;JMX_PORT=9997 bin/kafka-server-start.sh config/server.properties &amp;</span><br></pre></td></tr></table></figure><br>a)kafka02操作步骤与kafka01雷同，不同的地方如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">修改kafka02/config/server.properties</span><br><span class="line">broker.id=2</span><br><span class="line">port=9092</span><br><span class="line">##其他配置和kafka-0保持一致</span><br><span class="line">启动kafka02</span><br><span class="line">JMX_PORT=9998 bin/kafka-server-start.shconfig/server.properties &amp;</span><br></pre></td></tr></table></figure>
<p>b)kafka03操作步骤与kafka01雷同，不同的地方如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">修改kafka03/config/server.properties</span><br><span class="line">broker.id=3</span><br><span class="line">port=9093</span><br><span class="line">##其他配置和kafka-0保持一致</span><br><span class="line">启动kafka02</span><br><span class="line">JMX_PORT=9999 bin/kafka-server-start.shconfig/server.properties &amp;</span><br></pre></td></tr></table></figure>

<p>6)创建Topic(包含一个分区，三个副本)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;bin/kafka-create-topic.sh--zookeeper 192.168.0.1:2181 --replica 3 --partition 1 --topicmy-replicated-topic</span><br></pre></td></tr></table></figure>
<p>7)查看topic情况</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;bin/kafka-list-top.sh --zookeeper 192.168.0.1:2181</span><br><span class="line">topic: my-replicated-topic  partition: 0 leader: 1  replicas: 1,2,0  isr: 1,2,0</span><br></pre></td></tr></table></figure>
<p>8)创建发送者</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;bin/kafka-console-producer.sh--broker-list 192.168.0.1:9091 --topic my-replicated-topic</span><br><span class="line">my test message1</span><br><span class="line">my test message2</span><br><span class="line">^C</span><br></pre></td></tr></table></figure>
<p>9)创建消费者</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;bin/kafka-console-consumer.sh --zookeeper127.0.0.1:2181 --from-beginning --topic my-replicated-topic</span><br><span class="line">...</span><br><span class="line">my test message1</span><br><span class="line">my test message2</span><br><span class="line">^C</span><br></pre></td></tr></table></figure>
<p>10)杀掉server1上的broker</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;pkill -9 -f config/server.properties</span><br><span class="line">11)查看topic</span><br><span class="line">&gt;bin/kafka-list-top.sh --zookeeper192.168.0.1:2181</span><br><span class="line">topic: my-replicated-topic  partition: 0 leader: 1  replicas: 1,2,0  isr: 1,2,0</span><br><span class="line">发现topic还正常的存在</span><br></pre></td></tr></table></figure>
<p>11）创建消费者，看是否能查询到消息</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;bin/kafka-console-consumer.sh --zookeeper192.168.0.1:2181 --from-beginning --topic my-replicated-topic</span><br><span class="line">...</span><br><span class="line">my test message 1</span><br><span class="line">my test message 2</span><br><span class="line">^C</span><br><span class="line">说明一切都是正常的。</span><br></pre></td></tr></table></figure>
<p>OK,以上就是对Kafka个人的理解，不对之处请大家及时指出。</p>
<h4 id="补充说明："><a href="#补充说明：" class="headerlink" title="补充说明："></a>补充说明：</h4><p>1、public Map&lt;String, List&lt;KafkaStream&lt;byte[], byte[]&gt;&gt;&gt; createMessageStreams(Map&lt;String, Integer&gt; topicCountMap)，其中该方法的参数Map的key为topic名称，value为topic对应的分区数，譬如说如果在kafka中不存在相应的topic时，则会创建一个topic，分区数为value，如果存在的话，该处的value则不起什么作用</p>
<p>2、关于生产者向指定的分区发送数据，通过设置partitioner.class的属性来指定向那个分区发送数据，如果自己指定必须编写相应的程序，默认是kafka.producer.DefaultPartitioner,分区程序是基于散列的键。</p>
<p>3、在多个消费者读取同一个topic的数据，为了保证每个消费者读取数据的唯一性，必须将这些消费者group_id定义为同一个值，这样就构建了一个类似队列的数据结构，如果定义不同，则类似一种广播结构的。</p>
<p>4、在consumerapi中，参数设计到数字部分，类似Map&lt;String,Integer&gt;,<br>numStream,指的都是在topic不存在的时，会创建一个topic，并且分区个数为Integer,numStream,注意如果数字大于broker的配置中num.partitions属性，会以num.partitions为依据创建分区个数的。</p>
<p>5、producerapi，调用send时，如果不存在topic，也会创建topic，在该方法中没有提供分区个数的参数，在这里分区个数是由服务端broker的配置中num.partitions属性决定的</p>
]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
  </entry>
  <entry>
    <title>Mysql中in和exist的区别</title>
    <url>/2025562487.html</url>
    <content><![CDATA[<h3 id="in和exists的区别"><a href="#in和exists的区别" class="headerlink" title="in和exists的区别"></a>in和exists的区别</h3><p><em>转自：<a href="https://www.cnblogs.com/zhuyeshen/p/10955417.html" target="_blank" rel="noopener">https://www.cnblogs.com/zhuyeshen/p/10955417.html</a><br>参考：<a href="http://www.pianshen.com/article/534497498/" target="_blank" rel="noopener">http://www.pianshen.com/article/534497498/</a><br>&#8195;&#8195;<a href="https://www.cnblogs.com/xiaoxiong-kankan/p/7928153.html" target="_blank" rel="noopener">https://www.cnblogs.com/xiaoxiong-kankan/p/7928153.html</a></em></p>
<h4 id="表结构"><a href="#表结构" class="headerlink" title="表结构"></a>表结构</h4><p>首先，查询中涉及到的两个表，一个user和一个order表，具体表的内容如下：</p>
<p>user表：<br><img src="/images/100001.png" alt="avatar">  </p>
<a id="more"></a>
<p>order表：<br><img src="/images/100002.png" alt="avatar"></p>
<h4 id="in查询"><a href="#in查询" class="headerlink" title="in查询"></a>in查询</h4><p>确定给定的值是否与子查询或列表中的值相匹配。in在查询的时候，首先查询子查询的表，然后将内表和外表做一个笛卡尔积，然后按照条件进行筛选。所以相对内表比较小的时候，in的速度较快。</p>
<p>具体sql语句如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> 1 SELECT</span><br><span class="line"> 2     *</span><br><span class="line"> 3 FROM</span><br><span class="line"> 4     `user`</span><br><span class="line"> 5 WHERE</span><br><span class="line"> 6     `user`.id IN (</span><br><span class="line"> 7         SELECT</span><br><span class="line"> 8             `order`.user_id</span><br><span class="line"> 9         FROM</span><br><span class="line">10             `order`</span><br><span class="line">11     )</span><br></pre></td></tr></table></figure>
<p>这条语句很简单，通过子查询查到的user_id 的数据，去匹配user表中的id然后得到结果。该语句执行结果如下：<br><img src="/images/100003.png" alt="avatar"></p>
<p>它的执行流程是什么样子的呢？让我们一起来看一下。</p>
<p>首先，在数据库内部，查询子查询，执行如下代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT</span><br><span class="line">    `order`.user_id</span><br><span class="line">FROM</span><br><span class="line">    `order`</span><br></pre></td></tr></table></figure>
<p>执行完毕后，得到结果如下：<br><img src="/images/100004.png" alt="avatar"><br>此时，将查询到的结果和原有的user表做一个笛卡尔积，结果如下：<br><img src="/images/100005.png" alt="avatar"><br>此时，再根据我们的user.id IN order.user_id的条件，将结果进行筛选（既比较id列和user_id 列的值是否相等，将不相等的删除）。最后，得到两条符合条件的数据。<br><img src="/images/100006.png" alt="avatar">
　　　　</p>
<h4 id="exists"><a href="#exists" class="headerlink" title="exists"></a>exists</h4><p>指定一个子查询，检测行的存在。遍历循环外表，然后看外表中的记录有没有和内表的数据一样的。匹配上就将结果放入结果集中。</p>
<p>具体sql语句如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> 1 SELECT</span><br><span class="line"> 2     `user`.*</span><br><span class="line"> 3 FROM</span><br><span class="line"> 4     `user`</span><br><span class="line"> 5 WHERE</span><br><span class="line"> 6     EXISTS (</span><br><span class="line"> 7         SELECT</span><br><span class="line"> 8             `order`.user_id</span><br><span class="line"> 9         FROM</span><br><span class="line">10             `order`</span><br><span class="line">11         WHERE</span><br><span class="line">12             `user`.id = `order`.user_id</span><br><span class="line">13     )</span><br></pre></td></tr></table></figure>
<p>这条sql语句的执行结果和上面的in的执行结果是一样的:<br><img src="/images/100007.png" alt="avatar"></p>
<p>但是，不一样的是它们的执行流程完全不一样：</p>
<p>使用exists关键字进行查询的时候，首先，我们先查询的不是子查询的内容，而是查我们的主查询的表，也就是说，我们先执行的sql语句是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT `user`.* FROM `user`</span><br></pre></td></tr></table></figure>
<p>得到的结果如下：<br><img src="/images/100008.png" alt="avatar"><br>然后，根据表的每一条记录，执行以下语句，依次去判断where后面的条件是否成立：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">EXISTS (</span><br><span class="line">        SELECT</span><br><span class="line">            `order`.user_id</span><br><span class="line">        FROM</span><br><span class="line">            `order`</span><br><span class="line">        WHERE</span><br><span class="line">            `user`.id = `order`.user_id</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>如果成立则返回true不成立则返回false。如果返回的是true的话，则该行结果保留，如果返回的是false的话，则删除该行，最后将得到的结果返回。</p>
<h4 id="区别及应用场景"><a href="#区别及应用场景" class="headerlink" title="区别及应用场景"></a>区别及应用场景</h4><p>in 和 exists的区别: 如果子查询得出的结果集记录较少，主查询中的表较大且又有索引时应该用in, 反之如果外层的主查询记录较少，子查询中的表大，又有索引时使用exists。其实我们区分in和exists主要是造成了驱动顺序的改变(这是性能变化的关键)，如果是exists，那么以外层表为驱动表，先被访问，如果是IN，那么先执行子查询，所以我们会以驱动表的快速返回为目标，那么就会考虑到索引及结果集的关系了 ，另外IN时不对NULL进行处理。</p>
<p>in 是把外表和内表作hash 连接，而exists是对外表作loop循环，每次loop循环再对内表进行查询。一直以来认为exists比in效率高的说法是不准确的。</p>
<h4 id="not-in-和not-exists"><a href="#not-in-和not-exists" class="headerlink" title="not in 和not exists"></a>not in 和not exists</h4><p>如果查询语句使用了not in 那么内外表都进行全表扫描，没有用到索引；而not extsts 的子查询依然能用到表上的索引。所以无论那个表大，用not exists都比not in要快。</p>
]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
  </entry>
  <entry>
    <title>电商系统中关于订单和库存的几个问题</title>
    <url>/3786447307.html</url>
    <content><![CDATA[<p><em>原创整理，转载请注明出处</em></p>
<h3 id="电商系统关于订单和库存的几个问题"><a href="#电商系统关于订单和库存的几个问题" class="headerlink" title="电商系统关于订单和库存的几个问题"></a>电商系统关于订单和库存的几个问题</h3><p>最近参加面试，被问及电商项目中如何处理并发情况下用户下单、减库存、事务一致性等问题。由于之前的项目类似于一个单体项目，对于高并发、高可用的的设计未有考虑，故面试问答很不流畅，有一定的相关概念，却没有一整套解决相关问题的流程。所以决定做个总结，以备后用。<br>总结起来有一下几个问题：</p>
<a id="more"></a>
<ol>
<li>系统是如何保证高并发下用户体验的？</li>
<li>扣减库存是下单扣减还是支付扣减？为什么？</li>
<li>对于库存的扣减，如何保证不超卖？</li>
<li>如何控制恶意库存占用？</li>
</ol>
<h4 id="问题一：系统是如何保证高并发下用户体验的？"><a href="#问题一：系统是如何保证高并发下用户体验的？" class="headerlink" title="问题一：系统是如何保证高并发下用户体验的？"></a>问题一：系统是如何保证高并发下用户体验的？</h4><p>首先我们来明确一个概念：并发。并发是什么？有人说并发就是两个或多个任务一起执行。在单核CPU中，执行任务的线程是交替获取cup时间切片的，由于切换并执行非常快，在外界看来，多个任务是一起执行的。实际上，它们是交替执行的。在虚拟机中，有个专门的内存区域叫程序计数器，它的目的就是记录每个线程执行的字节码的行号指示器，以应对线程切换执行时，切入执行时机的问题。与并发类似的概念是并行，它是真实地同步执行，所以只有在多核CPU中才会有并行地概念。<br>所以，对于高并发用户体验可用性问题，实际上就是系统地性能能达到什么样地程度问题。<br>这当中，数据库的访问性能往往又是系统的性能瓶颈。根据经验数据，用户在访问互联网时，超过90%的操作只是读取数据，提交、修改的数据不到10%。因此可以将内容相对固定、主要供用户浏览的页面生产缓存，而无需访问数据库。<br>对于静态内容（网页、图片、音频文件、脚本文件等）可以选择CDN（Content Delivery Network，内容分发网络）方式发布，从而通过专业内容发布服务提高网站访问速度。频繁修改的数据可以采用缓存的办法处理。</p>
<h4 id="问题二：扣减库存是下单扣减还是支付扣减？为什么？"><a href="#问题二：扣减库存是下单扣减还是支付扣减？为什么？" class="headerlink" title="问题二：扣减库存是下单扣减还是支付扣减？为什么？"></a>问题二：扣减库存是下单扣减还是支付扣减？为什么？</h4><p>核心思路是预锁库存，即下单时扣减，支付设置一个有效期，超过有效期释放库存。对于可能的恶意库存占用，限定用户在某个时间内的购买数量（或者达到一定数量提示其转到大客户购买处，单独处理）（PS：有位面试官牢牢抓住恶意库存占用问题问，最后聊得很尴尬，其实这本质上是产品得取舍问题，哪怕是支付后扣减，也有诸如支付了却提示库存不足得问题，这就要看产品根据实际情况做出权衡，技术上总归有实现思路的）。</p>
<p>摘抄他人总结的12306的实现思路<br>因为买火车票和购物不一样，购物可以付款后出库，但是买票这种，支付前就必须出库，因此，要将出库过程提前， 只有出库成功，才能生成订单，同样要引入redis库存</p>
<ol>
<li><p>先扣缓存中的库存，扣除成功后，然后才可以去扣mysql中的库存</p>
</li>
<li><p>如果扣除缓存中的库存失败，就会挡在外面，返回库存不足，这些请求不会穿刺到mysql中，挡住了大多数的请求压力。</p>
</li>
<li><p>redis库存会和mysql库存不一致，极端情况下是肯定有的，需要进行库存同步</p>
<p>3.1 当缓存库存比数据库库存多，那么就会出现，查询有票，但是就无法下单，下单的时候就说库存不足， 这样也不会超卖，当redis的库存多的那部分扣完了，就可以把请求全部当在外面了。 对于12306，有时候，查询的时候有票，但是下单的时候返回库存不足，然后重新查询发现还是有库存， 这种情况应该就是redis中库存和mysql中库存不一致造成的。</p>
<p>3.2 当缓存库存比数据库缓存少，那么不会出问题，只会出现有票，但是没有出售的情况，等完成库存同步一下， 明天又准确了。</p>
<p>3.3 当然，mysql扣除库存的部分，还需要在前面加入队列缓冲，避免请求过多，让应用程序或数据库崩溃。</p>
</li>
</ol>
<h4 id="问题三：对于库存的扣减，如何保证不超卖？"><a href="#问题三：对于库存的扣减，如何保证不超卖？" class="headerlink" title="问题三：对于库存的扣减，如何保证不超卖？"></a>问题三：对于库存的扣减，如何保证不超卖？</h4><p>方式一<br>可以对读操作加上显式锁（即在select …语句最后加上for update）这样一来用户1在进行读操作时用户2就需要排队等待了<br>但是问题来了，如果该商品很热门并发量很高那么效率就会大大的下降，怎么解决？<br>我们可以有条件有选择的在读操作上加锁，比如可以对库存做一个判断，当库存小于一个量时开始加锁，让购买者排队，这样一来就解决了超卖现象。<br>方式二<br>数据库表增加版本字段如version，每次修改时版本号+1<br>如果更新操作顺序执行，则数据的版本（version）依次递增，不会产生冲突。但是如果发生有不同的业务操作对同一版本的数据进行修改，那么，先提交的操作（图中B）会把数据version更新为2，当A在B之后提交更新时发现数据的version已经被修改了，那么A的更新操作会失败。<br>PDO update更新后，不但要验证返回状态是否为true,并且同时验证影响行数是否大于0<br>方式三 redis的队列来实现<br>将要促销的商品数量以队列的方式存入redis中，每当用户抢到一件促销商品则从队列中删除一个数据，确保商品不会超卖。这个操作起来很方便，而且效率极高，最终我们采取这种方式来实现</p>
<h4 id="问题四-如何控制恶意库存占用？"><a href="#问题四-如何控制恶意库存占用？" class="headerlink" title="问题四:如何控制恶意库存占用？"></a>问题四:如何控制恶意库存占用？</h4><p>首先，这个问题是否真实存在，或者说发生概率有多大？</p>
<p>产品设计有时候是一个博弈的过程，如果一个功能需要付出10的成本，却只覆盖了1的需求，这个功能是否真的需要投入开发？举个栗子，打车平台的车主入驻需要验证车主真实身份，在真实性安全性方面做好保障，某打车平台A在车主入驻的时候需要填写繁复的资料，比如验证身份证，举着身份证拍照等，结果导致只有5%的车主愿意走完整个验证流程，但实际上虚假的车主可能仅占1%，为了这1%的车主放弃了另外94%的服务者，是不是太亏了？</p>
<p>当然不是说问题小，就可以不去解决，但是可以考虑有没有侧面的低成本解决方案。</p>
<p>延续上面的例子，另外一个打车平台B在车主入驻的时候仅需要填写基础资料，那如何验证司机真实性呢？机智的B公司邀请司机绑定银行卡，载客收益到账是司机的最基本需求，而银行卡是与个人身份绑定的，已经经过银行验证过用户真实性的，较低的注册成本使得大部分的司机都愿意走完注册流程，开始使用B产品。</p>
<p>下面正面回答下问题，因为非电商行业，所以仅靠逻辑推理，如有错误，请及时沟通</p>
<p>1、反作弊策略。定义恶意下单的时间频次，IP或者UID信息，当操作行为触发反作弊策略后，可以对该商品的库存锁定机制做调整，对于某些运营活动的爆款需要设立白名单或者调整反作弊策略；</p>
<p>2、允许超卖。库存100件，允许超卖x%；</p>
<p>3、调整库存锁定/解锁策略，比如在下单后5min才触发库存锁定，或者下单后15min未支付则解除库存锁定；</p>
<p>5、当库存小于一定量时，界面设计上则显示库存紧张，以用户支付请求为节点，“先支付先得”。</p>
<p>参考：<br>电商产品如何防止恶意下单导致的库存被占用？<a href="https://www.pmcaff.com/discuss/index/1000000000162678?from=related&amp;pmc_param%5Bentry_id%5D=1000000000163080" target="_blank" rel="noopener">https://www.pmcaff.com/discuss/index/1000000000162678?from=related&amp;pmc_param%5Bentry_id%5D=1000000000163080</a><br>高并发电商平台设计 <a href="http://www.legendshop.cn/new_547.html" target="_blank" rel="noopener">http://www.legendshop.cn/new_547.html</a><br>用Redis轻松实现秒杀系统 <a href="https://blog.csdn.net/lida1234567/article/details/82866617" target="_blank" rel="noopener">https://blog.csdn.net/lida1234567/article/details/82866617</a><br>如何解决电商网站超卖现象 <a href="https://blog.csdn.net/u013521220/article/details/78839307" target="_blank" rel="noopener">https://blog.csdn.net/u013521220/article/details/78839307</a><br>关于电商库存扣除实现思路 <a href="http://www.fecmall.com/topic/648" target="_blank" rel="noopener">http://www.fecmall.com/topic/648</a></p>
]]></content>
      <categories>
        <category>系统设计</category>
      </categories>
  </entry>
  <entry>
    <title>MyBatis面试题：# 和 $ 的区别是什么</title>
    <url>/182044780.html</url>
    <content><![CDATA[<p><em>转载自：Mybatis中文网：<a href="http://www.mybatis.cn/" target="_blank" rel="noopener">http://www.mybatis.cn/</a></em></p>
<p>经常碰到这样的面试题目：#{}和${}的区别是什么？</p>
<p>正确的答案是：<strong>#{}是预编译处理，${}是字符串替换。</strong></p>
<p>备注：${}是插值，插值的新认识见： <a href="http://www.mybatis.cn/archives/653.html" target="_blank" rel="noopener">http://www.mybatis.cn/archives/653.html</a></p>
<a id="more"></a>
<ol>
<li><p>mybatis在处理#{}时，会将sql中的#{}替换为?号，调用PreparedStatement的set方法来赋值。</p>
</li>
<li><p>mybatis在处理${}时，就是把${}替换成变量的值。</p>
</li>
<li><p>使用#{}可以有效的防止SQL注入，提高系统安全性。原因在于：预编译机制。预编译完成之后，SQL的结构已经固定，即便用户输入非法参数，也不会对SQL的结构产生影响，从而避免了潜在的安全风险。</p>
</li>
<li><p>预编译是提前对SQL语句进行预编译，而其后注入的参数将不会再进行SQL编译。我们知道，SQL注入是发生在编译的过程中，因为恶意注入了某些特殊字符，最后被编译成了恶意的执行操作。而预编译机制则可以很好的防止SQL注入。</p>
</li>
</ol>
<p>补充1:</p>
<p>$符号一般用来当作占位符，常使用Linux脚本的人应该对此有更深的体会吧。例如：$1，$2等等表示输入参数的占位符。知道了这点就能很容易区分$和#，从而不容易记错了。</p>
<p>补充2：</p>
<p>万事洞明皆学问，对于mybatis与sql这两门技术而言，看似简单，但是深挖进去会发现里面的东西还是挺多的。要想成为一名合格的开发人员，需要不断的去学习，更需要不断的去思考。可以参考：<a href="http://www.mybatis.cn/category/mysql-mybatis/" target="_blank" rel="noopener">http://www.mybatis.cn/category/mysql-mybatis/</a> ，系统的学习sql和mybatis的东西，这是本站2019年计划出版的小册子，欢迎试读。</p>
<p>补充3：</p>
<p>有网友提问：既然${}会引起sql注入，为什么有了#{}还需要有${}呢？那其存在的意义是什么？<br>可以这么去理解：#{}主要用于预编译，而预编译的场景其实非常受限，而${}用于替换，很多场景会出现替换，而这种场景可不是预编译，例如：MyBatis XML配置对抗MyBatis注解的一大杀器：SQL片段，抽取可重用的SQL语句。</p>
]]></content>
      <categories>
        <category>Mybatis</category>
      </categories>
  </entry>
  <entry>
    <title>Minor GC、Major GC和Full GC之间的区别</title>
    <url>/2196748335.html</url>
    <content><![CDATA[<p><em>原文链接： javacodegeeks 翻译： ImportNew.com - 光光头去打酱油</em></p>
<p><em>译文链接： <a href="http://www.importnew.com/15820.html" target="_blank" rel="noopener">http://www.importnew.com/15820.html</a></em></p>
<p>在 Plumbr 从事 GC 暂停检测相关功能的工作时，我被迫用自己的方式，通过大量文章、书籍和演讲来介绍我所做的工作。在整个过程中，经常对 Minor、Major、和 Full GC 事件的使用感到困惑。这也是我写这篇博客的原因，我希望能清楚地解释这其中的一些疑惑。<br>文章要求读者熟悉 JVM 内置的通用垃圾回收原则。堆内存划分为 Eden、Survivor 和 Tenured/Old 空间，代假设和其他不同的 GC 算法超出了本文讨论的范围。</p>
<a id="more"></a>
<h3 id="Minor-GC"><a href="#Minor-GC" class="headerlink" title="Minor GC"></a>Minor GC</h3><p>从年轻代空间（包括 Eden 和 Survivor 区域）回收内存被称为 Minor GC。这一定义既清晰又易于理解。但是，当发生Minor GC事件的时候，有一些有趣的地方需要注意到：</p>
<p>当 JVM 无法为一个新的对象分配空间时会触发 Minor GC，比如当 Eden 区满了。所以分配率越高，越频繁执行 Minor GC。<br>内存池被填满的时候，其中的内容全部会被复制，指针会从0开始跟踪空闲内存。Eden 和 Survivor 区进行了标记和复制操作，取代了经典的标记、扫描、压缩、清理操作。所以 Eden 和 Survivor 区不存在内存碎片。写指针总是停留在所使用内存池的顶部。<br>执行 Minor GC 操作时，不会影响到永久代。从永久代到年轻代的引用被当成 GC roots，从年轻代到永久代的引用在标记阶段被直接忽略掉。<br>质疑常规的认知，所有的 Minor GC 都会触发“全世界的暂停（stop-the-world）”，停止应用程序的线程。对于大部分应用程序，停顿导致的延迟都是可以忽略不计的。其中的真相就 是，大部分 Eden 区中的对象都能被认为是垃圾，永远也不会被复制到 Survivor 区或者老年代空间。如果正好相反，Eden 区大部分新生对象不符合 GC 条件，Minor GC 执行时暂停的时间将会长很多。<br>所以 Minor GC 的情况就相当清楚了——每次 Minor GC 会清理年轻代的内存。</p>
<h3 id="Major-GC-vs-Full-GC"><a href="#Major-GC-vs-Full-GC" class="headerlink" title="Major GC vs Full GC"></a>Major GC vs Full GC</h3><p>大家应该注意到，目前，这些术语无论是在 JVM 规范还是在垃圾收集研究论文中都没有正式的定义。但是我们一看就知道这些在我们已经知道的基础之上做出的定义是正确的，Minor GC 清理年轻带内存应该被设计得简单：<br><strong>Major GC 是清理永久代。</strong></p>
<p>Full GC 是清理整个堆空间—包括年轻代和永久代。<br>很不幸，实际上它还有点复杂且令人困惑。首先，许多 Major GC 是由 Minor GC 触发的，所以很多情况下将这两种 GC 分离是不太可能的。另一方面，许多现代垃圾收集机制会清理部分永久代空间，所以使用“cleaning”一词只是部分正确。<br>这使得我们不用去关心到底是叫 Major GC 还是 Full GC，大家应该关注当前的 GC 是否停止了所有应用程序的线程，还是能够并发的处理而不用停掉应用程序的线程。<br>这种混乱甚至内置到 JVM 标准工具。</p>
<p>下面一个例子很好的解释了我的意思。让我们比较两个不同的工具 Concurrent Mark 和 Sweep collector (-XX:+UseConcMarkSweepGC)在 JVM 中运行时输出的跟踪记录。<br>第一次尝试通过 jstat 输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1 my-precious: me$ jstat -gc -t 4235 1s</span><br><span class="line">12345678910111213 Time S0C    S1C    S0U    S1U      EC       EU        OC         OU       MC     MU    CCSC   CCSU   YGC     YGCT    FGC    FGCT     GCT    5.7 34048.0 34048.0  0.0   34048.0 272640.0 194699.7 1756416.0   181419.9  18304.0 17865.1 2688.0 2497.6      3    0.275   0      0.000    0.275 6.7 34048.0 34048.0 34048.0  0.0   272640.0 247555.4 1756416.0   263447.9  18816.0 18123.3 2688.0 2523.1      4    0.359   0      0.000    0.359 7.7 34048.0 34048.0  0.0   34048.0 272640.0 257729.3 1756416.0   345109.8  19072.0 18396.6 2688.0 2550.3      5    0.451   0      0.000    0.451 8.7 34048.0 34048.0 34048.0 34048.0 272640.0 272640.0 1756416.0  444982.5  19456.0 18681.3 2816.0 2575.8      7    0.550   0      0.000    0.550 9.7 34048.0 34048.0 34046.7  0.0   272640.0 16777.0  1756416.0   587906.3  20096.0 19235.1 2944.0 2631.8      8    0.720   0      0.000    0.72010.7 34048.0 34048.0  0.0   34046.2 272640.0 80171.6  1756416.0   664913.4  20352.0 19495.9 2944.0 2657.4      9    0.810   0      0.000    0.81011.7 34048.0 34048.0 34048.0  0.0   272640.0 129480.8 1756416.0   745100.2  20608.0 19704.5 2944.0 2678.4     10    0.896   0      0.000    0.89612.7 34048.0 34048.0  0.0   34046.6 272640.0 164070.7 1756416.0   822073.7  20992.0 19937.1 3072.0 2702.8     11    0.978   0      0.000    0.97813.7 34048.0 34048.0 34048.0  0.0   272640.0 211949.9 1756416.0   897364.4  21248.0 20179.6 3072.0 2728.1     12    1.087   1      0.004    1.09114.7 34048.0 34048.0  0.0   34047.1 272640.0 245801.5 1756416.0   597362.6  21504.0 20390.6 3072.0 2750.3     13    1.183   2      0.050    1.23315.7 34048.0 34048.0  0.0   34048.0 272640.0 21474.1  1756416.0   757347.0  22012.0 20792.0 3200.0 2791.0     15    1.336   2      0.050    1.38616.7 34048.0 34048.0 34047.0  0.0   272640.0 48378.0  1756416.0   838594.4  22268.0 21003.5 3200.0 2813.2     16    1.433   2      0.050    1.484</span><br></pre></td></tr></table></figure>
<p>这个片段是 JVM 启动后第17秒提取的。基于该信息，我们可以得出这样的结果，运行了12次 Minor GC、2次 Full GC，时间总跨度为50毫秒。通过 jconsole 或者 jvisualvm 这样的基于GUI的工具你能得到同样的结果。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1 java -XX:+PrintGCDetails -XX:+UseConcMarkSweepGC eu.plumbr.demo.GarbageProducer</span><br><span class="line">12345678910111213141516171819 3.157: [GC (Allocation Failure) 3.157: [ParNew: 272640K-&gt;34048K(306688K), 0.0844702 secs] 272640K-&gt;69574K(2063104K), 0.0845560 secs] [Times: user=0.23 sys=0.03, real=0.09 secs] 4.092: [GC (Allocation Failure) 4.092: [ParNew: 306688K-&gt;34048K(306688K), 0.1013723 secs] 342214K-&gt;136584K(2063104K), 0.1014307 secs] [Times: user=0.25 sys=0.05, real=0.10 secs] ... cut for brevity ...11.292: [GC (Allocation Failure) 11.292: [ParNew: 306686K-&gt;34048K(306688K), 0.0857219 secs] 971599K-&gt;779148K(2063104K), 0.0857875 secs] [Times: user=0.26 sys=0.04, real=0.09 secs] 12.140: [GC (Allocation Failure) 12.140: [ParNew: 306688K-&gt;34046K(306688K), 0.0821774 secs] 1051788K-&gt;856120K(2063104K), 0.0822400 secs] [Times: user=0.25 sys=0.03, real=0.08 secs] 12.989: [GC (Allocation Failure) 12.989: [ParNew: 306686K-&gt;34048K(306688K), 0.1086667 secs] 1128760K-&gt;931412K(2063104K), 0.1087416 secs] [Times: user=0.24 sys=0.04, real=0.11 secs] 13.098: [GC (CMS Initial Mark) [1 CMS-initial-mark: 897364K(1756416K)] 936667K(2063104K), 0.0041705 secs] [Times: user=0.02 sys=0.00, real=0.00 secs] 13.102: [CMS-concurrent-mark-start]13.341: [CMS-concurrent-mark: 0.238/0.238 secs] [Times: user=0.36 sys=0.01, real=0.24 secs] 13.341: [CMS-concurrent-preclean-start]13.350: [CMS-concurrent-preclean: 0.009/0.009 secs] [Times: user=0.03 sys=0.00, real=0.01 secs] 13.350: [CMS-concurrent-abortable-preclean-start]13.878: [GC (Allocation Failure) 13.878: [ParNew: 306688K-&gt;34047K(306688K), 0.0960456 secs] 1204052K-&gt;1010638K(2063104K), 0.0961542 secs] [Times: user=0.29 sys=0.04, real=0.09 secs] 14.366: [CMS-concurrent-abortable-preclean: 0.917/1.016 secs] [Times: user=2.22 sys=0.07, real=1.01 secs] 14.366: [GC (CMS Final Remark) [YG occupancy: 182593 K (306688 K)]14.366: [Rescan (parallel) , 0.0291598 secs]14.395: [weak refs processing, 0.0000232 secs]14.395: [class unloading, 0.0117661 secs]14.407: [scrub symbol table, 0.0015323 secs]14.409: [scrub string table, 0.0003221 secs][1 CMS-remark: 976591K(1756416K)] 1159184K(2063104K), 0.0462010 secs] [Times: user=0.14 sys=0.00, real=0.05 secs] 14.412: [CMS-concurrent-sweep-start]14.633: [CMS-concurrent-sweep: 0.221/0.221 secs] [Times: user=0.37 sys=0.00, real=0.22 secs] 14.633: [CMS-concurrent-reset-start]14.636: [CMS-concurrent-reset: 0.002/0.002 secs] [Times: user=0.00 sys=0.00, real=0.00 secs]</span><br></pre></td></tr></table></figure>
<p>在点头同意这个结论之前，让我们看看来自同一个 JVM 启动收集的垃圾收集日志的输出。显然- XX ： + PrintGCDetails 告诉我们一个不同且更详细的故事：</p>
<p>基于这些信息，我们可以看到12次 Minor GC 后开始有些和上面不一样了。没有运行两次 Full GC，这不同的地方在于单个 GC 在永久代中不同阶段运行了两次：</p>
<p>最初的标记阶段，用了0.0041705秒也就是4ms左右。这个阶段会暂停“全世界（ stop-the-world）”的事件，停止所有应用程序的线程，然后开始标记。<br>并行执行标记和清洗阶段。这些都是和应用程序线程并行的。<br>最后 Remark 阶段，花费了0.0462010秒约46ms。这个阶段会再次暂停所有的事件。<br>并行执行清理操作。</p>
<p>正如其名，此阶段也是并行的，不会停止其他线程。<br>所以，正如我们从垃圾回收日志中所看到的那样，实际上只是执行了 Major GC 去清理老年代空间而已，而不是执行了两次 Full GC。<br>如果你是后期做决 定的话，那么由 jstat 提供的数据会引导你做出正确的决策。它正确列出的两个暂停所有事件的情况，导致所有线程停止了共计50ms。但是如果你试图优化吞吐量，你会被误导的。清 单只列出了回收初始标记和最终 Remark 阶段，jstat的输出看不到那些并发完成的工作。</p>
<p>结论<br>考虑到这种情况，最好避免以 Minor、Major、Full GC 这种方式来思考问题。而应该监控应用延迟或者吞吐量，然后将 GC 事件和结果联系起来。<br>随着这些 GC 事件的发生，你需要额外的关注某些信息，GC 事件是强制所有应用程序线程停止了还是并行的处理了部分事件。</p>
]]></content>
      <categories>
        <category>Jvm虚拟机</category>
      </categories>
  </entry>
</search>
